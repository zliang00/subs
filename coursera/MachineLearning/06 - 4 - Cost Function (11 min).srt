1
00:00:00,160 --> 00:00:01,704
このビデオでは、

2
00:00:01,704 --> 00:00:04,010
ロジスティック回帰において

3
00:00:04,040 --> 00:00:05,869
パラメータのシータをどうフィッティングするかを扱う。

4
00:00:05,880 --> 00:00:06,982
その中でも特に

5
00:00:07,020 --> 00:00:10,386
最適化目的の関数、言い換えると

6
00:00:10,400 --> 00:00:14,470
コスト関数を定義したいと思う、

7
00:00:15,390 --> 00:00:17,370
これが教師あり学習で

8
00:00:17,370 --> 00:00:19,892
ロジスティック回帰にフィッティングする問題だ。

9
00:00:19,960 --> 00:00:22,210
m個のトレーニング手本の

10
00:00:22,210 --> 00:00:24,964
訓練セットがある。

11
00:00:24,964 --> 00:00:26,577
そしていつもどおり

12
00:00:26,577 --> 00:00:28,130
私たちの手本データは

13
00:00:28,150 --> 00:00:32,830
N+1 次元の特徴量のベクトルで表される。

14
00:00:32,830 --> 00:00:35,133
そしていつも通り

15
00:00:35,180 --> 00:00:36,498
x0=1 とする。

16
00:00:36,498 --> 00:00:38,315
私たちの最初の特徴量、またの名をゼロ番目の特徴量は

17
00:00:38,315 --> 00:00:39,951
いつも 1 だ。

18
00:00:39,970 --> 00:00:41,203
そしてこれは分類問題なので

19
00:00:41,203 --> 00:00:43,335
私たちの訓練セットは

20
00:00:43,350 --> 00:00:44,999
各ラベルの y が

21
00:00:45,010 --> 00:00:48,422
0 か 1 のどちらかである、という性質を持つ。

22
00:00:48,422 --> 00:00:50,576
これが仮説で

23
00:00:50,576 --> 00:00:52,007
そのパラメータは

24
00:00:52,007 --> 00:00:54,460
ここのシータとなる。

25
00:00:54,490 --> 00:00:55,572
そして私たちの議論したい

26
00:00:55,610 --> 00:00:57,339
問題とは、

27
00:00:57,340 --> 00:00:58,846
これらの訓練セットが与えられた時に

28
00:00:58,880 --> 00:01:02,482
パラメータのシータをどう選ぶか、言い換えるとどうフィッティングするかということだ。

29
00:01:02,510 --> 00:01:04,125
線形回帰のモデルを開発していた時に戻ると、

30
00:01:04,125 --> 00:01:08,463
こんなコスト関数を使っていた。

31
00:01:08,480 --> 00:01:10,868
ここでは以前とは少し違う書き方をした、

32
00:01:10,900 --> 00:01:12,663
1/2m と書く代わりに

33
00:01:12,670 --> 00:01:16,440
1/2 を和の中に入れた。

34
00:01:16,440 --> 00:01:17,440
今回はこのコスト関数を

35
00:01:17,440 --> 00:01:19,132
別の書き方で

36
00:01:19,140 --> 00:01:20,663
書いてみたい、

37
00:01:20,700 --> 00:01:22,009
それはこんな風に二次の項で

38
00:01:22,030 --> 00:01:23,920
書く代わりに、

39
00:01:23,920 --> 00:01:27,100
こんな風に書いてみよう、

40
00:01:28,310 --> 00:01:31,476
cost の、 h の x に、カンマ、y と。

41
00:01:31,500 --> 00:01:35,605
そして cost の h の x カンマ y の項を

42
00:01:35,605 --> 00:01:37,176
こう、

43
00:01:37,210 --> 00:01:39,727
定義する。

44
00:01:39,740 --> 00:01:42,641
それは単に誤差の二乗の半分。

45
00:01:42,670 --> 00:01:43,800
つまり今や、私たちはより明白に

46
00:01:43,800 --> 00:01:46,018
コスト関数は

47
00:01:46,018 --> 00:01:48,145
訓練セットすべてに

48
00:01:48,145 --> 00:01:49,740
足し合わせる、、、より正確に言うと

49
00:01:49,740 --> 00:01:51,427
1/m 掛けることの、

50
00:01:51,427 --> 00:01:56,046
訓練セットすべてにこのコスト項を足し合わせたもの、であることが分かる。

51
00:01:56,050 --> 00:01:58,065
そしてこの等式をさらに

52
00:01:58,065 --> 00:01:59,470
もう少しだけ単純化して、

53
00:01:59,490 --> 00:02:02,587
これらの上付き添字を取り除いてしまうと便利だ。

54
00:02:02,610 --> 00:02:04,408
すると、単に

55
00:02:04,408 --> 00:02:05,527
cost の h の x カンマ y は

56
00:02:05,527 --> 00:02:06,618
イコール、

57
00:02:06,618 --> 00:02:08,925
1/2 誤差の二乗。

58
00:02:08,925 --> 00:02:10,336
そしてこのコスト関数の解釈は

59
00:02:10,360 --> 00:02:11,876
これは私の学習アルゴリズムに

60
00:02:11,890 --> 00:02:13,447
支払って欲しい

61
00:02:13,460 --> 00:02:15,110
コストだ、

62
00:02:15,110 --> 00:02:16,701
もし学習アルゴリズムが

63
00:02:16,750 --> 00:02:18,737
出力した値が、

64
00:02:18,737 --> 00:02:19,912
この推定、h の x だった時で、

65
00:02:19,912 --> 00:02:21,258
そして実際の値のラベルが

66
00:02:21,310 --> 00:02:24,035
y だった時に。

67
00:02:24,050 --> 00:02:27,836
つまり、単にこれらの上付き添字を取り除いた。

68
00:02:27,840 --> 00:02:29,756
そして線形回帰の場合、

69
00:02:29,756 --> 00:02:31,537
定義したコストは、当然ながら

70
00:02:31,537 --> 00:02:32,757
これのコストは

71
00:02:32,757 --> 00:02:34,535
1/2 掛ける

72
00:02:34,540 --> 00:02:36,232
推定した値と

73
00:02:36,232 --> 00:02:37,663
実際に観測された値との

74
00:02:37,670 --> 00:02:38,943
差の二乗だ。

75
00:02:38,943 --> 00:02:41,103
今、このコスト関数は

76
00:02:41,103 --> 00:02:42,848
線形回帰には問題ないが、

77
00:02:42,848 --> 00:02:47,418
今興味があるのはロジスティック回帰だ。

78
00:02:47,430 --> 00:02:49,146
もし J のここに代入される

79
00:02:49,150 --> 00:02:51,992
コスト関数を最小化できたら

80
00:02:52,020 --> 00:02:53,817
問題ないだろう。

81
00:02:53,817 --> 00:02:55,476
でもこの場合のコスト関数は

82
00:02:55,480 --> 00:02:57,640
パラメータのシータに対して、

83
00:02:57,640 --> 00:03:01,807
非凸であることが知られている。

84
00:03:01,820 --> 00:03:03,968
非凸とはこういう意味だ。

85
00:03:03,990 --> 00:03:05,313
私たちはあるコスト関数 J のシータがあり

86
00:03:05,313 --> 00:03:08,118
ロジスティック回帰の場合

87
00:03:08,140 --> 00:03:12,113
この関数hは

88
00:03:12,113 --> 00:03:13,495
非線形だ。いいかな？

89
00:03:13,500 --> 00:03:14,538
それは、

90
00:03:14,538 --> 00:03:16,384
(1 足す e の マイナスシータの転置に x) 分の 1 だった。

91
00:03:16,384 --> 00:03:19,591
つまりそれは、非常に複雑な非線形の関数だ。

92
00:03:19,591 --> 00:03:21,108
そして sigmoid 関数を

93
00:03:21,130 --> 00:03:22,104
これに代入して、

94
00:03:22,104 --> 00:03:23,239
そしてこのコスト関数を

95
00:03:23,300 --> 00:03:25,016
ここに代入して

96
00:03:25,020 --> 00:03:26,746
そして J のシータがどうなるかを

97
00:03:26,746 --> 00:03:28,200
プロットしてみると、

98
00:03:28,210 --> 00:03:29,650
J のシータはこんな感じの

99
00:03:29,650 --> 00:03:33,493
関数となることが分かる。

100
00:03:33,500 --> 00:03:35,958
見ての通り、多くの局所最適があり、

101
00:03:35,958 --> 00:03:37,321
それを表す正式な用語が

102
00:03:37,340 --> 00:03:39,488
非凸関数、という言葉だ。

103
00:03:39,500 --> 00:03:40,644
そしてこんな類の関数に

104
00:03:40,644 --> 00:03:41,880
最急降下法を

105
00:03:41,880 --> 00:03:43,192
適用しても、

106
00:03:43,192 --> 00:03:45,160
大域的な極小に収束するという

107
00:03:45,170 --> 00:03:47,747
保証はない。

108
00:03:47,747 --> 00:03:48,867
一方、

109
00:03:48,870 --> 00:03:50,350
こんなコスト関数 J のシータ、

110
00:03:50,350 --> 00:03:52,100
これは凸関数で、

111
00:03:52,100 --> 00:03:53,599
一つの弓形の関数で、こんな見た目のもの、

112
00:03:53,599 --> 00:03:55,250
こういうものに対して

113
00:03:55,250 --> 00:03:56,675
最急降下法を実行すれば

114
00:03:56,675 --> 00:03:58,543
大域極小に

115
00:03:58,543 --> 00:04:01,147
収束することが

116
00:04:01,170 --> 00:04:04,917
保証されるわけだ。

117
00:04:04,917 --> 00:04:07,020
そして二乗のコスト関数を

118
00:04:07,020 --> 00:04:08,460
使う問題点としては、

119
00:04:08,520 --> 00:04:10,400
この真ん中の

120
00:04:10,400 --> 00:04:12,371
sigmoid 関数が、

121
00:04:12,371 --> 00:04:14,107
まさに非線形なので

122
00:04:14,107 --> 00:04:15,987
J のシータは非凸の関数になってしまう、

123
00:04:15,987 --> 00:04:17,962
もしそれをコスト関数の

124
00:04:17,962 --> 00:04:21,294
二乗で定義すれば、だ。

125
00:04:21,294 --> 00:04:22,313
だから私たちがやりたいのは、

126
00:04:22,320 --> 00:04:23,822
その代わりに

127
00:04:23,822 --> 00:04:25,576
異なるコスト関数で

128
00:04:25,576 --> 00:04:28,063
凸関数で

129
00:04:28,063 --> 00:04:29,257
つまり最急降下法のような

130
00:04:29,280 --> 00:04:30,919
素晴らしいアルゴリズムを適用して

131
00:04:30,940 --> 00:04:33,683
大域極小を見つけることを保証できるようにすることだ。

132
00:04:33,683 --> 00:04:37,295
これがロジスティック回帰で使うコスト関数だ。

133
00:04:37,295 --> 00:04:39,313
アルゴリズムが

134
00:04:39,320 --> 00:04:40,710
h の x という値を出力した時に

135
00:04:40,710 --> 00:04:42,924
支払うコストは

136
00:04:42,924 --> 00:04:44,596
こんなだと言っているわけだ。

137
00:04:44,620 --> 00:04:46,722
つまりこれはある数、0.7 などで、

138
00:04:46,722 --> 00:04:48,670
それは h の x という値を推定している。

139
00:04:48,670 --> 00:04:50,780
そしてコストは

140
00:04:50,780 --> 00:04:52,032
実際の値がラベル y となるとすると、

141
00:04:52,032 --> 00:04:54,087
コストは、

142
00:04:54,090 --> 00:04:56,061
y=1 の時は

143
00:04:56,100 --> 00:04:57,861
-log h(x) として、

144
00:04:57,861 --> 00:04:59,447
そして y=0 の時は

145
00:04:59,460 --> 00:05:02,010
-log(1-h(x)) とする。

146
00:05:02,020 --> 00:05:04,205
これは一見、非常に複雑な関数に見える。

147
00:05:04,230 --> 00:05:05,773
だがこの関数が何なのか、

148
00:05:05,773 --> 00:05:08,147
直感を得るためにプロットしてみよう。

149
00:05:08,160 --> 00:05:11,054
y=1 のケースから始めよう。

150
00:05:11,070 --> 00:05:12,461
もし y=1 なら

151
00:05:12,461 --> 00:05:14,958
コスト関数は

152
00:05:14,958 --> 00:05:18,240
-log h(x) だから、

153
00:05:18,240 --> 00:05:19,601
プロットすると、

154
00:05:19,601 --> 00:05:21,564
横軸を

155
00:05:21,580 --> 00:05:22,961
h(x) とすると、

156
00:05:22,961 --> 00:05:24,722
この仮説は 0 から 1 までの

157
00:05:24,730 --> 00:05:26,611
値のどれかを出力することを

158
00:05:26,630 --> 00:05:28,465
知っている。

159
00:05:28,465 --> 00:05:28,465
いいかな？

160
00:05:28,490 --> 00:05:30,514
つまり h(x) は

161
00:05:30,530 --> 00:05:31,940
0 から 1 の間を変化する。

162
00:05:31,940 --> 00:05:35,469
このコスト関数がどんな感じかプロットすると、

163
00:05:35,470 --> 00:05:37,981
こんな感じになるだろう。

164
00:05:37,981 --> 00:05:39,044
どうしてプロットがこんな感じになるかを

165
00:05:39,044 --> 00:05:41,363
理解する一つの方法としては、

166
00:05:41,440 --> 00:05:44,988
z を横軸に log z を

167
00:05:45,000 --> 00:05:47,656
プロットすると

168
00:05:47,656 --> 00:05:48,794
こんな感じとなる。

169
00:05:48,794 --> 00:05:50,369
それはマイナス無限に近づいていく。

170
00:05:50,369 --> 00:05:53,700
これが log 関数のグラフだ。

171
00:05:53,700 --> 00:05:55,963
これが 0、これが 1 。

172
00:05:55,980 --> 00:05:57,560
ここでこの z はもちろん

173
00:05:57,560 --> 00:05:59,653
h(x) の役割を果たしている。

174
00:05:59,653 --> 00:06:02,030
だから -log z は

175
00:06:02,030 --> 00:06:06,329
こんな感じだ。

176
00:06:06,330 --> 00:06:08,098
単に符号を反転しただけだ。

177
00:06:08,100 --> 00:06:09,822
-log z。

178
00:06:09,822 --> 00:06:11,013
そして私たちはこの関数のうち、

179
00:06:11,020 --> 00:06:12,580
0 と 1 の間の領域にだけ

180
00:06:12,610 --> 00:06:14,014
関心がある。

181
00:06:14,014 --> 00:06:15,924
だからそこを取り除いて

182
00:06:15,924 --> 00:06:17,962
残ったのは

183
00:06:17,980 --> 00:06:21,555
曲線のこの部分。

184
00:06:21,630 --> 00:06:23,200
これこそが左の曲線となる。

185
00:06:23,200 --> 00:06:25,472
ここでこのコスト関数は

186
00:06:25,500 --> 00:06:29,666
いくつか興味深い、都合の良い性質を持っている。

187
00:06:29,690 --> 00:06:32,103
まず、もし y=1 で

188
00:06:32,103 --> 00:06:35,003
h(x)=1 の時は、

189
00:06:35,010 --> 00:06:37,367
つまり、仮説が正確だとすると、

190
00:06:37,410 --> 00:06:39,000
つまり h=1 を推定して

191
00:06:39,000 --> 00:06:40,261
y が厳密に

192
00:06:40,261 --> 00:06:42,744
推定した値と等しい。

193
00:06:42,744 --> 00:06:44,432
するとコストはイコール 0 となる。

194
00:06:44,432 --> 00:06:44,432
いいかな？

195
00:06:44,432 --> 00:06:47,475
これは曲線が実際には水平に漸近しているわけでは、、、

196
00:06:47,475 --> 00:06:49,866
やり直し！まず、気付くこととして

197
00:06:49,880 --> 00:06:51,006
h(x)=1 で

198
00:06:51,006 --> 00:06:53,056
仮説が y=1 を

199
00:06:53,056 --> 00:06:55,113
推定していて、

200
00:06:55,113 --> 00:06:56,342
そして実際に y=1 の時、

201
00:06:56,342 --> 00:06:58,502
コストはイコール 0 となる。

202
00:06:58,530 --> 00:07:00,975
それはこの点に対応する。

203
00:07:00,975 --> 00:07:00,975
いいかな？

204
00:07:01,030 --> 00:07:02,332
もし h(x)=1 で

205
00:07:02,332 --> 00:07:04,068
y=1 の場合だけに

206
00:07:04,068 --> 00:07:06,273
ここでは関心がある。

207
00:07:06,273 --> 00:07:08,366
そして h(x)=1 。

208
00:07:08,366 --> 00:07:11,063
するとコストはこの下の、イコール 0 となる。

209
00:07:11,063 --> 00:07:13,082
そしてこれは、私たちが望むことでもある。

210
00:07:13,082 --> 00:07:13,968
何故なら、もし私たちが

211
00:07:13,968 --> 00:07:17,673
y の結果を正しく推定したら、コストは 0 であるべきだ。

212
00:07:17,673 --> 00:07:21,466
だがここで、もう一つ気付くこととしては、

213
00:07:21,470 --> 00:07:23,456
h(x) が 0 に近づくと、

214
00:07:23,456 --> 00:07:25,037
つまりこの h、

215
00:07:25,037 --> 00:07:26,909
仮説の出力が 0 に近づくと

216
00:07:26,909 --> 00:07:30,163
コストは急増して、無限大に近付く。

217
00:07:30,163 --> 00:07:31,513
これがやっていることは

218
00:07:31,513 --> 00:07:34,271
以下のような直感を表現しているわけだ、

219
00:07:34,310 --> 00:07:36,890
それは、仮説が 0 を出力した時、

220
00:07:36,890 --> 00:07:38,574
つまりそれというのは仮説が

221
00:07:38,574 --> 00:07:39,960
y=1 となる可能性は 0 だ、と

222
00:07:39,960 --> 00:07:41,541
言っているわけだが、

223
00:07:41,541 --> 00:07:42,516
それはつまり患者が

224
00:07:42,520 --> 00:07:44,010
病院に行った時、

225
00:07:44,020 --> 00:07:45,594
「あなたの腫瘍が悪性である可能性、

226
00:07:45,610 --> 00:07:47,337
y=1 となる可能性は 0 だ」

227
00:07:47,337 --> 00:07:49,807
と言われるようなものだ。

228
00:07:49,807 --> 00:07:52,154
つまりあなたの腫瘍が悪性である可能性は

229
00:07:52,160 --> 00:07:55,130
絶対にない、ということ。

230
00:07:55,150 --> 00:07:56,776
でもそこでもし患者の腫瘍が

231
00:07:56,776 --> 00:08:00,111
実際には悪性だと判明したとする。

232
00:08:00,111 --> 00:08:01,879
つまり y=1 となってしまったということだ、

233
00:08:01,880 --> 00:08:03,291
私たちがそうなる確率は 0 だ！と

234
00:08:03,300 --> 00:08:05,375
宣言した後にも関わらず。

235
00:08:05,390 --> 00:08:08,716
それが悪性である可能性は完璧に 0 だ、と言ったのに。

236
00:08:08,716 --> 00:08:09,759
それだけの確信を持って

237
00:08:09,760 --> 00:08:11,186
患者に言ったのにも関わらず、

238
00:08:11,240 --> 00:08:13,018
私たちが誤っている、ということが判明したのだ、

239
00:08:13,018 --> 00:08:14,688
それはこのアルゴリズムに

240
00:08:14,690 --> 00:08:16,122
非常に高い罰則のコストを与えるべきだろう、

241
00:08:16,122 --> 00:08:17,963
その直感が、この y=1 で

242
00:08:17,963 --> 00:08:20,474
h(x) が 0 に近づくと

243
00:08:20,474 --> 00:08:21,900
無限大へと行くことで

244
00:08:21,900 --> 00:08:24,334
表すことができている。

245
00:08:24,334 --> 00:08:26,725
これは y=1 の時だが、

246
00:08:26,725 --> 00:08:28,875
では次に y=0 の時の

247
00:08:28,875 --> 00:08:32,371
コスト関数がどんな形か見てみよう。

248
00:08:32,410 --> 00:08:35,710
y=0 の時には、

249
00:08:35,720 --> 00:08:39,121
コストはこんな式となる。

250
00:08:39,121 --> 00:08:40,403
そして関数、

251
00:08:40,403 --> 00:08:42,751
-log(1-z) をプロットすると、

252
00:08:42,780 --> 00:08:45,839
コスト関数は

253
00:08:45,839 --> 00:08:49,245
こんな見た目となる。

254
00:08:49,245 --> 00:08:50,256
そして 0 から 1 に行く。

255
00:08:50,270 --> 00:08:53,263
こんな感じ。

256
00:08:53,280 --> 00:08:54,611
だから y=0 の時の

257
00:08:54,611 --> 00:08:55,872
コスト関数をプロットすると、

258
00:08:55,872 --> 00:08:57,823
こんな感じになり、

259
00:08:57,823 --> 00:09:00,763
そしてこの曲線が

260
00:09:00,763 --> 00:09:02,404
どんなかというと、

261
00:09:02,404 --> 00:09:04,937
今回は h(x) が 1 に近づくと

262
00:09:04,937 --> 00:09:08,273
無限大に向かって急上昇する。

263
00:09:08,290 --> 00:09:09,880
何故ならそれは

264
00:09:09,900 --> 00:09:11,199
y が 0 だと判明したが、

265
00:09:11,200 --> 00:09:11,200


266
00:09:11,200 --> 00:09:13,966
私たちは、y=1 と確信を持って、

267
00:09:13,966 --> 00:09:15,286
確率 1 として推定した

268
00:09:15,320 --> 00:09:17,281
ということに対応するから。

269
00:09:17,281 --> 00:09:21,569
だから、非常に大きなコストを払うことになる。

270
00:09:21,569 --> 00:09:23,143
では y=0 の時の

271
00:09:23,143 --> 00:09:25,063
コスト関数をプロットしてみよう。

272
00:09:25,063 --> 00:09:29,702
y=0 の時は、これが私たちのコスト関数となる。

273
00:09:29,702 --> 00:09:31,914
この式を見てみると、

274
00:09:31,914 --> 00:09:33,726
そしてプロットしてみると、

275
00:09:33,726 --> 00:09:36,221
-log(1-z) なので

276
00:09:36,221 --> 00:09:37,428
こんな見た目、

277
00:09:37,428 --> 00:09:40,071
こんな図になる。

278
00:09:40,071 --> 00:09:41,669
0 から 1 までの範囲で

279
00:09:41,680 --> 00:09:43,610
横軸が

280
00:09:43,610 --> 00:09:45,850
z軸。

281
00:09:45,850 --> 00:09:47,221
つまりもしこのコスト関数をとって、

282
00:09:47,221 --> 00:09:48,397
y=0 の時を

283
00:09:48,397 --> 00:09:49,614
プロットすると、

284
00:09:49,614 --> 00:09:51,186
その結果は

285
00:09:51,186 --> 00:09:55,109
こんな見た目のコスト関数となる。

286
00:09:55,109 --> 00:09:56,743
このコスト関数はどういうものかというと、

287
00:09:56,743 --> 00:09:58,650
h(x) が 1 に近付くと急上昇する、

288
00:09:58,660 --> 00:09:59,530
言い換えると

289
00:09:59,560 --> 00:10:01,448
正の無限大に

290
00:10:01,448 --> 00:10:03,707
行く。

291
00:10:03,710 --> 00:10:05,443
これは仮説が、つまり h(x) が 1 を、

292
00:10:05,443 --> 00:10:07,159
確かさ、つまり確率 1 で

293
00:10:07,180 --> 00:10:08,847
推定し、

294
00:10:08,850 --> 00:10:10,406
絶対に y=1 だと

295
00:10:10,406 --> 00:10:12,121
推定したのに、

296
00:10:12,121 --> 00:10:14,283
y=0 と

297
00:10:14,283 --> 00:10:15,563
判明した場合に、

298
00:10:15,563 --> 00:10:17,219
仮説、つまりは学習アルゴリズムに

299
00:10:17,219 --> 00:10:18,206
非常に大きなコストを課すのは

300
00:10:18,206 --> 00:10:21,940
合理的だ、という直感を取り込んだものと言える。

301
00:10:21,940 --> 00:10:21,940


302
00:10:21,940 --> 00:10:25,942
逆に、 h(x)=0 で

303
00:10:25,950 --> 00:10:27,483
y=0 なら、

304
00:10:27,483 --> 00:10:28,983
仮説は的中したことになる。

305
00:10:29,000 --> 00:10:30,626
y の推定値は 0 で

306
00:10:30,630 --> 00:10:32,371
y が結局

307
00:10:32,371 --> 00:10:34,376
0 となった

308
00:10:34,376 --> 00:10:36,701
だからこの点では

309
00:10:36,750 --> 00:10:40,139
コスト関数は 0 に行く。

310
00:10:40,160 --> 00:10:42,163
このビデオでは

311
00:10:42,163 --> 00:10:43,886
単体のトレーニング手本について

312
00:10:43,886 --> 00:10:46,428
コスト関数を定義した。

313
00:10:46,428 --> 00:10:50,251
凸解析がらみのトピックはこのコースの範囲を越えてしまう。

314
00:10:50,270 --> 00:10:51,594
だが私たちの選んだ

315
00:10:51,620 --> 00:10:53,080
コスト関数が

316
00:10:53,150 --> 00:10:54,774
凸型の最適化問題に

317
00:10:54,774 --> 00:10:57,926
帰着することを証明することができて、

318
00:10:57,960 --> 00:11:00,081
全体のコスト関数 J の

319
00:11:00,081 --> 00:11:01,463
シータは凸となり、

320
00:11:01,463 --> 00:11:04,368
局所最適が存在しないことが分かる。

321
00:11:04,370 --> 00:11:05,691
次のビデオでは

322
00:11:05,691 --> 00:11:07,753
これら単体のトレーニング手本に対しての

323
00:11:07,753 --> 00:11:08,923
コスト関数というアイデアを使って

324
00:11:08,923 --> 00:11:10,839
さらに開発を進めて

325
00:11:10,839 --> 00:11:12,522
トレーニングセット全体の

326
00:11:12,522 --> 00:11:13,773
コスト関数を定義する。

327
00:11:13,780 --> 00:11:16,104
そしてそれを、

328
00:11:16,104 --> 00:11:17,404
これまでやってきたよりも

329
00:11:17,404 --> 00:11:19,699
よりシンプルに書く方法も見つける。

330
00:11:19,699 --> 00:11:21,016
そしてそれを元に

331
00:11:21,030 --> 00:11:22,779
最急降下法を実行して

332
00:11:22,779 --> 00:11:25,835
それがロジスティック回帰のアルゴリズムとなる。
