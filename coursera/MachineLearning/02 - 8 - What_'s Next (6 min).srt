1
00:00:00,000 --> 00:00:04,839
今や、あなたは線形回帰と再急降下法を知った。

2
00:00:04,839 --> 00:00:09,437
ここから次は、これらのアイデアの、いくつかの重要な拡張を伝授する予定だ。

3
00:00:09,437 --> 00:00:13,668
具体的にはこれだ。
まず、この最小化問題を解くには、

4
00:00:13,668 --> 00:00:18,468
シータ0とシータ1について、繰り返しのアルゴリズムを使わずに解く方法が

5
00:00:18,468 --> 00:00:22,978
あることが判明する。

6
00:00:22,978 --> 00:00:27,555
再急降下法のようなアルゴリズムは、知っての通り、何度か繰り返しが必要だった。

7
00:00:27,555 --> 00:00:32,285
だからシータ0とシータ1を基本的には一発で解くアルゴリズムには、

8
00:00:32,285 --> 00:00:36,897
長所と短所があることが分かる。

9
00:00:36,897 --> 00:00:41,685
1つの長所としては、悩ましい学習率アルファがもはや存在しないこと。

10
00:00:41,685 --> 00:00:46,414
だからある種の問題にはより速い。

11
00:00:46,414 --> 00:00:51,051
長所と短所についてはあとで議論する。
2番目に、たくさんの特徴量が存在する学習を

12
00:00:51,051 --> 00:00:55,424
今後議論することになる。
ここまでは、ただ一つだけの特徴量である、

13
00:00:55,424 --> 00:01:00,027
家の広さを価格を予測するために使ってきた。

14
00:01:00,027 --> 00:01:04,688
つまり x を取ってきて、y を予測するのに使った。
だがその他の学習の問題には

15
00:01:04,688 --> 00:01:09,899
学習の問題もありうる。
たとえば、広さだけでなく、

16
00:01:09,899 --> 00:01:15,448
寝室の数も、何階建てかも、築年数も知っているとしよう。

17
00:01:15,448 --> 00:01:20,930
そしてそれらを価格を予測するのに使いたいとする。

18
00:01:20,930 --> 00:01:26,005
その場合はこれらの特徴量を x1、x2、x3、x4 と呼ぶだろう。

19
00:01:26,005 --> 00:01:31,554
今や、見ての通り、4つの特徴量がある。
これら4つの特徴量を使って、家の価格がなぜそうなのかを予測したい。

20
00:01:31,554 --> 00:01:36,797
これらすべての特徴量、この場合は4つを

21
00:01:36,797 --> 00:01:41,858
すべてプロットしたり可視化するのは難しいことが分かる。

22
00:01:41,858 --> 00:01:47,243
例として、この種のデータセットをプロットしようとしたとしよう。

23
00:01:47,243 --> 00:01:52,823
縦軸は価格とする。そして一つの軸をここに

24
00:01:52,823 --> 00:01:58,078
もう一つの軸をここにおける。
この軸を家の広さとしようか、

25
00:01:58,078 --> 00:02:02,822
この軸を寝室の数としよう。
しかしこれでは、単に最初の2つの特徴量、

26
00:02:02,822 --> 00:02:07,414
広さと寝室までのプロットだ。
そして3つ目の追加の特徴量が来たら

27
00:02:07,414 --> 00:02:11,677
どうしたらいいのか？
これらのデータをどうプロットしたらいいか分からない。

28
00:02:11,677 --> 00:02:15,886
なぜなら4次元とか5次元の形状が必要なのだから。
3次元以上をどうプロットしたらいいのか

29
00:02:15,886 --> 00:02:19,930
さっぱり分からない。
ここにあるような3次元以上の場合は。

30
00:02:19,930 --> 00:02:24,139
また、見ての通り、記法も少し複雑になる。

31
00:02:24,139 --> 00:02:28,238
つまり単に x ではなくて x1 から x4 までの変数を扱うことになる。

32
00:02:28,238 --> 00:02:33,519
これらの添字は4つの異なる特徴量を表すために用いることとする。

33
00:02:33,519 --> 00:02:38,059
これがこれらすべてをシンプルに保ち、

34
00:02:38,059 --> 00:02:42,828
データに何が起こってるのか理解するベストな記法であると、分かるようになる。
どうプロットしたら良いのかすら分からないままでも。

35
00:02:42,828 --> 00:02:47,425
ベストな記法は結局、線形代数の記法ということになる。

36
00:02:47,425 --> 00:02:52,194
線形代数は記法を提供してくれて、
行列やベクトルと一緒に使えるものや操作も提供してくれる。

37
00:02:52,194 --> 00:02:58,234
例えば、ここに行列があり、その列は、

38
00:02:58,234 --> 00:03:03,377
最初の列が4つの家の広さを、2つ目の列は寝室の数を、

39
00:03:03,377 --> 00:03:08,025
そして何階建てか、築年数をそれぞれ表す。

40
00:03:08,025 --> 00:03:12,496
つまり行列は数字のブロックで、
すべてのデータ、すべての特徴量を x を持ち運びできる。

41
00:03:12,496 --> 00:03:17,881
そしてそれらを効率的に構成して、

42
00:03:17,881 --> 00:03:23,565
このような大きな数字のブロックにする。
そしてこれが線形代数で言うところのベクトルで、

43
00:03:23,565 --> 00:03:29,118
ここの4つの数字は前のスライドで見た、4つの家の価格を表す。

44
00:03:29,118 --> 00:03:34,334
この後の一連のビデオで、私がやる予定のことは、

45
00:03:34,334 --> 00:03:38,730
線形代数の簡単な復習を行うことだ。
もし行列やベクトルを以前に見たことがなければ、

46
00:03:38,730 --> 00:03:43,293
もしこのスライドのすべてがあなたにとって真新しいなら、
または以前に線形代数を見たことがあっても

47
00:03:43,293 --> 00:03:47,745
随分昔のことでもうあまり慣れ親しんでる感じがしないなら、

48
00:03:47,745 --> 00:03:52,419
この後の一連のビデオを見てほしい。
そこでは、より強力な線形回帰を実装するのに必要な、

49
00:03:52,419 --> 00:03:57,093
簡単な線形代数の復習をする。

50
00:03:57,093 --> 00:04:01,489
線形代数は線形回帰に便利なだけでなく、

51
00:04:01,489 --> 00:04:05,972
これら行列やベクトルというものは、後の機械学習のモデルを実装するのに、

52
00:04:05,972 --> 00:04:10,272
しかも実際に計算効率の良い実装を行うのに、助けになってくれることが、やがて分かる。

53
00:04:10,272 --> 00:04:15,088
そして見て分かる通り、

54
00:04:15,088 --> 00:04:19,617
行列やベクトルは大きな規模のデータ、大きなトレーニングのデータを扱う時に、

55
00:04:19,617 --> 00:04:23,918
それらを編成する、とても効率の良い手段となってくれる。

56
00:04:23,918 --> 00:04:28,619
だからもし、もし仮に、線形代数にあまり慣れていないなら、
または線形代数が複雑に、怖いと感じるなら、

57
00:04:28,619 --> 00:04:33,263
それを前にあまり見たことがないなら、、、心配ご無用。

58
00:04:33,263 --> 00:04:37,793
機械学習を実装するには、結局は、

59
00:04:37,793 --> 00:04:42,002
線形代数のとても基本的な部分しか必要としない。

60
00:04:42,002 --> 00:04:46,840
だから本当にすぐ、必要なものは全部、今後のちょっとのビデオで集められる。

61
00:04:46,840 --> 00:04:53,386
今後の一連のビデオを見るべきか判断できるように、具体的なトピックを示す。

62
00:04:53,386 --> 00:04:57,804
これが私が扱う予定の内容だ。
行列とベクトルとは何かを話し、

63
00:05:00,013 --> 00:05:02,222
行列とベクトルを足したり引いたり掛けたりするかの方法を話し、
行列の逆行列と転置という考えについて話す。

64
00:05:02,222 --> 00:05:06,696
だからもしあなたが、この後の一連のビデオを見るべきか分からないなら、

65
00:05:06,696 --> 00:05:11,393
これら2つを見てみてほしい。
もしこれらの量をどうやって計算するか分かるなら、

66
00:05:11,393 --> 00:05:15,643
この行列の転置掛けるもう一つの行列が分かるなら、

67
00:05:15,643 --> 00:05:20,173
もしこれらを以前に見たことがあるなら、
もし逆行列とベクトルの積から引くことの数掛ける別のベクトルを

68
00:05:20,173 --> 00:05:24,423
どう計算するのか知っているなら。
もしこれら2つのことが完全に慣れ親しんだものならば、

69
00:05:24,423 --> 00:05:29,309
その時は線形代数に関するオプショナルなビデオは安全にスキップできる。

70
00:05:29,309 --> 00:05:34,607
だがもしこれらのコンセプト、これらの数のブロック、つまりこれらの行列が

71
00:05:34,607 --> 00:05:39,906
何を意味しているのか、ちょっとしっかりとは分からないなら、
どうか今後の一連のビデオを見てほしい。

72
00:05:39,906 --> 00:05:45,142
機械学習のアルゴリズムをプログラムしたり大量のデータを扱うのに必要な

73
00:05:45,142 --> 00:05:49,936
線形代数を手早く教える。
