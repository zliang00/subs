1
00:00:00,000 --> 00:00:05,399
このビデオでは、目的関数というものを定義する。

2
00:00:05,399 --> 00:00:10,688
これは、データに対し、どのように最適な直線を当てはめるか算出するのに役に立つ。

3
00:00:10,688 --> 00:00:16,758
線形回帰では、ここに示すような訓練セットがある。思い出して欲しい。

4
00:00:16,758 --> 00:00:21,972
 m で表記されているのは、訓練サンプルの数だ。だから m=47 。

5
00:00:21,972 --> 00:00:27,731
そして、予測をするのに使う仮説の数式は、この線形関数だ。

6
00:00:27,731 --> 00:00:33,723
もう少し専門用語を使うと、この theta 0 および theta 1 、これだ、

7
00:00:33,723 --> 00:00:39,759
こうした theta i は、モデルのパラメータという。

8
00:00:39,759 --> 00:00:44,578
このビデオでは、

9
00:00:44,578 --> 00:00:49,654
これら二つのパラメータの値を、どのように選択するかについて話す。

10
00:00:49,654 --> 00:00:54,408
パラメータ theta 0 と theta 1 の選び方次第では、異なった仮説、

11
00:00:54,408 --> 00:00:59,355
異なった仮説関数となる。皆さんの中には既に知っている人もいると思うが、

12
00:00:59,355 --> 00:01:04,367
このスライドでいくつか例を見ていきたいと思う。

13
00:01:04,367 --> 00:01:09,378
もし theta 0 が 1.5 で theta 1 が 0 だと、仮説関数はこのようになる。

14
00:01:09,378 --> 00:01:15,701
そう、なぜなら、仮説関数は h(x) = 1.5 + 0x、

15
00:01:15,701 --> 00:01:22,645
つまり、この定数値関数で、1.5 の値で水平になる。

16
00:01:22,645 --> 00:01:29,332
もし theta 0 が 0 で theta 1 が 0.5 だと、仮説はこうなる。

17
00:01:29,332 --> 00:01:35,536
それはこの点 (2, 1) を通過するはずだ、ここでは h(x)、

18
00:01:35,536 --> 00:01:40,666
あるいは、実際には h theta(x) だが、時々 theta を省略することがある。

19
00:01:40,666 --> 00:01:46,518
だから h(x) は単に = 0.5  x となり、それはこのような形になる。

20
00:01:46,518 --> 00:01:52,443
そして最後に、もし theta 0 が 1 で theta 1 が 0.5 だと、

21
00:01:52,443 --> 00:01:58,598
結果はこのような仮説になる。これは点 (2, 2) を通過するはずだ。

22
00:01:58,598 --> 00:02:04,468
こんな風に。そしてこれは新しい h(x) あるいは新しい h theta(x) だ。

23
00:02:04,468 --> 00:02:09,980
覚えていると思うが、これは h theta(x) だが、単に簡略化のために

24
00:02:09,980 --> 00:02:16,584
時々私は h(x) と書く。線形回帰では訓練セットがあり、

25
00:02:16,584 --> 00:02:22,439
ここにプロットしたようなものになるかもしれない。ここで行いたいのは、

26
00:02:22,439 --> 00:02:28,295
パラメータ theta 0 と theta 1 の値を算出することだ。
そしてその結果として得られる直線が、

27
00:02:28,295 --> 00:02:33,799
データによく適合した直線に対応するようにすることだ。ちょうどこの線のように。

28
00:02:33,799 --> 00:02:39,756
では、どのようにして theta 0 と theta 1 の値を、データに対して

29
00:02:39,756 --> 00:02:45,350
よく適合するように算出するか。
考え方としては、パラメータ theta 0 と theta 1 を 選ぶ際に、

30
00:02:45,350 --> 00:02:51,162
h(x)、つまり入力 x に対する予測値が

31
00:02:51,162 --> 00:02:56,330
最低でも y の値に近似するように、

32
00:02:56,330 --> 00:03:01,133
訓練セットのサンプル、訓練サンプルのそれぞれに対して選ぶ。

33
00:03:01,133 --> 00:03:06,505
訓練セットでは、数件のサンプルが与えられており、x が家を指定し、

34
00:03:06,505 --> 00:03:11,796
その実際の販売価格を知っている。だから、パラメータの値を選ぶのに、

35
00:03:11,796 --> 00:03:17,302
少なくとも訓練セットでは、訓練セットの x の値を与えて、適度な正確さで

36
00:03:17,302 --> 00:03:23,507
y の値に近い予測を出力することだ。これを形式化しよう。

37
00:03:23,507 --> 00:03:29,401
線形回帰で行うことは、最小化問題を解くことだ。

38
00:03:29,401 --> 00:03:38,787
だから、theta 0、theta 1 の上に minimize と書く。

39
00:03:38,787 --> 00:03:44,379
そして、これを小さくしたい。つまり、h(x) と y の差を小さくしたい。

40
00:03:44,379 --> 00:03:50,493
そしてさらに、最小化しようとするのは、仮説の出力と

41
00:03:50,493 --> 00:03:56,159
実際の家の販売価格の差の二乗だ。いいかな。では少し詳細を加えていく。

42
00:03:56,159 --> 00:04:01,379
私が (x(i), y(i)) という表記を使って

43
00:04:01,379 --> 00:04:07,418
i 番目の訓練サンプルを表したのを、思い出して欲しい。
だから、実際に欲しいのは、訓練セットの総和だ。

44
00:04:07,418 --> 00:04:13,202
i = 1 から m までの差の二乗の総和を行い、

45
00:04:13,202 --> 00:04:18,896
これは i 番目の家の仮説の予測値、

46
00:04:18,896 --> 00:04:24,380
その家の広さを入力した場合だ。引くことの、実際の価格、

47
00:04:24,380 --> 00:04:29,588
i 番目の家が実際に売れる価格だ。最小化したいのは、訓練セットの総和、

48
00:04:29,588 --> 00:04:35,281
i = 1 から m の間のこの二乗誤差の差分、

49
00:04:35,281 --> 00:04:41,091
家の予想価格と実際に売れる価格との差の二乗の合計だ。

50
00:04:41,091 --> 00:04:47,723
そして思い出して欲しいのは、ここで m と表記したのは訓練セットのサイズだった。

51
00:04:47,723 --> 00:04:53,347
だから、この m は訓練サンプルの数だ。

52
00:04:53,347 --> 00:04:59,045
「#」の記号は、訓練サンプルの「数」の省略だ。いいかな。

53
00:04:59,045 --> 00:05:04,888
そして計算をいくらか簡略化するために、実際には、それに 1/m を掛ける。

54
00:05:04,888 --> 00:05:09,578
つまり、平均誤差を最小化するわけだ。これを1/2m で最小化する。

55
00:05:09,578 --> 00:05:13,926
2 を加えたのは、定数として 1/2 をその前に掛けるということで、

56
00:05:13,926 --> 00:05:18,386
これにより計算が少し楽になる。何かを半分にして最小化しても、

57
00:05:18,386 --> 00:05:23,073
パラメータ theta 0 と theta 1 の値はやはり同じになるはずだ。
その関数を最小化した場合と同じに。

58
00:05:23,073 --> 00:05:27,647
そして単に確認だが、この数式の意味は明確だ。

59
00:05:27,647 --> 00:05:35,569
この式 h theta(x)、これは、既にお馴染みだと思う。

60
00:05:35,569 --> 00:05:44,880
これ =  theta 0 + theta 1  x(i) だ。

61
00:05:44,880 --> 00:05:49,814
そしてこの表記、theta 0 と theta 1 の上に minimize としてある意味は、

62
00:05:49,814 --> 00:05:54,369
theta 0 と theta 1 の値を、この式が最小になるよう求めなさい、ということだ。

63
00:05:54,369 --> 00:05:59,557
この式は theta 0 と theta 1 に依存する。わかっただろうか。

64
00:05:59,557 --> 00:06:04,382
まとめると、この問題は、ある条件で
 theta 0 と theta 1 の値を見つけることを定義している。

65
00:06:04,575 --> 00:06:09,292
その条件とは、平均ではなく、
実際には 1/2m 掛ける二乗誤差の総和が最小化なるように、ということだ。

66
00:06:09,292 --> 00:06:14,590
差は、訓練セットに対する、予測値から訓練セットの実際の家の価格を引いたものだ。

67
00:06:14,590 --> 00:06:19,694
これが最終的な線形回帰の目的関数だ。

68
00:06:19,694 --> 00:06:25,127
そしてこれを少し書き直して、もっときれいにするために、

69
00:06:25,127 --> 00:06:30,580
私が行うのは、慣習的に通常、目的関数の定義は正にこの通りだ。

70
00:06:30,860 --> 00:06:38,965
この数式、ここに書き出したものだ。

71
00:06:38,965 --> 00:06:48,388
私がしたいのは、最小化するパラメータは theta 0、theta 1、

72
00:06:48,388 --> 00:06:57,428
関数は J(theta 0, theta 1) 、今書いたもの、これが目的関数だ。

73
00:06:57,428 --> 00:07:06,943
さて、この目的関数は、二乗誤差関数と呼ばれたり、時には、

74
00:07:06,943 --> 00:07:14,461
二乗誤差目的関数と呼ばれることもある。

75
00:07:14,461 --> 00:07:19,006
ところでなぜ誤差を二乗にするのか。実は、二乗誤差目的関数は

76
00:07:19,006 --> 00:07:23,214
ほとんどの問題に、回帰問題において、妥当な選択であり、よく機能するのだ。

77
00:07:23,214 --> 00:07:27,815
他にもよく機能する目的関数はあるが、

78
00:07:27,815 --> 00:07:32,473
二乗誤差関数は、回帰問題でおそらく最も一般的に使われている。

79
00:07:32,473 --> 00:07:36,793
このクラスでは後に、代わりの目的関数についても話すが、この選択、今見せたものは

80
00:07:36,793 --> 00:07:41,282
ほとんどの線形回帰問題で試してみるのに非常に妥当なものだ。

81
00:07:41,282 --> 00:07:45,706
さて、これが目的関数だ。

82
00:07:45,706 --> 00:07:50,899
ここまでは、単にこの目的関数の数学的な定義を見ただけで、

83
00:07:50,899 --> 00:07:55,973
万が一この関数 J(theta 0, theta 1)、万が一この関数が少し抽象的に見えるならば、

84
00:07:55,973 --> 00:08:00,808
それが何をしているのか感覚がつかめていなければ、次のビデオで

85
00:08:00,808 --> 00:08:05,882
次の数件のビデオで、実際に少しもっと深く、目的関数 J が何をしているかを学び、

86
00:08:05,882 --> 00:08:10,776
それが何を計算していて、なぜそれを使いたいのか、

87
00:08:10,776 --> 00:08:12,329
皆さんがもっと直感的に理解できるようにしたいと思う。
