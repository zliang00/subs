1
00:00:00,000 --> 00:00:04,353
前のビデオでは、最急降下法の数学的定義をした。

2
00:00:04,353 --> 00:00:09,464
もっと掘り下げて行こう。このビデオでは、アルゴリズムの挙動、

3
00:00:09,464 --> 00:00:14,701
アルゴリズムの手順の意味について直感的理解を深めて行く。

4
00:00:14,701 --> 00:00:20,639
ここに前回見た最急降下法アルゴリズムがある。

5
00:00:20,639 --> 00:00:26,427
思い返してほしいのは、このパラメータ、この項、alpha は学習率だ。

6
00:00:26,427 --> 00:00:32,444
そしてそれは パラメータ theta j を更新する際のステップの大きさを制御する。

7
00:00:32,444 --> 00:00:41,360
そして、この二つ目の項は、導関数項だ。

8
00:00:41,360 --> 00:00:47,360
このビデオでは 2点について直感的理解を深める。
この二つの項が何をしているのか。

9
00:00:47,360 --> 00:00:53,077
なぜこの二つを組み合わせると、この更新が全体として意味をなすのか。

10
00:00:53,077 --> 00:00:58,460
こうした直感的理解を伝えるために、少し簡素化した例を使いたい。

11
00:00:58,460 --> 00:01:03,022
最小化する関数のパラメータを一つだけにする。

12
00:01:03,022 --> 00:01:07,294
だから目的関数 J のパラメータは一つ、theta 1 だけだ。
ちょうど少し前のビデオでやったように。

13
00:01:07,294 --> 00:01:11,913
そして theta 1 は実数だ。これで一次元のプロットになるので、

14
00:01:11,913 --> 00:01:16,416
見て理解するのが少し簡単になる。
そして最急降下法がこの関数に対して何を行うか理解する。

15
00:01:16,416 --> 00:01:23,940
さて、これが関数である J(theta 1) だとする。

16
00:01:24,660 --> 00:01:31,696
そして theta 1 は実数だ。

17
00:01:31,696 --> 00:01:39,202
では、theta 1 をこの位置にして最急降下法を初期化したとする。

18
00:01:39,202 --> 00:01:46,989
だから、関数上のこの点から開始すると想像してほしい。

19
00:01:46,989 --> 00:01:56,935
最急降下法が行うのは更新だ。 theta 1 は theta 1 - alpha 掛ける dd

20
00:01:56,935 --> 00:02:04,694
theta 1 J(theta 1) として更新される。

21
00:02:04,694 --> 00:02:11,636
余談だが、この導関数項について、私が表記方法を変えていて、

22
00:02:11,636 --> 00:02:16,132
なぜこうした偏微分の記号を使わないのかと疑問を持つ人がいるだろう。
もしこの記号の違いの意味が分からなくても、

23
00:02:16,132 --> 00:02:20,523
偏微分の記号と dd theta の違いは気にしないでいい。

24
00:02:20,523 --> 00:02:24,491
厳密には数学上、これは偏微分、そしてこれは常微分と言う。

25
00:02:24,491 --> 00:02:28,299
違いは、関数 J のパラメータの数による。

26
00:02:28,299 --> 00:02:32,428
しかしこれは数学上の独特の表現なので、この授業の目的としては、

27
00:02:32,428 --> 00:02:36,768
こうした 偏微分の記号と dd theta の表記は
まったく同じものと考えてよいので、何が違うのかと、心配しないでいい。

28
00:02:36,768 --> 00:02:41,056
授業の中では、数学的に正確な表記を使うが、

29
00:02:41,056 --> 00:02:45,190
実際の目的上は、こうした表記は実際には同じことだ。

30
00:02:45,360 --> 00:02:49,627
では、この式が何をするか見てみよう。

31
00:02:49,627 --> 00:02:54,293
まず、計算するのは、この微分の導関数だ。
皆さんが微分積分の微分を見たことがあるかわからないが、

32
00:02:54,293 --> 00:02:58,666
微分というのは、ここでは、基本的にこういうことだ。

33
00:02:58,666 --> 00:03:02,877
この点に対する接線を取る。
この直線、赤い線、ちょうどこの関数に接しているが、

34
00:03:02,877 --> 00:03:06,976
その赤い線の勾配を見てみようということ、それが微分だ。

35
00:03:06,976 --> 00:03:11,352
つまり、関数に対する接線の勾配は何かということだ。

36
00:03:11,352 --> 00:03:15,563
そして線の勾配はもちろん単に

37
00:03:15,563 --> 00:03:20,789
高さを幅で割ったものだ。

38
00:03:20,789 --> 00:03:28,378
さて、この線は正の勾配なので、正の微分となる。

39
00:03:28,378 --> 00:03:36,258
だから、theta の更新は theta 1 := theta 1 - alpha 掛ける 何かの正の値。

40
00:03:36,258 --> 00:03:43,103
ちなみに、alpha、学習率、は常に正の値だ。

41
00:03:43,103 --> 00:03:47,932
だから theta 1 に対し、それを (theta 1 - 何か) で更新する。

42
00:03:47,932 --> 00:03:52,644
結果的に theta 1 を左に移動することになる。

43
00:03:52,644 --> 00:03:57,473
theta 1 を減少すると、ご覧のようにこれが正しい動作だ、

44
00:03:57,473 --> 00:04:02,582
この方向に移動して、ここの最小値に近づいたのだから。

45
00:04:02,582 --> 00:04:08,115
最急降下法は、ここまでは正しく動作しているようだ。
では、別の例を見てみよう。同じ関数 J を使う。

46
00:04:08,115 --> 00:04:13,787
同じ関数 J(theta 1) を書く。

47
00:04:13,787 --> 00:04:19,181
では、仮に今回はパラメータをこの左ところで初期化したとする。
theta 1 はここになる。

48
00:04:19,181 --> 00:04:24,161
表面のここに点を追加する。

49
00:04:24,161 --> 00:04:29,567
では、導関数項、dd theta 1 J(theta 1) をこの点の値で評価すると、

50
00:04:29,567 --> 00:04:35,035
この線の勾配になる。この導関数項はこの線の勾配となる。

51
00:04:35,035 --> 00:04:42,745
しかしこの線は下に傾斜しているので、この線は負の勾配だ。

52
00:04:42,745 --> 00:04:48,718
言い換えると、この関数は負の導関数を持つということだ。
つまりその点で、負の勾配となっていることを意味する。

53
00:04:48,718 --> 00:04:54,770
だから、これは 0 以下となりる。
そして theta を更新すると、

54
00:04:54,770 --> 00:05:02,840
theta が (theta - alpha 掛ける負の値) で更新される。

55
00:05:02,840 --> 00:05:07,881
つまり (theta 1 - 負の値) となり、これは theta の値を増やすることを意味する。

56
00:05:07,881 --> 00:05:13,106
いいかな。負の値の引き算だから、実際には theta に加算することになり、

57
00:05:13,106 --> 00:05:17,900
結果的に theta の値が増加するという意味だ。

58
00:05:17,900 --> 00:05:23,002
この開始点から theta を増加させるというのは、これも意図通りの動作のようだ。

59
00:05:23,002 --> 00:05:28,335
最小値に近づくことになるので。

60
00:05:28,335 --> 00:05:33,874
以上、これにより導関数項がやることを理解できたと思う。

61
00:05:33,874 --> 00:05:39,956
では次に学習率 alpha を見て、それが何をしているのか理解しよう。

62
00:05:39,956 --> 00:05:46,641
さて、ここに最急降下法の更新ルールがある。この式だ。さて、どうなるだろうか。

63
00:05:46,641 --> 00:05:52,845
もし alpha が小さすぎり、あるいは alpha が大きすぎた場合はどうなるか。

64
00:05:52,845 --> 00:05:59,583
この最初の例は、alpha が小さすぎた場合にどうなるかだ。
これが関数 J、J(theta) 。

65
00:05:59,583 --> 00:06:04,230
ではここから始めよう。alpha が小さすぎる場合、どうなるかというと、

66
00:06:04,230 --> 00:06:09,322
更新がある小さな値で乗算される。
すると結果的にこのような小さなステップとなる。

67
00:06:09,322 --> 00:06:13,841
これが一つのステップだ。

68
00:06:13,841 --> 00:06:18,870
次にこの新しい点からもう一歩進むが、alpha が小さすぎるので

69
00:06:18,870 --> 00:06:25,342
また小さなステップとなる。だから、学習率が小さすぎる場合、

70
00:06:25,342 --> 00:06:30,589
結果的にこうした小さな小さなステップで最小値に進むことになるので、

71
00:06:30,589 --> 00:06:35,837
最小値に着くには、非常に多くのステップが必要になる。

72
00:06:35,837 --> 00:06:41,019
alpha が小さすぎると、小さな小さなステップで進むので、
最急降下法の実行に時間がかかる。

73
00:06:41,019 --> 00:06:45,829
さらに、大域的最小点に近づくには、非常に多くのステップが必要になる。

74
00:06:45,829 --> 00:06:52,236
では、alpha が大きすぎる場合はどうだろう。これが関数 J(theta) だ。

75
00:06:52,236 --> 00:06:57,590
実は alpha が大きすぎる場合、最急降下法が最小値を通り越して収束しなかったり、

76
00:06:57,590 --> 00:07:03,362
さらには発散してしまうことがある。つまり、こういうことだ。
ここから始めるとする。これは既に最小値に近い。

77
00:07:03,362 --> 00:07:08,647
この場合、微分は右向きになる。
しかし alpha が大きすぎると、大きなステップで進むので、

78
00:07:08,686 --> 00:07:14,140
このように大きなステップで進むので、結果的に目的関数の値が悪化する。

79
00:07:14,140 --> 00:07:20,051
なぜなら、この値で始めたのに、結果的に値が悪くなるからだ。

80
00:07:20,051 --> 00:07:25,190
今度は微分が左を向くので、theta の値は減少する。しかし学習率が大きすぎると、

81
00:07:25,190 --> 00:07:29,792
大きなステップでここからそこまで飛び移り、ここに落ち着く。

82
00:07:29,792 --> 00:07:35,372
そして学習率が大きすぎると、また大きなステップで次回も更新されて、

83
00:07:35,372 --> 00:07:41,034
行き過ぎ、行き過ぎ、を繰り返すことになり、

84
00:07:41,034 --> 00:07:46,765
ご覧のように実際には最小値からどんどん遠ざかる。だから alpha が大きすぎると、

85
00:07:46,765 --> 00:07:51,905
収束しなかったり、さらには発散してしまうことがある。
ではもう一つ皆さんに質問がある。

86
00:07:51,905 --> 00:07:56,057
これは少し難問だ。私が最初にこれを学んだ時、

87
00:07:56,057 --> 00:08:00,005
実はこれを理解するのにかなり時間がかかった。
もし最初に設定した theta 1 が、既に局所的最小値だったらどうだろう。

88
00:08:00,005 --> 00:08:04,106
最急降下法の一ステップで何が起きるだろう。

89
00:08:04,106 --> 00:08:10,857
では、theta 1 を局所的最小値で初期化したとする。

90
00:08:10,857 --> 00:08:16,713
だから theta 1 の初期値がここで、

91
00:08:16,713 --> 00:08:22,718
それは既に局所的最適値、局所的最小値だ。実は、局所的最適値では

92
00:08:22,718 --> 00:08:28,796
微分は 0 となる。これが勾配、ここが接点なので、

93
00:08:28,796 --> 00:08:35,528
この線の勾配は 0 と等しくなり、この導関数項も 0 となる。

94
00:08:35,528 --> 00:08:40,941
だから、最急降下法の更新では、theta 1 := theta 1- alpha 掛ける 0。

95
00:08:40,941 --> 00:08:46,284
だから、この意味は、もし既に局所的最小値にある場合は、

96
00:08:46,284 --> 00:08:51,222
theta 1 は変わらないということだ。

97
00:08:51,222 --> 00:08:56,132
なぜなら、更新は theta 1 := theta 1 となるからだ。

98
00:08:56,132 --> 00:09:00,694
パラメータが既に局所的最小値の場合、
最急降下法の一ステップはまったく何も変えない。

99
00:09:00,694 --> 00:09:05,257
つまり、パラメータは変わらない。これは望ましいことだ。

100
00:09:05,257 --> 00:09:09,706
なぜなら、解が局所的最適値に留まるからだ。これは、なぜ最急降下法が収束して

101
00:09:09,706 --> 00:09:14,326
学習率が固定されていても局所的最小値に落ち着くのかの説明にもなる。

102
00:09:14,326 --> 00:09:21,550
では例を見てみよう。これが目的関数 J(theta) だ。

103
00:09:21,550 --> 00:09:26,811
これを最小化したいとする。
そしてアルゴリズムを、最急降下法のアルゴリズムをここで初期化したとする。

104
00:09:26,811 --> 00:09:32,080
このマゼンタ色の点だ。最急降下法で更新を一ステップ実行すると、

105
00:09:32,080 --> 00:09:36,941
例えば、この点に移動したとする。なぜならそこの微分はかなり急だからだ。

106
00:09:36,941 --> 00:09:42,051
さて、今はこの緑の点にあり、最急降下法でまた一ステップ更新したとする。

107
00:09:42,051 --> 00:09:47,036
お気づきの通り、微分、つまり勾配は、緑の点ではそれほど急ではない。

108
00:09:47,036 --> 00:09:51,959
あそこのマゼンタ色の点と比較して。

109
00:09:51,959 --> 00:09:56,883
なぜなら、最小値に近づくにつれ、微分が 0 にどんどん近づくからだ。

110
00:09:56,883 --> 00:10:01,794
さて、最急降下法で一ステップ更新した後、新しい微分はやや小さくなった。

111
00:10:01,794 --> 00:10:06,635
最急降下法でまた一ステップ更新したい。

112
00:10:06,635 --> 00:10:11,598
当然、マゼンタ色の点からの場合よりいくぶんさらに小さなステップで、
この緑の点から進む。

113
00:10:11,598 --> 00:10:16,038
さて、新しい点、赤い点、は大域的最小値にさらに近くなっている。

114
00:10:16,038 --> 00:10:21,229
だから、ここの微分は緑の点の時よりさらに小さくなる。

115
00:10:21,229 --> 00:10:26,420
そして、最急降下法でまた一ステップ更新すると、導関数項はさらに小さくなるので

116
00:10:26,420 --> 00:10:31,360
theta 1 の更新の大きさもさらに小さくなり、ステップもまたこのように小さくなり、

117
00:10:31,360 --> 00:10:39,145
このように最急降下法が進むにつれ、自動的にステップがどんどん小さくなり、

118
00:10:39,145 --> 00:10:46,343
やがて非常に小さなステップで進むことになる。

119
00:10:46,343 --> 00:10:52,737
そして最後には、局所的最小値に収束する。さて、復習だ。

120
00:10:52,737 --> 00:10:57,716
最急降下法では局所的最小値に近づくにつれ、最急降下法は自動的に

121
00:10:57,716 --> 00:11:02,634
小さなステップを取るようになり、それは局所的最小値に近づくにつれ、

122
00:11:02,634 --> 00:11:07,122
定義により局所的最小値での微分は 0 に等しくなるからだ。

123
00:11:07,122 --> 00:11:12,408
だから、局所的最小値に近づくにつれ、この導関数項も自動的に小さくなり、

124
00:11:12,408 --> 00:11:16,957
このため最急降下法のステップも自動的に小さくなる。

125
00:11:16,957 --> 00:11:21,506
これが最急降下法の仕組みなので、経過的に alpha を減少させる実際の必要はない。

126
00:11:21,506 --> 00:11:26,258
さて、これが最急降下法アルゴリズムだ。

127
00:11:26,258 --> 00:11:30,713
そしてこれを使えば、どのような目的関数 J でも最小化を試みることができる。
単に線形回帰で定義された目的関数に限らない。

128
00:11:30,713 --> 00:11:34,738
次のビデオでは、

129
00:11:34,738 --> 00:11:38,549
関数 J を元通りの線形回帰の目的関数に戻す。

130
00:11:38,549 --> 00:11:43,057
前に定義した、二乗誤差の目的関数だ。

131
00:11:43,057 --> 00:11:47,351
そして最急降下法と二乗誤差の目的関数を組み合わせる。

132
00:11:47,351 --> 00:11:50,948
これによって最初の学習アルゴリズム、線形回帰アルゴリズムが構築できる。
