1
00:00:00,360 --> 00:00:01,753
ここまでで、いくつかの

2
00:00:01,760 --> 00:00:04,097
異なる学習アルゴリズムを見てきた。

3
00:00:04,097 --> 00:00:06,504
線形回帰とロジスティック回帰。

4
00:00:06,510 --> 00:00:08,583
それらは様々な問題でうまく機能する。

5
00:00:08,583 --> 00:00:09,684
だがそれをある種の機械学習の問題に

6
00:00:09,684 --> 00:00:11,903
適用しようとする時に、

7
00:00:11,903 --> 00:00:13,889
オーバーフィッティングと呼ばれる問題に遭遇して

8
00:00:13,900 --> 00:00:18,052
非常に低い性能しか発揮なくなることがある。

9
00:00:18,052 --> 00:00:18,866
このビデオで私がやりたいのは、

10
00:00:18,866 --> 00:00:20,393
あなたに、オーバーフィッティングの問題について

11
00:00:20,393 --> 00:00:22,400
説明し、

12
00:00:22,400 --> 00:00:24,083
そして続く一連の

13
00:00:24,083 --> 00:00:25,861
ビデオで、

14
00:00:25,861 --> 00:00:27,759
正規化（regularization）と呼ばれるテクニックを議論していきたい。

15
00:00:27,760 --> 00:00:29,787
正規化は、

16
00:00:29,787 --> 00:00:31,529
オーバーフィッティングの問題を

17
00:00:31,529 --> 00:00:33,607
緩和・減らすことを可能にしてくれて、

18
00:00:33,607 --> 00:00:36,844
これらの学習アルゴリズムをもっと良く機能するようにしてくれることがある。

19
00:00:36,860 --> 00:00:39,607
さて、オーバーフィッティングとは何だろう？

20
00:00:39,607 --> 00:00:41,616
説明のための例として、

21
00:00:41,620 --> 00:00:44,030
線形回帰で住居の価格を

22
00:00:44,050 --> 00:00:46,146
住居の広さの関数として

23
00:00:46,146 --> 00:00:47,123
予測したい、という例を

24
00:00:47,123 --> 00:00:50,730
引き続き用いることにしよう。

25
00:00:50,730 --> 00:00:51,870
一つできることは、

26
00:00:51,910 --> 00:00:53,620
このデータに線形関数を

27
00:00:53,620 --> 00:00:54,892
フィッティングする、というのがある。

28
00:00:54,892 --> 00:00:56,296
それをすると、こんな感じの

29
00:00:56,296 --> 00:00:58,913
データにフィットした直線が得られる。

30
00:00:58,913 --> 00:01:01,012
だがこれは良いモデルではない。

31
00:01:01,012 --> 00:01:02,543
データを見てみると、

32
00:01:02,560 --> 00:01:04,100
住居の広さが上昇するにつれて、

33
00:01:04,100 --> 00:01:06,274
住居の価格はだんだんと台地になっていく、

34
00:01:06,274 --> 00:01:08,268
あるいはある種、平坦になっていく、右に移動していくに連れて、

35
00:01:08,270 --> 00:01:11,721
というのが、明らかに見て取れる。

36
00:01:11,740 --> 00:01:14,020
だからこのアルゴリズムはトレーニングセットに

37
00:01:14,020 --> 00:01:15,898
あまりフィットできない。

38
00:01:15,898 --> 00:01:19,166
この問題をアンダーフィッティングと呼ぶ。

39
00:01:19,180 --> 00:01:20,494
また別の用語としては、

40
00:01:20,500 --> 00:01:24,666
このアルゴリズムは高バイアスだ、とも言う。

41
00:01:25,140 --> 00:01:26,841
これらはどちらも、だいたいは

42
00:01:26,890 --> 00:01:30,760
トレーニングデータにすら、あまり良くはフィッティングてきていない、という意味だ。

43
00:01:30,760 --> 00:01:32,328
用語はある種、

44
00:01:32,328 --> 00:01:34,515
歴史的、技術的なものだが、

45
00:01:34,515 --> 00:01:36,109
そのアイデアは、

46
00:01:36,110 --> 00:01:37,303
データに直線を

47
00:01:37,303 --> 00:01:38,909
フィッティングさせると、

48
00:01:38,920 --> 00:01:40,290
アルゴリズムは、住居の価格が

49
00:01:40,330 --> 00:01:42,638
広さに応じて線形に変わる、という

50
00:01:42,638 --> 00:01:44,633
強い前提あるいはバイアスを

51
00:01:44,650 --> 00:01:46,339
置く、ということを意味し、

52
00:01:46,339 --> 00:01:49,988
しかもデータはそれに反している。

53
00:01:50,000 --> 00:01:51,281
反対の証拠があるにも関わらず、

54
00:01:51,290 --> 00:01:54,174
前提がバイアスされたままで

55
00:01:54,174 --> 00:01:55,413
直線にフィットさせることに

56
00:01:55,440 --> 00:01:56,974
固執したままだと

57
00:01:56,974 --> 00:02:00,638
データにあまりよくフィットしない、という状態になる。

58
00:02:00,638 --> 00:02:02,173
ここで、真ん中は、

59
00:02:02,210 --> 00:02:04,626
二次関数でフィットさせるとする。

60
00:02:04,626 --> 00:02:06,222
このデータセットに、二次関数で

61
00:02:06,222 --> 00:02:07,793
フィッティングすると、

62
00:02:07,810 --> 00:02:10,211
こんな種類のカーブを得る。

63
00:02:10,211 --> 00:02:14,361
これはかなり良さそうだ。

64
00:02:14,361 --> 00:02:17,543
そして反対側の極端として、例えば四次の多項式でデータにフィッティングする、というのが考えられる。

65
00:02:17,550 --> 00:02:19,442
この場合5つのパラメータがある。

66
00:02:19,470 --> 00:02:22,926
シータ0からシータ4まで。

67
00:02:22,926 --> 00:02:23,926
そして、実際に曲線を

68
00:02:23,926 --> 00:02:26,727
5つのすべての手本を通るようにフィッティングできてしまう。

69
00:02:26,727 --> 00:02:29,507
たとえばこんな感じの曲線が得られる。

70
00:02:31,260 --> 00:02:32,454
これは一方では

71
00:02:32,460 --> 00:02:33,791
訓練セットに

72
00:02:33,791 --> 00:02:35,052
フィッティングするという点では

73
00:02:35,052 --> 00:02:36,291
非常に良く動作するように見える、

74
00:02:36,291 --> 00:02:38,269
少なくともデータの上をすべて通るのだから。

75
00:02:38,270 --> 00:02:40,284
だがまたこれは、非常にうねうねした曲線でもある。

76
00:02:40,300 --> 00:02:41,660
つまり上に行ったり下に行ったり

77
00:02:41,660 --> 00:02:43,430
あちこち通るので、住居の価格を予測するのには

78
00:02:43,430 --> 00:02:46,996
それほど良さそうには思えない。

79
00:02:47,000 --> 00:02:48,924
だから、この問題を

80
00:02:48,924 --> 00:02:51,967
オーバーフィッティングと呼んでいて、

81
00:02:51,970 --> 00:02:53,165
また別の用語としては、

82
00:02:53,170 --> 00:02:57,304
このアルゴリズムは高バリアンスだ、とも言う。

83
00:02:57,890 --> 00:02:59,951
この高バリアンスという用語はまた別の

84
00:02:59,951 --> 00:03:02,110
歴史的、技術的なものだ。

85
00:03:02,130 --> 00:03:03,797
だが感覚的には、

86
00:03:03,800 --> 00:03:05,080
そんなに高次の多項式に

87
00:03:05,080 --> 00:03:07,326
フィッティングすると、

88
00:03:07,330 --> 00:03:08,603
その場合、仮説は

89
00:03:08,620 --> 00:03:09,584
まるでどんな関数にも

90
00:03:09,584 --> 00:03:11,995
フィッティングできてしまい、

91
00:03:11,995 --> 00:03:14,159
可能な仮説の数が単純に多すぎる、

92
00:03:14,159 --> 00:03:16,601
あまりにも変わりすぎるという問題に直面する。

93
00:03:16,610 --> 00:03:18,052
そして私たちは、仮説を

94
00:03:18,052 --> 00:03:19,279
良いものだけに制約できるほどには

95
00:03:19,279 --> 00:03:22,714
データを持っていない。だからこれはオーバーフィッティングと呼ばれる。

96
00:03:22,740 --> 00:03:24,340
そして真ん中。これには名前はないが、

97
00:03:24,350 --> 00:03:26,990
これはちょうど良い、という状態。

98
00:03:26,990 --> 00:03:29,911
二次の多項式、二次関数は

99
00:03:29,911 --> 00:03:32,559
このデータにフィッティングするのに、ただ単にちょうど良さそう。

100
00:03:32,559 --> 00:03:34,684
少し復習しておこう。

101
00:03:34,690 --> 00:03:37,042
オーバーフィッティングの問題は

102
00:03:37,042 --> 00:03:38,258
特徴量が多すぎる時に起こり、

103
00:03:38,258 --> 00:03:40,729
その場合、仮説は訓練セットには

104
00:03:40,729 --> 00:03:43,881
非常に良くフィットするように学習する。

105
00:03:43,881 --> 00:03:46,023
だから、あなたのコスト関数は

106
00:03:46,023 --> 00:03:47,344
実際に非常に 0 に近くなり、

107
00:03:47,344 --> 00:03:48,446
時には完全に0に

108
00:03:48,446 --> 00:03:50,750
一致することもある。

109
00:03:50,750 --> 00:03:52,063
だがそれは、こんなカーブに

110
00:03:52,063 --> 00:03:53,950
なっているかもしれない。

111
00:03:53,950 --> 00:03:55,314
つまりトレーニングセットにフィットさせようと

112
00:03:55,314 --> 00:03:57,103
あまりにも頑張りすぎていて、

113
00:03:57,110 --> 00:03:59,233
それは新しい手本に対しての一般化に

114
00:03:59,250 --> 00:04:01,117
失敗していて、だから新しい手本に対して

115
00:04:01,120 --> 00:04:03,018
価格を予測することにも失敗してしまう。

116
00:04:03,050 --> 00:04:04,337
ここで一般化という

117
00:04:04,350 --> 00:04:06,853
用語は、新規の手本に対して

118
00:04:06,853 --> 00:04:10,868
仮説がどれだけ良く適用できるか、という意味だ。

119
00:04:10,868 --> 00:04:12,274
それは、トレーニングセットにない、

120
00:04:12,320 --> 00:04:16,467
住居のデータについて、ということ。

121
00:04:16,600 --> 00:04:17,910
このスライドでは線形回帰の場合での

122
00:04:17,910 --> 00:04:20,802
オーバーフィッティングを見てきた。

123
00:04:20,810 --> 00:04:24,182
似たようなことはロジスティック回帰でもありうる。

124
00:04:24,190 --> 00:04:26,090
これは2つの特徴量、x1とx2による

125
00:04:26,090 --> 00:04:28,871
ロジスティック回帰の例だ。

126
00:04:28,910 --> 00:04:30,136
私たちができることは、

127
00:04:30,140 --> 00:04:31,522
このような単純な仮説に

128
00:04:31,522 --> 00:04:34,518
ロジスティック回帰をフィッティングする、というのが考えられる。

129
00:04:34,530 --> 00:04:38,076
ここでgはいつも通りsigmoid関数だ。

130
00:04:38,120 --> 00:04:39,334
そしてこうすると、結局

131
00:04:39,334 --> 00:04:41,593
単なる直線で

132
00:04:41,600 --> 00:04:42,923
陽性と陰性の手本を分割しようと試みる

133
00:04:42,923 --> 00:04:45,713
仮説が得られることになる。

134
00:04:45,713 --> 00:04:49,071
これは仮説にフィットしているようには見えない。

135
00:04:49,100 --> 00:04:50,659
つまり再びこれも、

136
00:04:50,659 --> 00:04:52,577
アンダーフィッティングしている例、あるいは

137
00:04:52,577 --> 00:04:56,040
仮説が高バイアスになっている例だ。

138
00:04:56,210 --> 00:04:57,504
対照的に、もしあなたの特徴量に

139
00:04:57,504 --> 00:04:59,146
これらの二次の項を

140
00:04:59,170 --> 00:05:01,032
追加したら、

141
00:05:01,032 --> 00:05:02,613
その場合はこんな感じの

142
00:05:02,613 --> 00:05:05,620
決定境界が得られるだろう。

143
00:05:05,620 --> 00:05:07,784
そしてこれは、かなり良くデータにフィットしているように見える。

144
00:05:07,784 --> 00:05:10,838
このトレーニングセットに対しては

145
00:05:10,860 --> 00:05:13,991
もっとも良さそうに見える。

146
00:05:14,010 --> 00:05:15,157
そして最後に、反対側の極端として、

147
00:05:15,170 --> 00:05:16,169
非常に高次の多項式に

148
00:05:16,169 --> 00:05:18,207
フィッティングすると、

149
00:05:18,207 --> 00:05:20,036
もし仮に多くの高次の項の特徴量を

150
00:05:20,036 --> 00:05:22,461
生成したとすると、

151
00:05:22,490 --> 00:05:24,730
その場合、ロジスティック回帰は

152
00:05:24,750 --> 00:05:26,551
自身をゆがめて、

153
00:05:26,560 --> 00:05:28,233
頑張って、

154
00:05:28,233 --> 00:05:31,742
あなたのトレーニングデータにフィットするように

155
00:05:31,742 --> 00:05:33,013
決定境界を見つけようとする、あるいは

156
00:05:33,030 --> 00:05:35,006
非常に長くなるように自分自身をゆがめて

157
00:05:35,006 --> 00:05:37,689
トレーニング手本一つ一つに非常に良くフィットするように進んでいく。

158
00:05:37,700 --> 00:05:39,547
そしてもし特徴量の x1 と x2 が

159
00:05:39,547 --> 00:05:39,547


160
00:05:39,550 --> 00:05:41,435
例えば癌の予測を

161
00:05:41,435 --> 00:05:43,350
表すとすると、

162
00:05:43,390 --> 00:05:46,448
癌が悪性か、乳腺腫瘍になりそうかの予想を提供するなら、

163
00:05:46,448 --> 00:05:47,988
これも、非常に良い仮説とは思えない、

164
00:05:47,988 --> 00:05:51,893
予測に使うには。

165
00:05:51,930 --> 00:05:53,463
だからここでも、

166
00:05:53,463 --> 00:05:55,432
これはオーバーフィッティングの例で、

167
00:05:55,432 --> 00:05:57,128
そして仮説は高バリアンスに

168
00:05:57,128 --> 00:05:59,403
なっていて、たぶん新規の手本に対して

169
00:05:59,403 --> 00:06:04,243
うまく一般化されそうにない。

170
00:06:04,560 --> 00:06:06,158
このコースの後の方で、

171
00:06:06,158 --> 00:06:08,453
学習アルゴリズムが

172
00:06:08,460 --> 00:06:09,794
おかしな状態になった時の

173
00:06:09,810 --> 00:06:11,490
デバッグや診断の話をする時に、

174
00:06:11,490 --> 00:06:13,297
どういう時にオーバーフィッティングが起きて

175
00:06:13,297 --> 00:06:14,953
どういう時にアンダーフィッティングが起きているかを識別する

176
00:06:14,953 --> 00:06:17,503
具体的なツールを紹介する。

177
00:06:17,503 --> 00:06:18,775
だが現時点では、まずオーバーフィッティングの問題が

178
00:06:18,780 --> 00:06:20,342
既に起こっているとして、

179
00:06:20,360 --> 00:06:22,206
それにどう対処したらいいか？を

180
00:06:22,250 --> 00:06:24,864
議論しよう。

181
00:06:24,864 --> 00:06:26,640
以前の例では、

182
00:06:26,660 --> 00:06:28,701
1次元とか2次元のデータだった。

183
00:06:28,701 --> 00:06:31,335
だから仮説をプロットして何が起きているのかを見ることができて、

184
00:06:31,335 --> 00:06:34,612
適切な次数の仮説を選ぶことができた。

185
00:06:34,620 --> 00:06:36,836
だから以前の住居の価格の例では、

186
00:06:36,836 --> 00:06:38,405
仮説を単にプロットしてみて、

187
00:06:38,410 --> 00:06:40,597
それを見て

188
00:06:40,600 --> 00:06:41,628
それがある種の、

189
00:06:41,628 --> 00:06:42,830
非常にうねうねと波打った関数で

190
00:06:42,830 --> 00:06:46,339
すべての住居の価格の点を通るようにフィッティングしてしまっているのがわかるかもしれない。

191
00:06:46,339 --> 00:06:47,701
そしてこのような図を用いて

192
00:06:47,740 --> 00:06:50,667
多項式の適切な次数を選ぶことができるかもしれない。

193
00:06:50,680 --> 00:06:54,166
つまり仮説をプロットするのは、

194
00:06:54,166 --> 00:06:55,728
どの次数を使うべきかを決める

195
00:06:55,750 --> 00:06:58,160
一つの方法になりえる。

196
00:06:58,160 --> 00:07:00,163
だがそれがいつも使えるわけではない。

197
00:07:00,180 --> 00:07:02,019
実のところ、多くの場合において、私たちは

198
00:07:02,019 --> 00:07:06,075
多くの特徴量を含む学習問題を扱うことになる。

199
00:07:06,075 --> 00:07:07,563
そしてまた、これは単に

200
00:07:07,563 --> 00:07:10,599
多項式の次数を選ぶだけの問題ではない。

201
00:07:10,630 --> 00:07:12,147
そして実際、

202
00:07:12,170 --> 00:07:13,779
そんなに多くの特徴量がある時には

203
00:07:13,779 --> 00:07:15,593
データをプロットするのがもっと難しくなり

204
00:07:15,630 --> 00:07:17,698
だからそれを可視化するのも

205
00:07:17,710 --> 00:07:19,211
もっと困難となる、

206
00:07:19,211 --> 00:07:22,396
どの特徴量を維持すべきか、そうでないかを決めるために使うプロットを。

207
00:07:22,420 --> 00:07:24,142
具体的には、住居の価格を予測するのに

208
00:07:24,160 --> 00:07:27,849
様々な特徴量を使って予測する、ということがありうる。

209
00:07:27,880 --> 00:07:31,373
そしてこれらすべての特徴量が、役に立ちそうに思えることがある。

210
00:07:31,373 --> 00:07:32,609
だが、多くの特徴量がある場合で、

211
00:07:32,609 --> 00:07:34,123
そしてトレーニングデータが

212
00:07:34,123 --> 00:07:35,820
少ない時には、

213
00:07:35,840 --> 00:07:37,776
その場合はオーバーフィッティングが問題になりうる。

214
00:07:37,776 --> 00:07:39,180
オーバーフィッティングの問題を解決するために

215
00:07:39,180 --> 00:07:40,651
取れる手段としては、

216
00:07:40,651 --> 00:07:43,780
大きく2つの選択肢がある。

217
00:07:43,780 --> 00:07:45,759
最初の選択肢は

218
00:07:45,770 --> 00:07:47,976
特徴量の数を減らすこと。

219
00:07:47,990 --> 00:07:49,337
具体的には、できることは、

220
00:07:49,337 --> 00:07:51,383
人力で特徴量のリストを

221
00:07:51,383 --> 00:07:53,236
見ていって、

222
00:07:53,236 --> 00:07:54,894
どれがもっとも重要な特徴量群で

223
00:07:54,894 --> 00:07:57,256
どれが維持すべきものか

224
00:07:57,256 --> 00:07:58,476
どれが捨て去るべきものかを

225
00:07:58,476 --> 00:08:01,844
決定する、というのが考えられる。

226
00:08:01,844 --> 00:08:03,401
このコースの後半では、

227
00:08:03,401 --> 00:08:06,018
モデル選択のアルゴリズムについて議論する。

228
00:08:06,040 --> 00:08:08,361
それは自動的に、どの特徴量を

229
00:08:08,361 --> 00:08:09,788
維持しつづけて、どの特徴量を

230
00:08:09,800 --> 00:08:12,500
捨て去るかを決定するアルゴリズムだ。

231
00:08:12,500 --> 00:08:13,987
この特徴量の数を減らす、

232
00:08:13,987 --> 00:08:15,562
というアイデアは

233
00:08:15,562 --> 00:08:17,853
うまく行くこともあり、オーバーフィッティングを低減しうる。

234
00:08:17,853 --> 00:08:19,383
そしてモデル選択の話をする時に

235
00:08:19,383 --> 00:08:22,534
この話もより詳細に行う。

236
00:08:22,534 --> 00:08:24,386
だがここでは、その欠点の話をしていこう。

237
00:08:24,386 --> 00:08:25,603
特徴量をいくつか捨てることは、

238
00:08:25,603 --> 00:08:27,010
それは同時に問題について自分の持っている

239
00:08:27,370 --> 00:08:30,615
情報を捨てることでもある。

240
00:08:30,650 --> 00:08:31,942
例えば、それらの特徴量がすべて

241
00:08:31,942 --> 00:08:33,760
実際に住居の価格を予測するのに

242
00:08:33,780 --> 00:08:35,050
有用だったとしよう。

243
00:08:35,070 --> 00:08:36,636
その場合私たちは本当は

244
00:08:36,640 --> 00:08:37,687
情報の一部を捨てる、つまり

245
00:08:37,687 --> 00:08:40,990
特徴量を捨てたくはないだろう。

246
00:08:41,540 --> 00:08:44,515
二番目の選択肢は、

247
00:08:44,515 --> 00:08:45,995
これは続く一連のビデオで扱うが、

248
00:08:46,010 --> 00:08:49,268
それは正規化(regularization)を行う、ということ。

249
00:08:49,268 --> 00:08:50,390
ここでは、すべての特徴量を

250
00:08:50,390 --> 00:08:52,579
維持しつづけて、

251
00:08:52,579 --> 00:08:55,063
その代わりパラメータシータjの

252
00:08:55,063 --> 00:08:56,506
倍率を下げる。

253
00:08:56,520 --> 00:08:58,745
そしてこの手法は

254
00:08:58,750 --> 00:09:00,690
後で見るように、以下のようなケースではうまく行く：

255
00:09:00,690 --> 00:09:01,925
それは多くの特徴量を持っていて、

256
00:09:01,925 --> 00:09:03,822
その各々が少しずつ

257
00:09:03,822 --> 00:09:05,502
Yの値を予測するのに

258
00:09:05,502 --> 00:09:07,723
貢献している、という場合だ。

259
00:09:07,740 --> 00:09:10,283
ちょうど住居の価格の予測の例で見たように。

260
00:09:10,283 --> 00:09:11,413
そこでは多くの特徴量がありえて、

261
00:09:11,413 --> 00:09:12,720
それらはおのおの、

262
00:09:12,750 --> 00:09:16,902
いくらか有用で、それらを捨てたくない。

263
00:09:16,930 --> 00:09:19,247
以上は正規化のアイデアを

264
00:09:19,250 --> 00:09:22,790
非常に高いレベルで記述したものだ。

265
00:09:22,790 --> 00:09:24,354
だからこれらの詳細のすべてが

266
00:09:24,360 --> 00:09:26,763
明確にわかりやすいわけではないと、わかっている。

267
00:09:26,763 --> 00:09:28,316
だが次のビデオから、

268
00:09:28,316 --> 00:09:30,960
厳密にどう正規化を適用するのか、

269
00:09:30,960 --> 00:09:35,117
正規化とは何を意味するのかを定式化していく。

270
00:09:35,140 --> 00:09:36,810
その時には、私たちはこれを

271
00:09:36,810 --> 00:09:38,310
どのように用いれば、

272
00:09:38,310 --> 00:09:40,412
学習アルゴリズムをうまく機能させ、

273
00:09:40,412 --> 00:09:42,460
オーバーフィッティングを避けられるか、知るだろう。
