1
00:00:00,454 --> 00:00:02,208
前回のビデオで、

2
00:00:02,238 --> 00:00:04,581
最急降下法のアルゴリズムについて話し、

3
00:00:04,581 --> 00:00:06,005
線形回帰のモデルと

4
00:00:06,005 --> 00:00:09,071
二乗誤差のコスト関数についても話した。

5
00:00:09,071 --> 00:00:10,822
このビデオでは、

6
00:00:10,822 --> 00:00:12,695
最急降下法とコスト関数を

7
00:00:12,695 --> 00:00:14,672
両方合わせて、

8
00:00:14,672 --> 00:00:16,652
そこから直線を私たちのデータにフィッティングするための

9
00:00:16,652 --> 00:00:19,431
線形回帰のアルゴリズムを作ろう。

10
00:00:21,001 --> 00:00:22,743
これは前回のビデオで

11
00:00:22,743 --> 00:00:25,077
私たちが作ったものだ。

12
00:00:25,077 --> 00:00:27,095
これが私たちの最急降下法アルゴリズムで

13
00:00:27,095 --> 00:00:29,197
もはやお馴染みとなっただろう。

14
00:00:29,197 --> 00:00:31,199
そしてあなたは、線形回帰のモデルと

15
00:00:31,199 --> 00:00:36,461
線形の仮説、そして二乗誤差のコスト関数も分かっているはず。

16
00:00:36,461 --> 00:00:38,612
今回やろうとしているのは、

17
00:00:38,612 --> 00:00:42,288
最急降下法を、二乗誤差のコスト関数を

18
00:00:44,519 --> 00:00:47,537
最小化するために適用する、ということ。

19
00:00:47,891 --> 00:00:49,381
最急降下法を

20
00:00:49,381 --> 00:00:50,896
適用するためには、

21
00:00:50,896 --> 00:00:52,695
このコード片を書くためには、

22
00:00:52,695 --> 00:00:54,191
私たちが必要とするキーとなる項は

23
00:00:54,191 --> 00:00:58,384
この微分の項だ。

24
00:00:59,692 --> 00:01:00,798
つまり、私たちはこの偏微分の項が

25
00:01:00,798 --> 00:01:02,830
何になるのかを知る必要がある。

26
00:01:02,830 --> 00:01:04,478
そのためには

27
00:01:04,478 --> 00:01:06,249
コスト関数Jの定義を

28
00:01:06,249 --> 00:01:08,418
代入して、

29
00:01:08,418 --> 00:01:11,074
結局、これに、

30
00:01:12,613 --> 00:01:15,156
1 から m までの和を取ることの

31
00:01:15,156 --> 00:01:18,856
この二乗誤差の

32
00:01:20,456 --> 00:01:22,023
コスト関数の項。

33
00:01:22,023 --> 00:01:23,794
ここでやったことは単に、

34
00:01:23,794 --> 00:01:25,538
コスト関数の定義を

35
00:01:25,538 --> 00:01:28,275
ここに代入しただけ。

36
00:01:28,275 --> 00:01:30,563
そしてさらに単純化して、

37
00:01:30,563 --> 00:01:34,136
結局イコール、

38
00:01:34,136 --> 00:01:36,983
これに、 1 から m までの和をとることの、

39
00:01:36,983 --> 00:01:40,609
シータ0 足す シータ1 xi

40
00:01:41,163 --> 00:01:43,427
マイナス yi の二乗。

41
00:01:43,427 --> 00:01:44,777
ここでやったのは

42
00:01:44,777 --> 00:01:46,983
仮説の定義を

43
00:01:46,983 --> 00:01:49,376
ここに代入した。

44
00:01:49,622 --> 00:01:51,669
結局、私たちは

45
00:01:51,669 --> 00:01:52,523
2 つのケースについての

46
00:01:52,523 --> 00:01:54,011
偏微分がどうなるかを知る必要があるわけだ。

47
00:01:54,011 --> 00:01:55,284
j=0 のケースと、

48
00:01:55,284 --> 00:01:57,006
j=1 のケース。

49
00:01:57,006 --> 00:01:58,547
つまり、シータ0と

50
00:01:58,547 --> 00:02:00,767
シータ1についての偏微分の

51
00:02:00,767 --> 00:02:04,115
両方が知りたい。

52
00:02:04,115 --> 00:02:07,012
そして直接答えを書いてしまうことにする。

53
00:02:07,012 --> 00:02:10,996
この最初の項は整理すると

54
00:02:10,996 --> 00:02:14,218
1/m 和を取ることの

55
00:02:14,218 --> 00:02:16,446
トレーニングセット全体で、

56
00:02:16,446 --> 00:02:21,146
この x(i)、、、引くことの y(i)。

57
00:02:21,146 --> 00:02:23,951
そしてこの項は、

58
00:02:23,951 --> 00:02:26,186
シータ1による偏微分は、結局

59
00:02:26,186 --> 00:02:34,954
この項となる。、、、y(i) と、掛ける x(i)。

60
00:02:35,031 --> 00:02:36,187
いいかな。

61
00:02:36,402 --> 00:02:38,690
これらの偏微分を計算するには

62
00:02:38,690 --> 00:02:40,761
つまり、この式から

63
00:02:40,761 --> 00:02:43,406
これらの式を導くには

64
00:02:43,406 --> 00:02:46,414
偏微分の項を計算するのに

65
00:02:46,414 --> 00:02:51,090
いくらか多変数の解析学が必要だ。

66
00:02:51,090 --> 00:02:53,118
もし解析学を知っているなら

67
00:02:53,118 --> 00:02:54,825
どうぞご自由に導出してみてほしい、

68
00:02:54,825 --> 00:02:57,060
実際に微分してみて、

69
00:02:57,060 --> 00:02:59,853
前に挙げた答えと一致するかチェックしてみてほしい。

70
00:02:59,853 --> 00:03:00,855
だが、もしあなたが

71
00:03:00,855 --> 00:03:02,611
あまり解析学に詳しくなくても、

72
00:03:02,611 --> 00:03:04,235
気にしないでいい。

73
00:03:04,235 --> 00:03:06,260
単にこれらの式を受け入れてもらえば

74
00:03:06,260 --> 00:03:08,025
何の問題もない。

75
00:03:08,025 --> 00:03:09,462
そうすれば解析学や、

76
00:03:09,462 --> 00:03:10,340
そういうことを一切知らなくても、

77
00:03:10,340 --> 00:03:14,551
宿題はできるし、最急降下法も実装できるし、それを活用することもできる。

78
00:03:14,551 --> 00:03:16,520
だがこれらの定義に従えば、

79
00:03:16,520 --> 00:03:18,156
微分とは何か、ということに

80
00:03:18,156 --> 00:03:19,993
立ち戻ってみれば、

81
00:03:19,993 --> 00:03:21,316
それはコスト関数 J の

82
00:03:21,316 --> 00:03:22,889
単なる勾配に過ぎない。

83
00:03:22,889 --> 00:03:24,724
さて、今や、それらすべてを

84
00:03:24,724 --> 00:03:27,157
最急降下法のアルゴリズムに代入することができる。

85
00:03:27,157 --> 00:03:28,794
これが最急降下法、

86
00:03:28,794 --> 00:03:30,309
または回帰で、

87
00:03:30,309 --> 00:03:32,971
これを収束するまで繰り返す。

88
00:03:32,971 --> 00:03:35,552
シータ0とシータ1を

89
00:03:35,552 --> 00:03:37,163
同じ マイナス アルファ 掛ける

90
00:03:37,163 --> 00:03:39,591
微分の項。

91
00:03:39,591 --> 00:03:43,202
つまりこの項。

92
00:03:43,202 --> 00:03:46,854
これが線形回帰のアルゴリズムだ。

93
00:03:46,854 --> 00:03:52,696
最初の方のこの項は

94
00:03:52,696 --> 00:03:54,495
もちろん

95
00:03:54,495 --> 00:03:56,071
対応するシータ0による偏微分の項だ。

96
00:03:56,071 --> 00:03:59,995
前のスライドで見た。

97
00:03:59,995 --> 00:04:03,454
そしてこの二番目の項のこれ、

98
00:04:03,454 --> 00:04:04,199
この項は

99
00:04:04,199 --> 00:04:05,947
シータ1による偏微分の項で、

100
00:04:05,947 --> 00:04:11,188
これも前のスライドで既に出した。

101
00:04:11,188 --> 00:04:13,582
ここで少し注意がある。

102
00:04:13,582 --> 00:04:15,485
最急降下法を実装する時は

103
00:04:15,485 --> 00:04:17,067
実装の詳細な話だが、

104
00:04:17,067 --> 00:04:19,248
シータ0とシータ1を同時に更新するように

105
00:04:19,248 --> 00:04:24,452
実装しなくてはならない。

106
00:04:24,452 --> 00:04:28,270
では最急降下法がどう機能するか見てみよう。

107
00:04:28,270 --> 00:04:29,447
最急降下法で解く場合に疑われる問題としては

108
00:04:29,447 --> 00:04:32,839
局所最適に落ち着いてしまう、ということがある。

109
00:04:32,839 --> 00:04:34,433
最初に最急降下法を説明した時に

110
00:04:34,449 --> 00:04:36,136
あなたにこの図を見せた、

111
00:04:36,136 --> 00:04:37,724
これは表面の丘を

112
00:04:37,724 --> 00:04:38,788
下っていくもの。

113
00:04:38,788 --> 00:04:40,120
そしてどこから始めるかによって

114
00:04:40,120 --> 00:04:42,872
異なる局所最適解に落ち着く可能性がある。

115
00:04:42,872 --> 00:04:44,846
つまり、ここに行き着く場合もあれば、ここに行き着く場合もありうる。

116
00:04:44,846 --> 00:04:46,958
しかし、線形回帰の場合の

117
00:04:46,958 --> 00:04:49,025
コスト関数の gradient は、

118
00:04:49,025 --> 00:04:50,791
いつも弓形の関数で

119
00:04:50,791 --> 00:04:52,754
このような形に

120
00:04:52,754 --> 00:04:55,305
必ずなる。

121
00:04:55,305 --> 00:04:57,573
これを表す専門用語があって、

122
00:04:57,573 --> 00:05:01,163
それは凸型関数と呼ばれる。

123
00:05:02,825 --> 00:05:04,920
私はわざわざ

124
00:05:04,920 --> 00:05:06,561
凸関数の正式な定義を

125
00:05:06,561 --> 00:05:09,557
説明する気はない。

126
00:05:09,557 --> 00:05:11,310
だが直感的には、凸関数は

127
00:05:11,310 --> 00:05:14,793
弓形の関数ということだ。

128
00:05:14,793 --> 00:05:18,006
そしてつまり、この関数は

129
00:05:18,006 --> 00:05:19,738
大域最適な解以外の

130
00:05:19,738 --> 00:05:22,433
局所最適解は持たない。

131
00:05:22,433 --> 00:05:24,265
そしてこの種類のコスト関数、

132
00:05:24,265 --> 00:05:25,590
それは線形回帰を適用する時はいつもそうだが、

133
00:05:25,590 --> 00:05:27,695
この種のコスト関数に最急降下法を適用すると、

134
00:05:27,695 --> 00:05:29,201
いつも大域最適な結果を返す、

135
00:05:29,201 --> 00:05:33,623
なぜなら、大域最適以外の局所最適が存在しないから。

136
00:05:33,623 --> 00:05:37,272
では次に、実際のアルゴリズムの動作を見てみよう。

137
00:05:38,026 --> 00:05:40,085
いつも通り、ここに仮説関数の

138
00:05:40,085 --> 00:05:42,182
プロットがある。

139
00:05:42,182 --> 00:05:45,024
そしてコスト関数 J もある。

140
00:05:45,763 --> 00:05:47,011
そしてパラメータの初期値を

141
00:05:47,011 --> 00:05:49,687
ここに初期化するとしよう。

142
00:05:49,687 --> 00:05:51,652
普通はパラメータは

143
00:05:51,652 --> 00:05:53,590
0, 0 で初期化するものだ、

144
00:05:53,590 --> 00:05:56,287
シータ0 = シータ1 = 0 で。

145
00:05:56,287 --> 00:05:58,331
しかし、このプレゼンでは

146
00:05:58,331 --> 00:05:59,948
例示のため、

147
00:05:59,948 --> 00:06:02,615
シータ0 をだいたい 900 に、

148
00:06:02,615 --> 00:06:06,831
シータ1をだいたい -0.1 とした。

149
00:06:06,831 --> 00:06:09,791
これはつまり、

150
00:06:09,791 --> 00:06:12,022
h(x) が -900 -0.1x となることを

151
00:06:12,022 --> 00:06:15,859
意味する。

152
00:06:15,859 --> 00:06:19,341
それはこの線。コスト関数上のここに対応する。

153
00:06:19,341 --> 00:06:20,491
ここで、最急降下法を一ステップだけ

154
00:06:20,491 --> 00:06:22,163
実行してみると、

155
00:06:22,163 --> 00:06:24,298
この、ここの点から

156
00:06:24,298 --> 00:06:27,065
少しだけ左下の

157
00:06:27,065 --> 00:06:29,564
この点へと

158
00:06:29,564 --> 00:06:31,732
動くことになる。

159
00:06:31,732 --> 00:06:35,279
そしてお気付きのように、直線が少しだけ変わる。

160
00:06:35,279 --> 00:06:36,547
さらに最急降下法を進めていくと、

161
00:06:36,547 --> 00:06:41,425
左の直線は変化していくことになる。

162
00:06:41,425 --> 00:06:42,376
いいかな？

163
00:06:42,376 --> 00:06:43,804
そしてコスト関数の方も

164
00:06:43,804 --> 00:06:47,544
新しい点に動く。

165
00:06:47,544 --> 00:06:48,745
そしてさらに最急降下法の

166
00:06:48,745 --> 00:06:50,697
ステップを進めていくと、おそらく

167
00:06:50,697 --> 00:06:53,058
コストを下っていく、

168
00:06:53,058 --> 00:06:55,079
だからパラメータはこの軌跡を

169
00:06:55,079 --> 00:06:58,062
辿ることになる。

170
00:06:58,062 --> 00:07:00,368
そして左を見ていると、これに対応した仮説は

171
00:07:00,368 --> 00:07:04,025
どんどん

172
00:07:04,025 --> 00:07:04,912
データに良く

173
00:07:04,912 --> 00:07:06,429
フィットしていくように見える、

174
00:07:06,429 --> 00:07:09,987
そして最終的に、

175
00:07:09,987 --> 00:07:12,663
大域最小値に落ち着く。

176
00:07:12,663 --> 00:07:16,189
そしてこの大域最小値は

177
00:07:16,189 --> 00:07:20,506
この仮説に対応していて、それはデータに良くフィットしたものとなっている。

178
00:07:20,922 --> 00:07:23,605
以上が最急降下法だ。

179
00:07:23,605 --> 00:07:24,969
それを実行して

180
00:07:24,969 --> 00:07:27,302
家の価格のデータセットに

181
00:07:27,302 --> 00:07:31,359
良く適合させることができた。

182
00:07:31,359 --> 00:07:34,108
そしてここからは、それを予測に使える。

183
00:07:34,108 --> 00:07:35,284
たとえばあなたの友人が

184
00:07:35,284 --> 00:07:36,452
1250 平方フィートの家を

185
00:07:36,452 --> 00:07:39,116
持っていたとしよう。

186
00:07:39,116 --> 00:07:40,684
今やあなたは、(グラフから)値を読み取って

187
00:07:40,684 --> 00:07:42,090
例えば、そうだなぁ、

188
00:07:42,090 --> 00:07:43,188
250,000 ドルで家が売れる、

189
00:07:43,188 --> 00:07:47,159
と分かる。

190
00:07:48,606 --> 00:07:49,982
最後に、これにもう一つの名前を

191
00:07:49,982 --> 00:07:51,930
与えておく。

192
00:07:51,930 --> 00:07:52,991
今見てきたこのアルゴリズムは

193
00:07:52,991 --> 00:07:55,030
バッチ最急降下法と

194
00:07:55,030 --> 00:07:57,074
呼ばれる場合もある。

195
00:07:57,074 --> 00:07:58,781
機械学習においては、

196
00:07:58,781 --> 00:08:00,203
と言っても私たちのように機械学習屋でも

197
00:08:00,203 --> 00:08:02,050
そうでもない人もいるが、

198
00:08:02,050 --> 00:08:04,381
機械学習屋にはそう呼ぶ人がいる。

199
00:08:04,381 --> 00:08:06,634
「バッチ」最急降下法という言葉には

200
00:08:06,634 --> 00:08:08,212
最急降下法の各ステップで

201
00:08:08,212 --> 00:08:09,551
すべてのトレーニング手本を見る、

202
00:08:09,551 --> 00:08:11,164
という事実を

203
00:08:11,164 --> 00:08:13,649
指している。

204
00:08:13,649 --> 00:08:15,783
最急降下法においては、

205
00:08:15,783 --> 00:08:18,875
偏微分を計算する時、

206
00:08:18,875 --> 00:08:21,307
これらの和を計算する。

207
00:08:21,307 --> 00:08:22,210
つまり最急降下法の

208
00:08:22,210 --> 00:08:23,510
各ステップで、

209
00:08:23,510 --> 00:08:25,278
このような計算をすることになるが、

210
00:08:25,278 --> 00:08:28,434
この和は M 個のトレーニング手本に渡って取る。

211
00:08:28,434 --> 00:08:29,835
つまりバッチ最急降下法という言葉が示すのは、

212
00:08:29,835 --> 00:08:31,195
トレーニング手本を

213
00:08:31,195 --> 00:08:33,193
トレーニング手本の全体を見る、

214
00:08:33,193 --> 00:08:34,559
という事実を指して、だ。

215
00:08:34,559 --> 00:08:35,742
これは実は、

216
00:08:35,742 --> 00:08:36,915
あまり良い名前ではない。

217
00:08:36,915 --> 00:08:39,444
だがこれが機械学習屋が呼ぶ名前だ。

218
00:08:39,444 --> 00:08:40,958
そして時には、

219
00:08:40,958 --> 00:08:42,665
違うバージョンの

220
00:08:42,665 --> 00:08:43,918
最急降下法で、

221
00:08:43,918 --> 00:08:45,969
バッチでなく、

222
00:08:45,969 --> 00:08:47,752
一度にトレーニング手本全体を見るのではなく

223
00:08:47,752 --> 00:08:49,722
トレーニング手本の

224
00:08:49,722 --> 00:08:51,529
小さなサブセットを見る、というものがある。

225
00:08:51,529 --> 00:08:54,874
私たちもコースの後半でそのバージョンを見ることになる。

226
00:08:54,874 --> 00:08:56,195
だが今のところ、たった今学んだアルゴリズムを

227
00:08:56,195 --> 00:08:57,410
使っていく。

228
00:08:57,410 --> 00:08:58,759
つまりここではバッチ最急降下法を使う。

229
00:08:58,759 --> 00:09:01,159
いまや最急降下法または線形回帰を

230
00:09:01,159 --> 00:09:04,149
どう実装するかを知ったわけだ。

231
00:09:05,856 --> 00:09:08,672
以上が、最急降下法による線形回帰だ。

232
00:09:09,349 --> 00:09:11,747
もしあなたが上級線形代数のコースを

233
00:09:11,747 --> 00:09:12,672
以前にとっていたら、、、

234
00:09:12,672 --> 00:09:14,206
あなたがたの内、何人かは

235
00:09:14,206 --> 00:09:16,279
線形代数の上級をとっていると思うけれど、

236
00:09:16,295 --> 00:09:17,678
その人たちはコスト関数 J の

237
00:09:17,678 --> 00:09:19,754
最小値を数値的に解く

238
00:09:19,754 --> 00:09:20,914
解法を知っているかもしれない、

239
00:09:20,914 --> 00:09:22,561
最急降下法のような繰り返しの

240
00:09:22,561 --> 00:09:25,604
アルゴリズムなしで解く方法だ。

241
00:09:25,604 --> 00:09:27,154
このコースでも後ほど、

242
00:09:27,154 --> 00:09:28,098
直接コスト関数の

243
00:09:28,098 --> 00:09:29,753
最小値を解く方法で、

244
00:09:29,753 --> 00:09:31,076
最急降下法の複数ステップなしでも使えるものを

245
00:09:31,076 --> 00:09:33,791
議論する。

246
00:09:33,791 --> 00:09:37,656
この別解は、正規方程式と呼ばれている。

247
00:09:37,656 --> 00:09:39,167
だが、その手法について聞いたことがあるなら

248
00:09:39,167 --> 00:09:40,141
知ってるかもしれないが、

249
00:09:40,141 --> 00:09:41,622
最急降下法の方が

250
00:09:41,622 --> 00:09:43,774
大きなデータセットに対しては

251
00:09:43,774 --> 00:09:45,008
正規方程式よりも良くスケールする。

252
00:09:45,008 --> 00:09:47,315
そして今や最急降下法について

253
00:09:47,315 --> 00:09:48,756
私たちは知ったわけだ。

254
00:09:48,756 --> 00:09:49,922
私たちはさまざまな文脈でこれを

255
00:09:49,922 --> 00:09:51,387
用いることができ、

256
00:09:51,387 --> 00:09:54,849
それはまた、様々な機械学習の問題でも用いることができるということだ。

257
00:09:54,849 --> 00:09:57,138
つまり、おめでとう！

258
00:09:57,138 --> 00:10:00,317
あなたは最初の機械学習のアルゴリズムを学んだのだ。

259
00:10:00,317 --> 00:10:02,508
のちほど、あなたに

260
00:10:02,508 --> 00:10:03,659
最急降下法を実装してもらう

261
00:10:03,659 --> 00:10:05,068
練習問題を設けてある。

262
00:10:05,068 --> 00:10:07,071
そこではきっと、これらのアルゴリズムがうまく行くのを自分で体験できることだろう。

263
00:10:07,071 --> 00:10:08,955
だがその前に、

264
00:10:08,955 --> 00:10:10,587
まず次の一連のビデオで、

265
00:10:10,587 --> 00:10:11,919
まず言っておきたいのは、

266
00:10:11,919 --> 00:10:13,223
最急降下法アルゴリズムの

267
00:10:13,223 --> 00:10:15,724
一般化についてで、

268
00:10:15,724 --> 00:10:16,935
それは最急降下法をますます

269
00:10:16,935 --> 00:10:18,403
パワフルにしてくれる。

270
00:10:18,403 --> 00:59:59,000
それは次のビデオでお話する。
