1
00:00:00,302 --> 00:00:01,883
このビデオでは、

2
00:00:01,883 --> 00:00:03,948
正規方程式について議論する。

3
00:00:03,948 --> 00:00:05,660
それはいくつかの線形回帰問題で、

4
00:00:05,660 --> 00:00:06,981
パラメータのシータを求める

5
00:00:06,981 --> 00:00:10,879
より良い解法になる。

6
00:00:10,879 --> 00:00:10,879


7
00:00:10,879 --> 00:00:13,096
具体的には、ここまで、

8
00:00:13,096 --> 00:00:14,399
線形回帰に用いてきた

9
00:00:14,399 --> 00:00:16,042
アルゴリズムは、最急降下法だった。

10
00:00:16,042 --> 00:00:17,823
それはコスト関数 J の

11
00:00:17,823 --> 00:00:19,410
シータを最小化するために

12
00:00:19,410 --> 00:00:21,354
この繰り返しのアルゴリズムを用いるが、

13
00:00:21,354 --> 00:00:23,792
これは多くのステップ、

14
00:00:23,792 --> 00:00:26,410
複数回の繰り返しの最急降下が、

15
00:00:26,410 --> 00:00:28,259
大域的最小値に収束するまで

16
00:00:28,259 --> 00:00:30,396
必要だった。

17
00:00:30,396 --> 00:00:32,563
対照的に、正規方程式は

18
00:00:32,563 --> 00:00:34,413
シータを解析的に

19
00:00:34,413 --> 00:00:36,986
解く方法なので、

20
00:00:36,986 --> 00:00:38,761
この繰り返しアルゴリズムを

21
00:00:38,761 --> 00:00:40,294
走らせなくても、

22
00:00:40,294 --> 00:00:41,365
シータの最適の値を

23
00:00:41,365 --> 00:00:42,791
一度に解くことができる。

24
00:00:42,791 --> 00:00:44,403
つまり、

25
00:00:44,403 --> 00:00:46,096
基本的には1ステップで

26
00:00:46,096 --> 00:00:48,136
ここの最適値が得られる。

27
00:00:49,136 --> 00:00:51,947
しかし、正規方程式には

28
00:00:52,209 --> 00:00:54,442
いくらかの利点と

29
00:00:54,442 --> 00:00:56,024
いくらかの欠点がある。

30
00:00:56,024 --> 00:00:57,817
だがそれについて議論したり

31
00:00:57,903 --> 00:00:59,426
それを使うべき時を議論する前に

32
00:00:59,426 --> 00:01:02,539
この手法が、何をやるのかの直感を得ることにしよう。

33
00:01:02,539 --> 00:01:04,633
この回の探索的な例として、

34
00:01:04,633 --> 00:01:06,120
非常に単純化した

35
00:01:06,120 --> 00:01:07,505
コスト関数Jのシータを

36
00:01:07,505 --> 00:01:09,291
考えてみよう。

37
00:01:09,291 --> 00:01:11,958
これは単なる実数シータの関数だ。

38
00:01:11,958 --> 00:01:13,642
つまり当面は、シータは

39
00:01:13,842 --> 00:01:16,615
単なるスカラーの値、生の値だと考えてみよう。

40
00:01:16,769 --> 00:01:18,918
ベクトルてはなく、単なる数だとする。

41
00:01:19,171 --> 00:01:24,595
実数のパラメータシータに関する二次関数の、コスト関数Jを考えてみる。

42
00:01:25,028 --> 00:01:27,420
つまりJのシータはこんな感じ。

43
00:01:27,851 --> 00:01:30,336
さて、どのように二次関数を最小化すればよいか？

44
00:01:30,720 --> 00:01:32,745
解析学をいくらか知っていれば、

45
00:01:32,858 --> 00:01:34,965
関数を最小化する方法は、

46
00:01:34,965 --> 00:01:36,628
微分を取って、

47
00:01:36,628 --> 00:01:38,991
その微分をイコール0とすることだと

48
00:01:38,991 --> 00:01:41,707
知っているかもしれない。

49
00:01:41,707 --> 00:01:44,721
だからJをシータに関して微分を取る。

50
00:01:44,797 --> 00:01:46,847
すると何かしらの式が得られるが、ここには書かない。

51
00:01:46,847 --> 00:01:49,161
微分をイコール0として、

52
00:01:49,161 --> 00:01:50,782
この結果、シータの値について

53
00:01:50,782 --> 00:01:53,503
解くことができ、

54
00:01:53,503 --> 00:01:57,866
その解が、Jのシータを最小化するシータだ。

55
00:01:57,866 --> 00:01:59,096
以上は、データが単なる実数の時の

56
00:01:59,096 --> 00:02:01,716
簡単な場合の話だ。

57
00:02:01,716 --> 00:02:04,272
私たちが現在関心がある問題においては、

58
00:02:04,929 --> 00:02:06,559
シータは単なる実数ではない。

59
00:02:06,559 --> 00:02:07,847
このn+1次元の

60
00:02:07,847 --> 00:02:11,986
パラメータベクトルだ。

61
00:02:11,986 --> 00:02:13,809
そしてコスト関数Jは

62
00:02:13,809 --> 00:02:15,742
このベクトル、あるいは

63
00:02:15,742 --> 00:02:17,501
シータ0からシータmまでの関数だ。

64
00:02:17,501 --> 00:02:18,924
そしてコスト関数は

65
00:02:18,924 --> 00:02:21,957
こんな感じだ。右側のような二次関数。

66
00:02:22,373 --> 00:02:25,712
コスト関数Jをどのように最小化するか？

67
00:02:25,712 --> 00:02:27,163
解析学を、

68
00:02:27,163 --> 00:02:29,377
もし知っていれば、

69
00:02:29,377 --> 00:02:30,709
これを行う一つの手としては

70
00:02:30,709 --> 00:02:38,604
Jの偏微分をとる、各パラメータシータjに関する。

71
00:02:38,604 --> 00:02:40,271
そして次に、それをすべてイコール0とする。

72
00:02:40,271 --> 00:02:41,394
そうすると、

73
00:02:41,394 --> 00:02:42,718
そしてシータ0、

74
00:02:42,718 --> 00:02:44,000
シータ1、、、とシータnまでの

75
00:02:44,000 --> 00:02:45,973
値を解くと、

76
00:02:45,973 --> 00:02:47,217
これがコスト関数Jを

77
00:02:47,217 --> 00:02:48,765
最小化するシータの値となる。

78
00:02:48,765 --> 00:02:50,878
ここで実際に

79
00:02:50,878 --> 00:02:52,176
解析をして

80
00:02:52,176 --> 00:02:53,597
パラメータの

81
00:02:53,597 --> 00:02:55,194
シータ0からシータnまで

82
00:02:55,194 --> 00:02:57,316
やっていけば、

83
00:02:57,316 --> 00:03:00,520
何かしら導出する方法が得られる。

84
00:03:00,520 --> 00:03:01,625
だがこのビデオで

85
00:03:01,625 --> 00:03:03,113
私は、実際の

86
00:03:03,113 --> 00:03:04,852
導出には

87
00:03:04,852 --> 00:03:06,297
踏み込まない。

88
00:03:06,297 --> 00:03:07,657
それは長くて多くが要求される。

89
00:03:07,657 --> 00:03:08,962
代わりに、私はあなたに、

90
00:03:08,962 --> 00:03:10,545
この過程を実装するのに必要なことを

91
00:03:10,545 --> 00:03:12,619
伝えたい。

92
00:03:12,619 --> 00:03:14,138
つまりあなたは

93
00:03:14,138 --> 00:03:15,511
偏微分項イコール0に

94
00:03:15,511 --> 00:03:16,892
対応したシータの値を

95
00:03:16,892 --> 00:03:19,273
解くことができる。

96
00:03:19,273 --> 00:03:21,733
あるいは、等価だが

97
00:03:21,733 --> 00:03:23,357
コスト関数Jのシータを

98
00:03:23,357 --> 00:03:25,901
最小化するシータの値を。

99
00:03:25,901 --> 00:03:27,283
私のいくつかのコメントは

100
00:03:27,283 --> 00:03:28,846
解析学に慣れ親しんだ人にしか

101
00:03:28,846 --> 00:03:29,914
分かりやすくない、という

102
00:03:29,914 --> 00:03:31,896
自覚はある。

103
00:03:31,896 --> 00:03:33,065
だから、もしあまり知らなくて、

104
00:03:33,065 --> 00:03:34,487
あまり解析学に親しみがなければ、

105
00:03:34,487 --> 00:03:36,354
気にしないで良い。

106
00:03:36,354 --> 00:03:37,404
アルゴリズムを実装して

107
00:03:37,404 --> 00:03:38,374
動かすために必要なことは

108
00:03:38,374 --> 00:03:41,358
この後にきちんと教える。

109
00:03:41,358 --> 00:03:42,585
ここで見ていく

110
00:03:42,585 --> 00:03:43,737
例として、

111
00:03:43,737 --> 00:03:46,339
m=4のトレーニング手本が

112
00:03:46,339 --> 00:03:49,056
あるとしよう。

113
00:03:50,409 --> 00:03:52,881
この正規方程式法を実装するために

114
00:03:52,881 --> 00:03:56,515
私は以下のようにする。

115
00:03:56,515 --> 00:03:57,640
データセットを持ってきて、

116
00:03:57,640 --> 00:04:00,375
さて、ここに4つのトレーニング手本がある。

117
00:04:00,375 --> 00:04:01,844
今回は、これら4つの手本が

118
00:04:01,844 --> 00:04:06,073
私の持っているデータのすべてだとしよう。

119
00:04:06,073 --> 00:04:07,890
私がやることは、

120
00:04:07,890 --> 00:04:09,007
データセットを持ってきて、

121
00:04:09,007 --> 00:04:11,289
追加の列を足す、これは

122
00:04:11,289 --> 00:04:14,579
追加の特徴量x0に対応する。

123
00:04:14,579 --> 00:04:15,967
それはいつもこの値、1を

124
00:04:15,967 --> 00:04:17,527
取る。

125
00:04:17,527 --> 00:04:18,681
そこで、

126
00:04:18,681 --> 00:04:19,943
Xと呼ばれる行列を

127
00:04:19,943 --> 00:04:22,638
構築する。

128
00:04:22,638 --> 00:04:24,632
それは、要するに、

129
00:04:24,632 --> 00:04:26,100
トレーニングデータのすべての特徴量を

130
00:04:26,100 --> 00:04:28,140
含んでいる。具体的には、

131
00:04:28,140 --> 00:04:31,528
これが、、、これが、

132
00:04:31,528 --> 00:04:33,743
特徴量のすべてだとして、

133
00:04:33,743 --> 00:04:34,797
これらの数をすべて持ってきて、

134
00:04:34,797 --> 00:04:37,777
この行列Xに入れる。いいかな？

135
00:04:37,777 --> 00:04:39,179
つまり、このデータを

136
00:04:39,179 --> 00:04:41,233
一回に一列ずつコピーする。

137
00:04:41,233 --> 00:04:45,962
そしてyにも同じことをする。

138
00:04:45,962 --> 00:04:47,087
予測したいと値を

139
00:04:47,087 --> 00:04:47,952
持ってきて、

140
00:04:47,952 --> 00:04:49,360
新しいベクトルを

141
00:04:49,360 --> 00:04:52,894
前と同様に構築して、

142
00:04:52,894 --> 00:04:55,440
ベクトルyと呼ぶ。

143
00:04:55,440 --> 00:04:58,038
つまりXは m 掛ける (n+1) 次元行列

144
00:04:59,653 --> 00:05:05,688
となる。そして

145
00:05:05,688 --> 00:05:07,490
Yはm次元ベクトルとなる。

146
00:05:07,490 --> 00:05:14,421
ここで

147
00:05:14,421 --> 00:05:16,624
mはトレーニング手本の数で、

148
00:05:16,984 --> 00:05:18,688
nは特徴量の数、

149
00:05:18,688 --> 00:05:20,713
n+1なのは、

150
00:05:20,713 --> 00:05:24,825
この追加の特徴量x0があるから。

151
00:05:24,825 --> 00:05:26,350
最後に行列Xと

152
00:05:26,350 --> 00:05:27,489
ベクトルYを

153
00:05:27,489 --> 00:05:28,595
持ってきて、

154
00:05:28,595 --> 00:05:31,065
そしてこれを計算し、

155
00:05:31,065 --> 00:05:32,419
それをシータに代入すると、、、

156
00:05:32,419 --> 00:05:34,440
(X転置 X)の逆行列

157
00:05:34,440 --> 00:05:36,516
掛けることの X転置 Y。

158
00:05:36,516 --> 00:05:38,583
これはコスト関数を最小化する

159
00:05:38,583 --> 00:05:42,559
シータを与える。

160
00:05:42,559 --> 00:05:43,436
このスライドでは

161
00:05:43,436 --> 00:05:44,416
多くのことをやった。

162
00:05:44,416 --> 00:05:47,514
具体的なデータセットの例を一つ使って、それを見てきた。

163
00:05:47,514 --> 00:05:49,241
そこで、以上をもっと一般的な形に

164
00:05:49,333 --> 00:05:50,770
書いておこう。

165
00:05:50,955 --> 00:05:53,418
その後で、このビデオの後半で

166
00:05:53,621 --> 00:05:56,531
この方程式について、もう少し解説する。

167
00:05:57,581 --> 00:06:00,687
これをどうやるのか、まだ完全に明確ではないだろう。

168
00:06:00,687 --> 00:06:02,129
一般的な場合は、

169
00:06:02,129 --> 00:06:04,124
m個のトレーニング手本があるとして、

170
00:06:04,124 --> 00:06:05,697
これは x1, y1,からxm, ymまでで、

171
00:06:05,697 --> 00:06:09,319
n個の特徴量があるとする。

172
00:06:09,319 --> 00:06:10,811
つまり各トレーニング手本、

173
00:06:10,811 --> 00:06:12,926
x(i)は、こんなベクトルで、

174
00:06:12,926 --> 00:06:16,297
n+1次元の特徴ベクトルだ。

175
00:06:16,943 --> 00:06:18,350
行列Xを構築する方法は、

176
00:06:18,350 --> 00:06:20,674
ところで、この行列はまた

177
00:06:20,674 --> 00:06:24,827
デザイン行列とも呼ばれているが、

178
00:06:24,827 --> 00:06:26,712
それは以下のように作る。

179
00:06:26,712 --> 00:06:28,640
各トレーニング手本に、こんな感じの

180
00:06:28,640 --> 00:06:30,549
特徴ベクトルを与える。

181
00:06:30,549 --> 00:06:34,491
ある種のn+1次元ベクトルだ。

182
00:06:34,491 --> 00:06:36,190
デザイン行列Xを構築する方法は

183
00:06:36,359 --> 00:06:39,734
単にこんな行列を作るだけだ。

184
00:06:39,734 --> 00:06:40,834
どうやるかというと

185
00:06:40,834 --> 00:06:42,109
最初のトレーニング手本を

186
00:06:42,109 --> 00:06:43,711
持ってきて、それはベクトルで、

187
00:06:43,711 --> 00:06:46,350
その転置を取る、

188
00:06:46,350 --> 00:06:48,692
するとこんな感じになる。

189
00:06:48,692 --> 00:06:50,250
こんな、横長の平たい形。

190
00:06:50,250 --> 00:06:55,153
このx1の転置を、デザイン行列の最初の行にする。

191
00:06:55,153 --> 00:06:56,225
次に二番目のトレーニング手本、x2を

192
00:06:56,225 --> 00:06:58,682
持ってきて、

193
00:06:58,682 --> 00:07:00,437
そしてそれを転置する。

194
00:07:00,437 --> 00:07:01,838
それをXの二番目の行に置く。

195
00:07:01,838 --> 00:07:04,068
以下同様に、最後のトレーニング手本まで

196
00:07:04,068 --> 00:07:07,206
降りていく。

197
00:07:07,206 --> 00:07:09,279
その転置を取ると、

198
00:07:09,279 --> 00:07:10,850
それが行列Xの

199
00:07:10,850 --> 00:07:12,665
最後の行となる。

200
00:07:12,665 --> 00:07:14,418
以上が、私の行列Xの作り方だ。

201
00:07:14,418 --> 00:07:17,129
Xは M掛ける N+1 次元の

202
00:07:17,129 --> 00:07:19,836
行列。

203
00:07:19,836 --> 00:07:21,953
具体例で考えよう。

204
00:07:21,953 --> 00:07:23,505
特徴量が1つだけだとしよう。

205
00:07:23,505 --> 00:07:24,670
本当に、たった一つだけの

206
00:07:24,670 --> 00:07:26,631
特徴量しかないとする、x0以外では。

207
00:07:26,631 --> 00:07:28,165
x0はいつもイコール1だった。

208
00:07:28,165 --> 00:07:30,376
つまり、今回の特徴ベクトル

209
00:07:30,376 --> 00:07:32,186
x(i)はイコール、

210
00:07:32,186 --> 00:07:33,878
この1とーーこれはx0だが、

211
00:07:33,878 --> 00:07:35,912
さらに実際の特徴量となる、

212
00:07:35,912 --> 00:07:37,662
例えば、住居の広さとか。

213
00:07:37,662 --> 00:07:40,947
そしてデザイン行列Xは、イコール、

214
00:07:40,947 --> 00:07:42,589
最初の行は、、、基本的には

215
00:07:42,589 --> 00:07:46,071
これを持ってきて、その転置を取る。

216
00:07:46,071 --> 00:07:51,644
だから結局、1とx(1)の1だ。

217
00:07:51,644 --> 00:07:53,309
二番目の行は、

218
00:07:53,309 --> 00:07:56,077
結局 1と、

219
00:07:56,077 --> 00:07:58,046
x(1)の2だ。

220
00:07:58,046 --> 00:07:59,046
以下同様にx(1)のmまで

221
00:07:59,046 --> 00:08:01,420
降りていく。

222
00:08:01,420 --> 00:08:03,084
最終的には、

223
00:08:03,084 --> 00:08:07,776
これは m掛ける2 次元の行列となる。

224
00:08:07,776 --> 00:08:08,821
以上が、Xを構築する

225
00:08:08,821 --> 00:08:11,251
やり方だ。

226
00:08:11,251 --> 00:08:13,886
そしてベクトルYは、、、

227
00:08:13,886 --> 00:08:15,487
たまにベクトルであることを明示するために

228
00:08:15,487 --> 00:08:16,541
Yの上に矢印を描くこともあるが、

229
00:08:16,541 --> 00:08:19,871
だいたいは、単にYと書く。

230
00:08:19,871 --> 00:08:21,182
いずれにしても、ベクトルYは、

231
00:08:21,182 --> 00:08:23,275
トレーニングセットの

232
00:08:23,275 --> 00:08:25,098
住居のすべてのラベル、

233
00:08:25,098 --> 00:08:27,076
すべての正解の価格を持ってきて、

234
00:08:27,076 --> 00:08:28,963
それを単にM次元のベクトルとして

235
00:08:28,963 --> 00:08:32,011
積み上げれば良い。

236
00:08:32,011 --> 00:08:34,511
それがYとなる。最後に、

237
00:08:34,511 --> 00:08:36,724
行列XとベクトルYを

238
00:08:36,724 --> 00:08:38,184
構築したら、

239
00:08:38,184 --> 00:08:40,887
シータを単に (X転置 X)^-1 掛ける、

240
00:08:40,887 --> 00:08:47,243
X転置 Yで、計算できる。

241
00:08:47,243 --> 00:08:49,356
ここで少し、

242
00:08:49,356 --> 00:08:51,348
この式の意味を確認しておきたい。

243
00:08:51,348 --> 00:08:52,242
どう実装するかわかるように。

244
00:08:52,242 --> 00:08:55,221
具体的には、この (X転置 X)^-1、 とは何か？

245
00:08:55,221 --> 00:08:57,903
(X転置 X)^-1 とは

246
00:08:57,903 --> 00:09:02,101
行列(X転置 X)の逆行列だ。

247
00:09:02,101 --> 00:09:04,498
具体的には、例えば、

248
00:09:04,498 --> 00:09:08,055
Aに、イコール

249
00:09:08,055 --> 00:09:11,120
X転置 掛ける X を代入する。

250
00:09:11,120 --> 00:09:12,542
X転置 は行列で、

251
00:09:12,542 --> 00:09:14,063
X転置 掛ける X は、

252
00:09:14,063 --> 00:09:15,305
また別の行列となる。

253
00:09:15,305 --> 00:09:17,560
これを行列Aと呼ぶ。

254
00:09:17,560 --> 00:09:19,968
次に、 (X転置 X)^-1は、

255
00:09:19,968 --> 00:09:22,352
行列Aの逆行列をとれば良い。いいかな？

256
00:09:23,245 --> 00:09:24,417
つまり、Aの逆行列。

257
00:09:26,025 --> 00:09:28,919
以上が、これを計算する方法だ。

258
00:09:28,919 --> 00:09:31,451
X転置 X を計算して、その逆行列を計算する。

259
00:09:31,451 --> 00:09:34,296
Octaveでどうやるのかはまだ話していない。

260
00:09:34,296 --> 00:09:35,941
これは後の一連のビデオでやるが、

261
00:09:35,941 --> 00:09:37,211
Octaveの

262
00:09:37,211 --> 00:09:39,073
プログラミング言語、

263
00:09:39,073 --> 00:09:40,652
あるいは似たような言語である

264
00:09:40,652 --> 00:09:42,957
matlabのプログラミング言語でも、

265
00:09:42,957 --> 00:09:46,937
この量を計算するコマンドは、

266
00:09:47,384 --> 00:09:50,326
(X転置 X)の逆行列

267
00:09:50,326 --> 00:09:52,537
掛ける X転置 Y は、以下のようになる。

268
00:09:52,537 --> 00:09:54,903
Octaveでは、 X'は

269
00:09:54,903 --> 00:09:58,354
Xの転置を表すのに使う記法だ。

270
00:09:58,354 --> 00:10:00,737
つまり、この赤の四角で囲まれた式は、

271
00:10:00,737 --> 00:10:03,588
それは X転置 掛ける Xを

272
00:10:03,588 --> 00:10:06,633
計算していて、

273
00:10:06,633 --> 00:10:08,551
pinvは行列の

274
00:10:08,551 --> 00:10:09,701
逆行列を計算する関数だ。

275
00:10:09,701 --> 00:10:11,818
つまりこれは、

276
00:10:11,818 --> 00:10:14,656
(X転置 X) の逆行列を計算する。

277
00:10:14,656 --> 00:10:16,453
そしてそれをXの転置に掛けて、

278
00:10:16,453 --> 00:10:18,267
それをさらにYに掛ける。

279
00:10:18,267 --> 00:10:19,712
以上でこの式の

280
00:10:19,712 --> 00:10:22,325
計算が終わる。

281
00:10:22,325 --> 00:10:24,369
この式の証明はしていないが。

282
00:10:24,369 --> 00:10:25,994
しかし、次のことを、数学的に示すことはできる。

283
00:10:25,994 --> 00:10:27,382
私はここでそれを

284
00:10:27,382 --> 00:10:28,537
やる気はないけれど、、、

285
00:10:28,537 --> 00:10:31,071
この式は、最適なシータの値を

286
00:10:31,071 --> 00:10:32,316
与える、ということを。

287
00:10:32,316 --> 00:10:34,865
ここで最適とは、その値をシータに入れると、

288
00:10:34,865 --> 00:10:36,512
そのシータこそが、まさに

289
00:10:36,512 --> 00:10:38,000
その線形回帰の

290
00:10:38,000 --> 00:10:40,169
コスト関数Jのシータを

291
00:10:40,169 --> 00:10:41,993
最小化するシータだという意味だ。

292
00:10:41,993 --> 00:10:44,530
最後に細かいことを一つ。以前のビデオで

293
00:10:44,530 --> 00:10:46,131
フィーチャースケーリングについて議論した。

294
00:10:46,131 --> 00:10:47,061
それは特徴量の範囲を

295
00:10:47,061 --> 00:10:48,878
だいたい似たような

296
00:10:48,878 --> 00:10:50,726
スケールにする、

297
00:10:50,726 --> 00:10:54,900
お互いにだいたい似たような値の範囲にする、というアイデアだった。

298
00:10:54,900 --> 00:10:56,872
もしあなたが、この正規方程式法を

299
00:10:56,872 --> 00:10:59,843
使うならば、その時は

300
00:10:59,843 --> 00:11:02,315
フィーチャースケーリングは不要だ。

301
00:11:02,315 --> 00:11:04,361
実際に以下のような場合でも構わない：

302
00:11:04,361 --> 00:11:06,094
例えば、ある特徴量x1が

303
00:11:06,094 --> 00:11:07,552
0から1の間で、

304
00:11:07,552 --> 00:11:08,846
ある特徴量x2が

305
00:11:08,846 --> 00:11:10,550
0から1000の範囲の間で、

306
00:11:10,550 --> 00:11:12,019
そしてある特徴量x3が

307
00:11:12,019 --> 00:11:14,159
0から10の-5乗などの

308
00:11:14,159 --> 00:11:15,822
範囲を取るとしても。

309
00:11:15,822 --> 00:11:17,263
もし正規方程式法を

310
00:11:17,263 --> 00:11:18,321
使うなら、

311
00:11:18,321 --> 00:11:20,296
これでも問題ない。

312
00:11:20,296 --> 00:11:21,550
フィーチャースケーリングは不要だ。

313
00:11:21,550 --> 00:11:22,740
もちろん、

314
00:11:22,740 --> 00:11:25,667
もし最急降下法を使う時は、

315
00:11:25,667 --> 00:11:27,814
その場合は、相変わらずフィーチャースケーリングは重要だが。

316
00:11:28,030 --> 00:11:31,020
最後に、どこで最急降下法を使い、

317
00:11:31,020 --> 00:11:33,273
どこで正規方程式法を使うべきか。

318
00:11:33,273 --> 00:11:35,800
これがそれらの長所と短所だ。

319
00:11:35,800 --> 00:11:38,305
m個のトレーニング手本があり、

320
00:11:38,305 --> 00:11:40,918
n個の特徴量があるとする。

321
00:11:40,918 --> 00:11:42,854
最急降下法の欠点の一つは

322
00:11:42,854 --> 00:11:46,015
学習率アルファを決める必要がある。

323
00:11:46,015 --> 00:11:47,374
そして、しばしばこれは、

324
00:11:47,374 --> 00:11:49,128
最急降下法を、異なる学習率アルファで

325
00:11:49,128 --> 00:11:51,154
再走行して、どれが最適かを確認しなくてはならない、という意味だ。

326
00:11:51,154 --> 00:11:54,274
これは追加の仕事で、追加の面倒だ。

327
00:11:54,274 --> 00:11:55,976
もう一つの最急降下法の欠点は、

328
00:11:55,976 --> 00:11:57,841
多くの繰り返しが必要、ということだ。

329
00:11:57,841 --> 00:11:59,346
だから場合によっては、

330
00:11:59,346 --> 00:12:00,839
非常に遅くなりうる。しかし、これに関しては

331
00:12:00,839 --> 00:12:04,391
もう少し続きがある。それはすぐ後で見ることになる。

332
00:12:04,391 --> 00:12:07,544
正規方程式では、学習率アルファを選ばなくて良い。

333
00:12:07,821 --> 00:12:11,208
だから本当に便利で、実装もシンプル。

334
00:12:11,208 --> 00:12:13,888
単に走らせるだけでよく、普通は正しく機能する。

335
00:12:13,888 --> 00:12:15,061
そして繰り返しの必要もない。

336
00:12:15,061 --> 00:12:16,129
つまりJのシータをプロットしたり

337
00:12:16,129 --> 00:12:17,456
収束をチェックしたり、

338
00:12:17,456 --> 00:12:20,497
それら追加の作業は不要だ。

339
00:12:20,497 --> 00:12:21,931
ここまで、天秤は正規方程式が好ましい方に

340
00:12:21,931 --> 00:12:23,846
傾いているように見える。

341
00:12:24,826 --> 00:12:27,085
ここで、正規方程式の欠点も述べておこう。

342
00:12:27,612 --> 00:12:29,435
それは最急降下法の利点でもある。

343
00:12:29,681 --> 00:12:31,447
最急降下法は、特徴量が

344
00:12:31,928 --> 00:12:34,698
非常に多くても、非常に良く機能する。

345
00:12:34,698 --> 00:12:36,168
だから、たとえ数百万個の

346
00:12:36,168 --> 00:12:37,812
特徴量でも、

347
00:12:37,812 --> 00:12:40,865
最急降下法を実行できて、それはかなり効率的だ。

348
00:12:40,865 --> 00:12:43,381
それは納得できる振る舞いをする。

349
00:12:43,381 --> 00:12:46,566
それとは対照的に、正規方程式では、

350
00:12:46,566 --> 00:12:48,014
パラメータのために、データを解くには、

351
00:12:48,014 --> 00:12:50,394
この項を解く必要がある。

352
00:12:50,394 --> 00:12:53,058
このX転置 Xの逆行列を計算する必要がある。

353
00:12:53,058 --> 00:12:56,328
この行列 X転置 X。

354
00:12:56,328 --> 00:13:00,206
これは n 掛ける n 行列となる。ここでnは特徴量の数。

355
00:13:00,770 --> 00:13:02,947
これは、X転置の次元と

356
00:13:02,947 --> 00:13:03,917
Xの次元を

357
00:13:03,917 --> 00:13:05,529
見れば分かるように、

358
00:13:05,529 --> 00:13:07,024
この二つを掛けあわせると、

359
00:13:07,024 --> 00:13:08,749
積の次元は、

360
00:13:08,749 --> 00:13:10,983
行列 X転置 X は

361
00:13:10,983 --> 00:13:13,727
n 掛ける n 次元の行列となる、

362
00:13:13,727 --> 00:13:15,853
ここで nは特徴量の数。

363
00:13:15,853 --> 00:13:18,641
そしてほとんどの実装では、

364
00:13:18,641 --> 00:13:20,990
行列の逆行列の計算コストは

365
00:13:20,990 --> 00:13:23,087
だいたい行列の次元の

366
00:13:23,087 --> 00:13:25,707
三乗になる。

367
00:13:25,707 --> 00:13:28,180
だからこの逆行列のコストは

368
00:13:28,180 --> 00:13:29,964
だいたい3乗のオーダーとなる。

369
00:13:29,964 --> 00:13:31,213
時には、nの三乗より多少高速なこともあるが、

370
00:13:31,213 --> 00:13:35,050
私たちの目的では、似たようなものだ。

371
00:13:35,489 --> 00:13:36,605
つまりn, 特徴量の数が非常に大きくなると、

372
00:13:37,643 --> 00:13:39,025
この量を計算するのに

373
00:13:39,025 --> 00:13:40,570
遅くなる可能性があり、

374
00:13:40,570 --> 00:13:44,289
正規方程式法は、実際さらに遅くなる。

375
00:13:44,289 --> 00:13:45,491
だから、nが大きい時には

376
00:13:45,491 --> 00:13:47,622
私はふつう

377
00:13:47,622 --> 00:13:49,490
最急降下法を使う。

378
00:13:49,490 --> 00:13:51,872
この3乗の時間がもったいない。

379
00:13:51,872 --> 00:13:53,525
だが、nが相対的に小さければ、

380
00:13:53,525 --> 00:13:57,395
正規方程式はパラメータを解く、より良い方法だ。

381
00:13:57,395 --> 00:13:59,080
小さいとか大きいとは、どういう意味か？

382
00:13:59,080 --> 00:14:00,741
うーん、nが100とかの

383
00:14:00,741 --> 00:14:02,130
オーダーなら、

384
00:14:02,130 --> 00:14:03,822
100x100行列の逆行列を求めることは、

385
00:14:03,822 --> 00:14:06,539
現代のコンピューティングの水準なら、問題ない。

386
00:14:06,539 --> 00:14:10,966
もしnが1000なら？まだ正規方程式法を使う。

387
00:14:10,966 --> 00:14:12,583
1000x1000行列の逆行列を求めるのは、

388
00:14:12,583 --> 00:14:15,408
現代のコンピュータなら、実際は非常に速い。

389
00:14:15,408 --> 00:14:18,406
nが1万なら？このくらいから迷い始める。

390
00:14:18,406 --> 00:14:20,618
1万行x1万行 の、行列の逆行列を求めるのは、

391
00:14:20,618 --> 00:14:22,208
ある程度、遅くなり始める。

392
00:14:22,208 --> 00:14:23,471
だから最急降下法に

393
00:14:23,471 --> 00:14:25,007
変えようかと思うし、

394
00:14:25,007 --> 00:14:27,007
変えなくても良いようにも思う。

395
00:14:27,114 --> 00:14:28,672
n=1万 なら、

396
00:14:28,672 --> 00:14:31,148
1万 x 1万 行列の、逆行列を計算することはできる。

397
00:14:31,148 --> 00:14:34,345
だがそれよりずっと大きくなると、たぶん私なら最急降下法を使うだろう。

398
00:14:34,345 --> 00:14:35,834
だからnが10の6乗、

399
00:14:35,834 --> 00:14:36,920
つまり 100万個の特徴量なら、

400
00:14:36,920 --> 00:14:38,963
100万 x 100万 の行列の逆行列を

401
00:14:38,963 --> 00:14:41,565
計算するのは、

402
00:14:41,565 --> 00:14:42,631
非常に高くつく。

403
00:14:42,631 --> 00:14:46,163
だからそれほど多くの特徴量なら、私はきっと最急降下法を選ぶと思う。

404
00:14:46,163 --> 00:14:47,859
だから、正確にどれだけの数の

405
00:14:47,859 --> 00:14:49,282
特徴量から最急降下法にするのか、という

406
00:14:49,282 --> 00:14:52,655
明確な数字を提示するのは難しい。

407
00:14:52,655 --> 00:14:53,855
だが私ならば、

408
00:14:53,855 --> 00:14:55,501
大体、1万くらいから、最急降下法に

409
00:14:55,501 --> 00:14:58,258
変えようか、と考え始める。

410
00:14:58,335 --> 00:15:00,663
または

411
00:15:00,663 --> 00:15:04,324
このクラスの後半で扱う、別のいくつかのアルゴリズムの時も、最急降下法を使う。

412
00:15:04,324 --> 00:15:05,765
まとめよう。

413
00:15:05,765 --> 00:15:06,999
特徴量の数がそれほど多くなければ、

414
00:15:06,999 --> 00:15:08,475
正規方程式は、パラメータシータを解くための

415
00:15:08,475 --> 00:15:12,229
非常に素晴らしい代替案だ。

416
00:15:12,583 --> 00:15:13,983
具体的には、特徴量の数が

417
00:15:13,983 --> 00:15:15,749
1000以下なら、

418
00:15:15,749 --> 00:15:17,472
私はふだん、

419
00:15:17,472 --> 00:15:18,881
最急降下法ではなく

420
00:15:18,881 --> 00:15:21,955
正規方程式法を使う。

421
00:15:21,955 --> 00:15:23,549
このコースの後半でやる

422
00:15:23,549 --> 00:15:24,493
より複雑な学習アルゴリズムの

423
00:15:24,493 --> 00:15:26,235
アイデアを、

424
00:15:26,235 --> 00:15:27,912
先取りして見てみると、

425
00:15:27,912 --> 00:15:29,617
例えば、分類器の話をする時には、

426
00:15:29,617 --> 00:15:32,188
ロジスティック回帰のアルゴリズムなどは、

427
00:15:32,834 --> 00:15:34,319
後で見る、これらのアルゴリズムは

428
00:15:34,319 --> 00:15:35,467
実は、、、

429
00:15:35,467 --> 00:15:37,592
正規方程式法は、実際にはうまく行かない。

430
00:15:37,592 --> 00:15:39,388
それらの、より洗練された

431
00:15:39,388 --> 00:15:41,190
学習アルゴリズムでは。

432
00:15:41,190 --> 00:15:43,916
それらのアルゴリズムに対しては、最急降下法を用いなくてはならない。

433
00:15:43,916 --> 00:15:46,682
だから、最急降下法は、知っておくと、非常に良いアルゴリズムだ。

434
00:15:46,682 --> 00:15:48,859
線形回帰は

435
00:15:48,982 --> 00:15:50,017
大量の特徴量を持つ場合があり、

436
00:15:50,017 --> 00:15:52,373
このコースで見ることになる

437
00:15:52,373 --> 00:15:53,893
これ以外のアルゴリズムのいくつかは、

438
00:15:53,893 --> 00:15:55,438
単純に、正規方程式法が

439
00:15:55,438 --> 00:15:58,747
適用できず、使えない。

440
00:15:58,747 --> 00:16:00,537
だが線形回帰の、この特定のモデルに関しては、

441
00:16:00,537 --> 00:16:02,904
正規方程式は、最急降下法より

442
00:16:02,904 --> 00:16:06,827
より速い代替案と

443
00:16:07,219 --> 00:16:08,612
なりえる。

444
00:16:09,604 --> 00:16:11,920
だからアルゴリズムの詳細、

445
00:16:12,007 --> 00:16:14,164
問題の詳細に応じて、

446
00:16:14,164 --> 00:16:15,550
そして使う特徴量の数に応じて、

447
00:16:15,550 --> 00:16:19,550
これらのアルゴリズムは、どちらも知っておくべきだ。
