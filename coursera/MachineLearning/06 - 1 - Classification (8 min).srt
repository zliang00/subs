1
00:00:00,460 --> 00:00:01,410
このビデオと続くいくつかのビデオでは、

2
00:00:01,580 --> 00:00:04,660
分類(クラシフィケーション)の問題について話す。

3
00:00:04,660 --> 00:00:04,660


4
00:00:05,520 --> 00:00:07,000
クラシフィケーションとは、

5
00:00:07,110 --> 00:00:08,160
予測したい値yが、離散的な値の場合だ。

6
00:00:08,570 --> 00:00:10,190
ロジスティック回帰と呼ばれる

7
00:00:10,420 --> 00:00:11,860
アルゴリズムを開発していく。

8
00:00:12,410 --> 00:00:13,620
それはこんにち、機械学習の分野では、最もポピュラーで

9
00:00:13,700 --> 00:00:16,560
最も広く使われているアルゴリズムの一つだ。

10
00:00:19,770 --> 00:00:22,150
これが分類問題の例だ。

11
00:00:23,170 --> 00:00:24,720
以前に、メールのスパム分類を

12
00:00:25,260 --> 00:00:26,700
分類問題の例だと

13
00:00:27,070 --> 00:00:28,260
言ったと思う。

14
00:00:29,380 --> 00:00:32,160
他の例としては、オンラインの売買を分類する、などが考えられる。

15
00:00:33,080 --> 00:00:34,110
たとえば物を売るwebサイトを

16
00:00:34,340 --> 00:00:35,530
持っているとして、

17
00:00:35,750 --> 00:00:36,740
実際の売買が

18
00:00:37,040 --> 00:00:39,140
詐欺かどうかを知りたいとする。

19
00:00:39,260 --> 00:00:40,920
例えば誰かが

20
00:00:41,060 --> 00:00:42,260
盗まれたクレジットカードを使っているかどうか、

21
00:00:42,580 --> 00:00:43,890
盗まれたパスワードを使っているかどうか。

22
00:00:44,560 --> 00:00:46,830
それも分類問題だ。

23
00:00:47,030 --> 00:00:48,220
そして以前、腫瘍を

24
00:00:48,410 --> 00:00:50,610
ガン化する悪性の物か、良性の腫瘍かの

25
00:00:51,640 --> 00:00:53,680
分類の例も話した。

26
00:00:55,070 --> 00:00:56,010
これらすべての問題で、

27
00:00:56,690 --> 00:00:57,610
私たちが予想したいのは

28
00:00:57,850 --> 00:00:58,870
変数Yで、それは

29
00:00:59,290 --> 00:01:00,110
2つの値のどちらかを取ると

30
00:01:00,420 --> 00:01:01,710
考えられる、

31
00:01:02,600 --> 00:01:04,120
0か1か、とか、

32
00:01:04,340 --> 00:01:05,780
スパムかスパムでないか、とか、

33
00:01:06,620 --> 00:01:08,740
詐称か詐称でないかとか、悪性か良性かとか。

34
00:01:10,490 --> 00:01:11,430
0で表しているクラス（分類）の

35
00:01:11,810 --> 00:01:13,160
もう一つの名前は、

36
00:01:13,810 --> 00:01:15,660
陰性(ネガティブクラス)で、

37
00:01:15,950 --> 00:01:16,920
1で表しているクラスのもう一つの呼び名は

38
00:01:17,020 --> 00:01:19,350
陽性(ポジティブクラス)だ。

39
00:01:20,170 --> 00:01:21,500
つまり0は

40
00:01:22,070 --> 00:01:23,460
良性の腫瘍を表し、

41
00:01:23,850 --> 00:01:25,940
1つまり陽性は、悪性の腫瘍を表すなど。

42
00:01:27,090 --> 00:01:28,410
2つのクラスに割り振る、

43
00:01:28,860 --> 00:01:29,940
スパムかスパムでないか

44
00:01:30,050 --> 00:01:31,140
などなど。

45
00:01:31,330 --> 00:01:32,470
2つのクラス、陽性か陰性か

46
00:01:32,790 --> 00:01:34,140
0か1か、に何を割り振るかは

47
00:01:34,500 --> 00:01:35,950
なんでも良く、任意だ。

48
00:01:36,250 --> 00:01:37,840
そこは任意で

49
00:01:38,680 --> 00:01:39,820
どうでも良く、

50
00:01:39,990 --> 00:01:40,970
陰性は何かがない、

51
00:01:41,460 --> 00:01:43,430
例えば悪性の腫瘍がない、などの

52
00:01:43,590 --> 00:01:44,690
不在っぽい感覚だ。

53
00:01:45,000 --> 00:01:47,440
他方、陽性は

54
00:01:47,860 --> 00:01:49,410
何かが存在している感じを

55
00:01:49,950 --> 00:01:52,110
語感として持つ、私たちの探している何かの存在。

56
00:01:52,770 --> 00:01:54,340
だが何が陰性で、何が陽性かの

57
00:01:54,560 --> 00:01:55,400
定義は任意で、

58
00:01:55,680 --> 00:01:58,480
それはどうでも良い。

59
00:02:00,090 --> 00:02:00,980
さて、まずは

60
00:02:01,340 --> 00:02:03,030
0と1だけがあるケースの

61
00:02:03,290 --> 00:02:04,540
分類問題から始めよう。

62
00:02:05,480 --> 00:02:07,010
多数クラスの問題、たとえば

63
00:02:07,440 --> 00:02:09,320
Yが 0、1、2、3の値を

64
00:02:09,750 --> 00:02:10,960
取るようなケースは

65
00:02:11,550 --> 00:02:13,120
そのあとで扱う。

66
00:02:14,220 --> 00:02:16,810
それは多値分類問題と呼ばれる。

67
00:02:17,680 --> 00:02:18,800
だがここからの2、3のビデオでは

68
00:02:18,950 --> 00:02:20,280
2つのクラスだけの、

69
00:02:20,660 --> 00:02:22,750
二値分類問題から始める。

70
00:02:23,580 --> 00:02:25,650
そして多値分類の話は、その後に考える。

71
00:02:26,980 --> 00:02:29,440
では、どのように分類アルゴリズムを作るか？

72
00:02:30,530 --> 00:02:31,670
ここにあるのは、

73
00:02:31,750 --> 00:02:32,730
腫瘍を悪性か良性か分類する

74
00:02:34,350 --> 00:02:35,800
分類タスクの

75
00:02:36,240 --> 00:02:37,540
訓練セットだ。

76
00:02:37,820 --> 00:02:39,260
見ての通り、malignancy(悪性)の値は

77
00:02:39,530 --> 00:02:41,200
2つの値だけを取る、つまり0またはNoか、

78
00:02:41,380 --> 00:02:43,210
1またはYesか。

79
00:02:44,550 --> 00:02:45,650
だからこれらの訓練セットが与えられた時

80
00:02:45,850 --> 00:02:46,970
私たちができることは、

81
00:02:47,440 --> 00:02:48,700
一つは既に知っているアルゴリズムの

82
00:02:49,120 --> 00:02:52,710
線形回帰をこのデータセットに適用して

83
00:02:53,150 --> 00:02:55,310
単に直線をこのデータにフィッティングさせる。

84
00:02:56,290 --> 00:02:57,480
もし、この訓練セットに、

85
00:02:57,780 --> 00:02:58,760
直線をフィットさせると、

86
00:02:58,900 --> 00:03:00,320
得られる仮説は

87
00:03:00,700 --> 00:03:03,530
多分、このようになる。

88
00:03:03,700 --> 00:03:05,920
以上が私の仮説だ。

89
00:03:06,020 --> 00:03:07,890
h of xはθ transpose x に一致する

90
00:03:08,020 --> 00:03:09,330
もし、

91
00:03:09,570 --> 00:03:11,270
予測をしたいなら

92
00:03:11,500 --> 00:03:12,980
試しに、

93
00:03:13,610 --> 00:03:16,760
分類器の閾値を0.5に設定してみる。

94
00:03:17,110 --> 00:03:19,880
これが0.5の値の縦線だ

95
00:03:21,760 --> 00:03:23,940
そしてもし仮説が

96
00:03:24,330 --> 00:03:25,490
0.5以上の値を出力したら

97
00:03:25,620 --> 00:03:27,510
yは1だと予測される

98
00:03:27,860 --> 00:03:29,940
もし0.5未満であれば、yは0だと予測される

99
00:03:31,070 --> 00:03:32,540
このときに何がおこるか見てみる。

100
00:03:32,740 --> 00:03:33,900
さて、0.5を持ってくる。

101
00:03:34,090 --> 00:03:36,670
それはまさに閾値のあるところだ。

102
00:03:37,070 --> 00:03:39,260
このように線形回帰を使うと、

103
00:03:39,920 --> 00:03:41,060
この点より右にあるすべての点は

104
00:03:41,330 --> 00:03:42,460
結局ポジティブなクラスとして

105
00:03:42,640 --> 00:03:43,690
予測することとなる、

106
00:03:44,280 --> 00:03:45,390
何故なら、出力の値は

107
00:03:45,690 --> 00:03:46,800
縦軸上では

108
00:03:47,270 --> 00:03:48,690
0.5より大きいから。

109
00:03:49,340 --> 00:03:50,730
そしてその点より左の

110
00:03:51,000 --> 00:03:52,260
すべての点は、

111
00:03:52,490 --> 00:03:54,170
ネガティブ、と予測することとなる。

112
00:03:55,660 --> 00:03:57,570
この特定の例の場合、

113
00:03:57,720 --> 00:03:59,400
線形回帰は実際に

114
00:03:59,790 --> 00:04:01,870
妥当と言えなくもない動きをしている、

115
00:04:02,190 --> 00:04:03,910
やりたいことが

116
00:04:04,140 --> 00:04:05,430
分類問題であっても。

117
00:04:05,500 --> 00:04:07,420
だが、ここで少しだけ問題を変更してみる。

118
00:04:08,060 --> 00:04:09,360
横軸を少し延長して

119
00:04:10,040 --> 00:04:11,460
もう一つ追加の

120
00:04:11,650 --> 00:04:12,640
訓練セットの例が

121
00:04:12,990 --> 00:04:15,030
右側にあるとしよう。

122
00:04:16,520 --> 00:04:17,830
追加の学習の例は

123
00:04:18,170 --> 00:04:19,200
見ての通り、ここある。

124
00:04:19,390 --> 00:04:21,710
それは実際には何も違いはない。

125
00:04:22,420 --> 00:04:23,470
訓練セットを見ると、

126
00:04:23,560 --> 00:04:26,340
良い仮説は、極めて明白だ。

127
00:04:26,890 --> 00:04:27,920
このあたりの右側にある

128
00:04:28,000 --> 00:04:29,050
すべてのものは

129
00:04:29,190 --> 00:04:30,270
この右側は、陽性と予測すべきだ

130
00:04:30,300 --> 00:04:31,280
これよりも

131
00:04:31,480 --> 00:04:32,690
左側なら陰性と予測すべきだろう

132
00:04:33,060 --> 00:04:34,700
なぜなら

133
00:04:34,880 --> 00:04:35,940
この訓練セットを見ると

134
00:04:36,200 --> 00:04:37,880
ある値よりも大きな腫瘍は

135
00:04:37,970 --> 00:04:39,190
このあたりの値よりも大きければ

136
00:04:39,490 --> 00:04:41,030
悪性だと判断できて、

137
00:04:41,200 --> 00:04:42,110
もっと小さい腫瘍は、

138
00:04:42,220 --> 00:04:44,660
悪性でないと考えられる。少なくともこの訓練セットの範囲では。

139
00:04:46,160 --> 00:04:47,280
だが、ここに

140
00:04:47,720 --> 00:04:49,060
もうひとつの例を追加するならば、

141
00:04:49,620 --> 00:04:50,660
そして線形回帰をもう一度実行すると、

142
00:04:51,580 --> 00:04:53,590
そのデータにフィットした直線が得られる。

143
00:04:54,430 --> 00:04:55,630
それはこんな見た目だろう。

144
00:04:57,890 --> 00:04:59,860
そして、仮に閾値を0.5とする。

145
00:05:02,480 --> 00:05:03,460
結局、閾値は

146
00:05:04,110 --> 00:05:05,550
このあたりとなる、

147
00:05:06,320 --> 00:05:07,320
つまりこの点より右のすべてが

148
00:05:07,570 --> 00:05:08,790
陽性と予測され、

149
00:05:08,960 --> 00:05:11,510
この点の左側はすべて陰性と判定される。

150
00:05:14,580 --> 00:05:15,720
つまり、これを線形回帰で行うのは

151
00:05:16,100 --> 00:05:18,500
極めて不都合に思える。

152
00:05:18,770 --> 00:05:19,840
何故なら、これらは私たちの陽性な標本で

153
00:05:19,930 --> 00:05:22,010
これらは陰性な標本だ。

154
00:05:23,050 --> 00:05:24,580
だから私たちが2つのクラスを

155
00:05:24,800 --> 00:05:26,000
だいたいこのあたりで分割するのは

156
00:05:26,550 --> 00:05:28,180
極めて自明に思える。

157
00:05:28,670 --> 00:05:30,030
しかしここから右に離れた

158
00:05:30,190 --> 00:05:31,280
一つの例を追加しても、

159
00:05:31,420 --> 00:05:33,340
この例は実際には何の新しい情報ももたらさない。

160
00:05:33,770 --> 00:05:34,950
つまり、ここから離れたところに

161
00:05:35,170 --> 00:05:36,300
追加した例が悪性だとわかっても、

162
00:05:37,030 --> 00:05:39,100
学習はなんの変化も起こしてはならない。

163
00:05:40,230 --> 00:05:41,210
しかし何か標本をそこに追加したら

164
00:05:41,740 --> 00:05:43,420
線形回帰は

165
00:05:44,410 --> 00:05:45,670
直線をデータにフィットするように、

166
00:05:45,980 --> 00:05:47,650
変更してしまった、

167
00:05:48,840 --> 00:05:50,000
このマゼンタ色の直線から

168
00:05:50,840 --> 00:05:51,940
ここの青い直線へと。

169
00:05:52,850 --> 00:05:54,770
そしてより悪い推論を行ってしまった。

170
00:05:56,950 --> 00:05:58,440
つまり、分類問題に

171
00:05:59,080 --> 00:06:01,030
線形回帰を適用するのは

172
00:06:01,610 --> 00:06:03,400
多くの場合は、良い考えではない。

173
00:06:04,430 --> 00:06:05,750
最初のケースでは

174
00:06:05,810 --> 00:06:07,090
この追加の学習の標本を

175
00:06:07,540 --> 00:06:08,780
足す前の例では、

176
00:06:09,810 --> 00:06:11,430
線形回帰は

177
00:06:11,650 --> 00:06:13,200
偶然、その例には

178
00:06:13,380 --> 00:06:14,990
うまくいく推論を

179
00:06:15,090 --> 00:06:16,290
与えてくれた。

180
00:06:16,670 --> 00:06:19,470
だが普通は、

181
00:06:19,980 --> 00:06:20,970
データセットに線形回帰を適用すると

182
00:06:21,820 --> 00:06:23,040
幸運な場合もあるが、

183
00:06:23,270 --> 00:06:24,130
多くの場合は良い考えではない。

184
00:06:24,260 --> 00:06:25,730
だから分類問題には

185
00:06:25,980 --> 00:06:27,960
線形回帰は使わない。

186
00:06:29,670 --> 00:06:30,820
そしてもう一つ、

187
00:06:31,250 --> 00:06:32,650
万が一、線形回帰を

188
00:06:32,930 --> 00:06:35,510
分類問題に使うと、おかしいことが起こりうる。

189
00:06:36,690 --> 00:06:38,220
分類問題では、

190
00:06:38,450 --> 00:06:39,790
Yが0か1かは、既にわかっている。

191
00:06:40,580 --> 00:06:41,620
だが線形回帰を使うと、

192
00:06:41,890 --> 00:06:43,050
推論は

193
00:06:44,210 --> 00:06:45,750
1より大きいか

194
00:06:46,060 --> 00:06:47,330
0より小さい値も出しうる。

195
00:06:47,500 --> 00:06:48,820
すべての学習の例は

196
00:06:49,050 --> 00:06:50,690
正しく Y を

197
00:06:51,140 --> 00:06:52,410
0 か 1 にラベリングしているにも関わらず。

198
00:06:53,900 --> 00:06:54,880
私たちはラベルが

199
00:06:55,520 --> 00:06:56,760
0 か 1 でなくてはならないのを

200
00:06:56,960 --> 00:06:58,160
知っているのに

201
00:06:58,350 --> 00:06:59,320
あー

202
00:06:59,420 --> 00:07:00,890
アルゴリズムが

203
00:07:01,210 --> 00:07:02,580
1 より大きいか 0 より小さい値を

204
00:07:02,840 --> 00:07:04,900
出力するのは、少しおかしい。

205
00:07:09,540 --> 00:07:10,900
だからこの後に続く一連のビデオで

206
00:07:11,000 --> 00:07:12,400
私たちが行うのは、

207
00:07:12,860 --> 00:07:14,640
ロジスティック回帰と呼ばれるアルゴリズムを

208
00:07:15,550 --> 00:07:17,390
開発することで、そのアルゴリズムの

209
00:07:17,780 --> 00:07:19,290
ロジスティック回帰の出力、つまり予測は

210
00:07:19,670 --> 00:07:21,220
いつも0と1の間で、

211
00:07:21,630 --> 00:07:22,750
1より大きくなったり

212
00:07:23,060 --> 00:07:24,170
0より小さくなったりしない、という

213
00:07:24,370 --> 00:07:26,370
特徴がある。

214
00:07:26,530 --> 00:07:28,570
ところで、ロジスティック回帰、と言うのは、

215
00:07:29,090 --> 00:07:30,150
分類問題に使うのに

216
00:07:30,350 --> 00:07:32,770
少しややこしい

217
00:07:33,330 --> 00:07:35,060
かもしれない。

218
00:07:35,780 --> 00:07:37,410
名前に「回帰」というのが

219
00:07:37,700 --> 00:07:39,360
入っているのに、実際はロジスティック回帰は

220
00:07:39,970 --> 00:07:41,280
分類アルゴリズムだから。

221
00:07:42,120 --> 00:07:43,040
でもそれは、単に歴史的事情で

222
00:07:43,160 --> 00:07:46,140
付いた名前なので、混乱しないように。

223
00:07:46,680 --> 00:07:48,340
ロジスティック回帰は実際は

224
00:07:48,430 --> 00:07:50,250
分類アルゴリズムで

225
00:07:50,380 --> 00:07:52,030
Yが離散的な値の時に使う。

226
00:07:52,160 --> 00:07:54,780
1、0とか0、1とか。

227
00:07:55,820 --> 00:07:57,440
ここまでくれば、

228
00:07:57,680 --> 00:07:59,180
分類問題に線形回帰を用いるのが

229
00:07:59,280 --> 00:08:00,950
何故良くないのか、

230
00:08:01,400 --> 00:08:02,660
わかってもらえただろう。

231
00:08:03,210 --> 00:08:04,480
次のビデオでは、

232
00:08:04,700 --> 00:08:05,680
ロジスティック回帰のアルゴリズムの

233
00:08:06,290 --> 00:08:07,640
詳細を見ていく。
