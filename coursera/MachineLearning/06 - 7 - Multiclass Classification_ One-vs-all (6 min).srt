1
00:00:00,200 --> 00:00:01,596
このビデオでは、ロジスティック回帰を

2
00:00:01,620 --> 00:00:03,659
多クラスの分類問題に

3
00:00:03,659 --> 00:00:06,089
適用する方法を議論する。

4
00:00:06,089 --> 00:00:07,526
具体的には、one vs all 分類と呼ばれる

5
00:00:07,526 --> 00:00:12,070
アルゴリズムを話したい。

6
00:00:12,150 --> 00:00:14,316
多クラスの分類問題とは何だろう？

7
00:00:14,316 --> 00:00:15,945
ここにその例がある。

8
00:00:15,945 --> 00:00:17,318
例えばあなたの e-mail を

9
00:00:17,320 --> 00:00:19,691
学習アルゴリズムによって

10
00:00:19,710 --> 00:00:21,076
別々のフォルダに移動させたい、としよう。

11
00:00:21,076 --> 00:00:23,398
あるいは自動で e-mail にタグをつけたい、とする。

12
00:00:23,398 --> 00:00:24,749
あなたは様々なフォルダやタグを使うだろう、

13
00:00:24,790 --> 00:00:27,052
仕事用の e-mail とか

14
00:00:27,060 --> 00:00:28,236
友達から来た e-mail とか

15
00:00:28,236 --> 00:00:31,561
家族から来た e-mail とか、趣味に関する e-mail とか。

16
00:00:31,590 --> 00:00:33,145
すると、私たちは4つのクラスへの

17
00:00:33,145 --> 00:00:34,856
分類問題に直面するわけだ。

18
00:00:34,900 --> 00:00:36,164
そこに数字を振っておくと、

19
00:00:36,180 --> 00:00:38,129
クラス y=1, y=2, 

20
00:00:38,129 --> 00:00:41,326
y=3, y=4 と。

21
00:00:41,326 --> 00:00:43,530
もう一つ別の例としては、

22
00:00:44,490 --> 00:00:45,790
医療の診断なども考えられる：

23
00:00:46,000 --> 00:00:47,260
患者があなたのオフィスに

24
00:00:47,800 --> 00:00:48,910
やってきて、

25
00:00:48,930 --> 00:00:51,395
鼻づまりだと言う。

26
00:00:51,395 --> 00:00:52,762
考えられる診断結果としては、

27
00:00:52,762 --> 00:00:54,140
病気ではない場合、これを y=1 とする、

28
00:00:54,140 --> 00:00:55,474
または風邪の場合、これを 2 とする。

29
00:00:55,490 --> 00:00:59,026
またはインフルエンザかもしれない。

30
00:00:59,026 --> 00:01:00,541
最後に、三番目の例としては、

31
00:01:00,541 --> 00:01:02,056
天気の分類のために

32
00:01:02,090 --> 00:01:03,906
機械学習を使いたいとする。

33
00:01:03,910 --> 00:01:05,299
例えば晴れ、曇り、

34
00:01:05,299 --> 00:01:07,937
雨、雪を。雪が降りそうな場所ならだが、

35
00:01:07,950 --> 00:01:10,211
分類したいとする。

36
00:01:10,230 --> 00:01:11,165
以上の三つの例はすべて

37
00:01:11,165 --> 00:01:12,808
y が離散的な

38
00:01:12,808 --> 00:01:14,300
小数の値を

39
00:01:14,300 --> 00:01:16,498
取ることができる。1 から 3 まで、

40
00:01:16,498 --> 00:01:17,810
1 から 4 まで、など。

41
00:01:17,890 --> 00:01:20,659
これが多クラスの分類問題だ。

42
00:01:20,659 --> 00:01:21,904
ところで、インデックスを

43
00:01:21,904 --> 00:01:23,632
0123 と振るか、1234 と振るかは

44
00:01:23,632 --> 00:01:27,063
どうでもいい。

45
00:01:27,090 --> 00:01:29,138
私はクラスを 1 から振って、

46
00:01:29,138 --> 00:01:31,569
0 からではないことが多いが。

47
00:01:31,569 --> 00:01:33,756
どちらでもいい。

48
00:01:33,756 --> 00:01:35,243
他方、以前の二値分類問題は

49
00:01:35,243 --> 00:01:39,375
私たちのデータセットはこんな感じだった。

50
00:01:39,375 --> 00:01:41,617
多クラスの分類問題では、

51
00:01:41,617 --> 00:01:42,792
私たちのデータセットはこんな感じになるだろう、

52
00:01:42,792 --> 00:01:44,362
ここで三つの異なる記号を

53
00:01:44,362 --> 00:01:48,399
三つのクラスを表すのに用いた。

54
00:01:48,410 --> 00:01:49,858
そこで問題はこうだ：

55
00:01:49,858 --> 00:01:51,613
三つのクラスのデータセットを与えられて、

56
00:01:51,613 --> 00:01:53,193
ここでは

57
00:01:53,193 --> 00:01:54,651
これが一つのクラスの手本で

58
00:01:54,651 --> 00:01:55,768
これがそれとは別のクラスの手本で、

59
00:01:55,790 --> 00:01:58,389
これがそれとも別のさらに別のクラスの手本だ。

60
00:01:58,410 --> 00:02:01,421
これらの状況で、どうやって学習アルゴリズムを機能させられるだろうか？

61
00:02:01,421 --> 00:02:02,598
私たちは既に、二択の分類を

62
00:02:02,598 --> 00:02:05,096
どうするのか知っている。

63
00:02:05,096 --> 00:02:06,594
ロジスティック回帰を用いることで、

64
00:02:06,594 --> 00:02:07,736
陽性と陰性のクラスを直線で

65
00:02:07,736 --> 00:02:10,613
どうやって分離するかを知っている。

66
00:02:10,613 --> 00:02:12,116
one vs all 分類と言われるアイデアを

67
00:02:12,116 --> 00:02:14,399
用いることで、

68
00:02:14,400 --> 00:02:15,730
これを、多クラスの分類にも

69
00:02:15,730 --> 00:02:18,646
使えるようにできる。

70
00:02:18,650 --> 00:02:21,617
one vs all はこんな風に機能する。

71
00:02:21,620 --> 00:02:25,777
ところで、この方法は one vs rest と呼ばれることもある。

72
00:02:25,777 --> 00:02:26,941
では、左に示したような

73
00:02:26,941 --> 00:02:28,138
訓練セットがあるとしよう。

74
00:02:28,150 --> 00:02:30,456
3つのクラスがあり、

75
00:02:30,470 --> 00:02:32,310
y1 で三角形を示し、

76
00:02:32,310 --> 00:02:34,405
y2 で四角形を

77
00:02:34,405 --> 00:02:37,970
y3 でバツを示すとする。

78
00:02:37,980 --> 00:02:39,460
そこでやることは、

79
00:02:39,480 --> 00:02:41,350
訓練セットを持ってきて、

80
00:02:41,350 --> 00:02:44,816
これを三つの異なる、二択問題に変換することだ。

81
00:02:44,816 --> 00:02:46,719
つまり、私はこれを、三つの別個の

82
00:02:46,750 --> 00:02:49,450
二値分類問題に変換する。

83
00:02:49,450 --> 00:02:51,660
クラス1、三角形から始めよう。

84
00:02:51,660 --> 00:02:52,990
本質的には、新しい三角形の

85
00:02:53,050 --> 00:02:55,418
偽の訓練セットを作る。

86
00:02:55,440 --> 00:02:56,913
そこではクラス2と3は

87
00:02:56,920 --> 00:02:58,151
陰性のクラスに割り当てて、

88
00:02:58,151 --> 00:02:59,873
クラス1は

89
00:02:59,873 --> 00:03:01,134
陽性のクラスに割り当てる。

90
00:03:01,134 --> 00:03:02,352
こうして右側に示したような

91
00:03:02,380 --> 00:03:03,700
訓練セットを作り

92
00:03:03,700 --> 00:03:05,508
そこに分類器を

93
00:03:05,508 --> 00:03:07,573
フィッティングさせる。これを

94
00:03:07,573 --> 00:03:10,200
h の下付き添字シータ、

95
00:03:10,220 --> 00:03:12,626
上付き添字 1 の x と呼ぼう。

96
00:03:12,640 --> 00:03:15,659
ここでは、三角形が陽性の手本で

97
00:03:15,659 --> 00:03:19,008
丸は陰性の手本となる。

98
00:03:19,008 --> 00:03:20,649
つまり三角形に

99
00:03:20,649 --> 00:03:21,800
値 1 を割り振って、

100
00:03:21,800 --> 00:03:25,291
丸に値 0 を割り振る。

101
00:03:25,300 --> 00:03:26,723
そして通常のロジスティック回帰の分類器で

102
00:03:26,723 --> 00:03:29,556
トレーニングを行う。

103
00:03:29,556 --> 00:03:34,173
その結果、ある決定境界が得られる。

104
00:03:34,173 --> 00:03:34,173
いいかな？

105
00:03:34,890 --> 00:03:37,693
上付き添字1は、クラス1を。

106
00:03:37,693 --> 00:03:40,777
つまりこれを最初の三角形のクラスのために行う。

107
00:03:40,800 --> 00:03:42,302
次に、同じことをクラス2に対しても行う。

108
00:03:42,302 --> 00:03:44,013
四角形をとり、

109
00:03:44,020 --> 00:03:45,456
四角形に陽性のクラスを

110
00:03:45,470 --> 00:03:47,001
割り振って、それ以外のすべてに

111
00:03:47,001 --> 00:03:50,213
つまり三角とバツに陰性のクラスを割り振る。

112
00:03:50,220 --> 00:03:54,173
そして二番目のロジスティック回帰分類器をフィッティングする。

113
00:03:54,173 --> 00:03:56,410
これは h の上付き添字2、と呼ぶことにする。

114
00:03:56,420 --> 00:03:58,352
ここでこの上付き添字2は、

115
00:03:58,352 --> 00:04:00,029
現在、私たちは

116
00:04:00,029 --> 00:04:01,860
これ、四角形のクラスを

117
00:04:01,870 --> 00:04:03,310
陽性のクラスと扱っている、ということを表している。

118
00:04:03,350 --> 00:04:07,518
そしてこんな分類器が得られるだろう。

119
00:04:07,518 --> 00:04:08,854
そして最後に、同様のことを

120
00:04:08,854 --> 00:04:10,143
三番目のクラスに対して行い、

121
00:04:10,143 --> 00:04:11,598
そして三番目の分類器、

122
00:04:11,610 --> 00:04:14,632
h 上付き添字 3 の x をフィッティングし、

123
00:04:14,632 --> 00:04:16,424
これが

124
00:04:16,440 --> 00:04:18,106
こんな感じの決定境界、あるいは

125
00:04:18,106 --> 00:04:19,749
陽性と陰性を分離する分類器

126
00:04:19,750 --> 00:04:22,863
となる。

127
00:04:22,870 --> 00:04:24,353
まとめよう。私たちがやったことは、

128
00:04:24,353 --> 00:04:27,872
3つの分類器をフィッティングした。

129
00:04:27,890 --> 00:04:29,403
i=1, 2, 3 について

130
00:04:29,403 --> 00:04:31,836
h 上付き添字 i 下付き添字シータ x の分類器を

131
00:04:31,880 --> 00:04:33,855
フィッティングしていく。

132
00:04:33,855 --> 00:04:35,193
こうして、与えられたxと

133
00:04:35,220 --> 00:04:36,446
パラメータシータに対して、

134
00:04:36,450 --> 00:04:38,208
y が i となる確率を

135
00:04:38,208 --> 00:04:41,834
推定しようと試みた。

136
00:04:41,834 --> 00:04:41,834
いいかな？

137
00:04:41,834 --> 00:04:43,229
最初のインスタンスでは、

138
00:04:43,230 --> 00:04:44,903
ここの最初のものは、

139
00:04:44,910 --> 00:04:47,277
この分類器は三角形で

140
00:04:47,280 --> 00:04:49,364
学習している。

141
00:04:49,364 --> 00:04:52,037
つまり三角形を陽性のクラスとみなしている。

142
00:04:52,060 --> 00:04:53,840
つまり、h の上付き添字 1 は

143
00:04:53,840 --> 00:04:55,163
本質的には

144
00:04:55,170 --> 00:04:57,343
y=1 となる確率を

145
00:04:57,350 --> 00:04:59,083
与えられた x とパラメータシータの条件で

146
00:04:59,083 --> 00:05:02,037
推定している。

147
00:05:02,037 --> 00:05:04,475
同様に、これは四角形のクラスを

148
00:05:04,480 --> 00:05:05,859
陽性の手本と

149
00:05:05,859 --> 00:05:07,400
みなしているので、

150
00:05:07,400 --> 00:05:10,748
これは y が 2 となる確率を推計しているのだ。以下同様。

151
00:05:10,750 --> 00:05:13,300
つまり今や、私たちは3つの分類器を持ち、

152
00:05:13,310 --> 00:05:16,649
それぞれ3つのクラスのうち一つに向けてトレーニングされている。

153
00:05:16,670 --> 00:05:17,859
まとめると、私たちがやったことは

154
00:05:17,860 --> 00:05:19,685
ロジスティック回帰の分類器、

155
00:05:19,700 --> 00:05:21,280
h の上付き添字 i の x を

156
00:05:21,300 --> 00:05:23,560
y が i となる確率を

157
00:05:23,560 --> 00:05:24,947
各 h i が推定するように

158
00:05:24,950 --> 00:05:26,183
トレーニングしたい。

159
00:05:26,183 --> 00:05:28,550
そして最終的に、

160
00:05:28,570 --> 00:05:29,740
推定を行いたい時には、、、

161
00:05:29,820 --> 00:05:31,772
新規の入力 x を与えられた時に、

162
00:05:31,772 --> 00:05:33,326
推定を行いたければ、

163
00:05:33,340 --> 00:05:34,729
私たちがやことは、

164
00:05:34,730 --> 00:05:36,706
手持ちの

165
00:05:36,706 --> 00:05:38,557
3つの分類器を

166
00:05:38,557 --> 00:05:40,010
入力 x に対して実行し、

167
00:05:40,010 --> 00:05:41,535
その中で一番大きなクラスの i を選ぶ、と

168
00:05:41,535 --> 00:05:44,068
いうことをする。

169
00:05:44,068 --> 00:05:45,387
つまり基本的には、

170
00:05:45,387 --> 00:05:47,180
もっとも自信の

171
00:05:47,180 --> 00:05:49,163
ありそうな分類器を、

172
00:05:49,210 --> 00:05:52,178
言い換えるともっとも大声でこれが正しいクラスだ、と

173
00:05:52,178 --> 00:05:54,352
言っているものを選ぶ。

174
00:05:54,352 --> 00:05:56,153
つまり最も高い確率を

175
00:05:56,190 --> 00:05:58,069
与える i の値ならなんでもいい。

176
00:05:58,069 --> 00:06:01,056
そして y の値を予測する。

177
00:06:02,660 --> 00:06:04,453
以上が多クラスの分類問題であり、

178
00:06:04,470 --> 00:06:07,677
one vs all 法だ。

179
00:06:07,677 --> 00:06:09,120
このちょっとした手法を用いることで

180
00:06:09,120 --> 00:06:10,521
ロジスティック回帰を用いて、

181
00:06:10,521 --> 00:06:12,033
多クラスの分類器の問題でも

182
00:06:12,033 --> 00:06:15,051
それを同様に機能させることができるようになった。
