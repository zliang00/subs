1
00:00:00,200 --> 00:00:03,878
これまで、複数変数の線形回帰について学んだ。

2
00:00:03,910 --> 00:00:05,185
このビデオでは、使う特徴量の

3
00:00:05,185 --> 00:00:06,369
選択について、

4
00:00:06,380 --> 00:00:07,830
少し触れておきたい。

5
00:00:07,830 --> 00:00:09,742
そして適切な特徴量を選ぶことで

6
00:00:09,750 --> 00:00:11,477
強力な別の学習アルゴリズムを

7
00:00:11,477 --> 00:00:13,803
得る方法にも触れておく。

8
00:00:13,810 --> 00:00:15,229
特に、私は多項式回帰について

9
00:00:15,229 --> 00:00:17,826
話をしたい。それは線形回帰の

10
00:00:17,826 --> 00:00:19,535
機構を用いて、非常に複雑な、

11
00:00:19,535 --> 00:00:21,247
非線形な関数を

12
00:00:21,247 --> 00:00:25,060
扱うことができるものだ。

13
00:00:25,690 --> 00:00:28,827
住居の価格の予測の例を、使っていこう。

14
00:00:29,300 --> 00:00:31,147
二つの特徴量があるとする：

15
00:00:31,147 --> 00:00:33,805
住居の間口と、住居の奥行きだ。

16
00:00:33,805 --> 00:00:35,428
これは私たちが売ろうとしている住居の絵だ。

17
00:00:35,428 --> 00:00:37,264
間口は

18
00:00:37,264 --> 00:00:40,103
この距離と定義される。

19
00:00:40,103 --> 00:00:43,009
基本的には、幅や

20
00:00:43,009 --> 00:00:44,949
長さだ、

21
00:00:44,960 --> 00:00:46,652
道に面した

22
00:00:46,652 --> 00:00:47,994
方の。

23
00:00:48,020 --> 00:00:49,468
そして住居の

24
00:00:49,500 --> 00:00:53,120
奥行きとは

25
00:00:53,130 --> 00:00:54,758
あなたの家がどれだけ奥まっているかだ。

26
00:00:54,770 --> 00:00:57,992
間口があって、奥行きがある。

27
00:00:57,992 --> 00:00:59,858
間口と奥行きと呼ばれている。

28
00:00:59,858 --> 00:01:01,355
あなたは、こんな線形回帰の

29
00:01:01,360 --> 00:01:04,163
モデルを建てるかもしれない。

30
00:01:04,180 --> 00:01:06,062
間口が最初の特徴量x1で、

31
00:01:06,062 --> 00:01:07,535
奥行きは二番目の特徴量x2だ。

32
00:01:07,535 --> 00:01:10,169
しかし、線形回帰を

33
00:01:10,169 --> 00:01:11,772
適用する時に、

34
00:01:11,772 --> 00:01:13,342
必ずしも、得られた特徴量x1とx2を

35
00:01:13,342 --> 00:01:16,607
そのまま使わなくてもいい。

36
00:01:16,610 --> 00:01:20,531
独自に、新たな特徴量を作っても良い。

37
00:01:20,531 --> 00:01:21,709
だから例えば、私が住居の価格を

38
00:01:21,710 --> 00:01:22,895
予測したいとすると、

39
00:01:22,895 --> 00:01:24,840
私は代わりに、こんな風にしても良い。それは、

40
00:01:24,850 --> 00:01:27,468
住居の価格を本当に決定しているのは、

41
00:01:27,490 --> 00:01:29,133
住居の広さ、つまり

42
00:01:29,133 --> 00:01:32,164
私の所有している土地の面積だ、と決めても良い。

43
00:01:32,190 --> 00:01:33,365
だから、新しい特徴量を作る。

44
00:01:33,380 --> 00:01:34,609
この特徴量をxと呼ぶことにし、

45
00:01:34,609 --> 00:01:40,409
これは間口 掛ける 奥行き。

46
00:01:40,440 --> 00:01:42,404
これは掛け算の記号。

47
00:01:42,404 --> 00:01:44,334
間口 x 奥行、何故なら

48
00:01:44,334 --> 00:01:46,040
これは私が所有している

49
00:01:46,090 --> 00:01:48,035
土地の面積だから。

50
00:01:48,035 --> 00:01:50,651
そして私は、仮説として

51
00:01:50,710 --> 00:01:53,327
この一つだけの特徴量を

52
00:01:53,350 --> 00:01:54,785
使う、

53
00:01:54,785 --> 00:01:57,430
つまり土地の面積。

54
00:01:57,580 --> 00:01:58,939
長方形の面積は

55
00:01:58,940 --> 00:02:00,345
知っての通り

56
00:02:00,345 --> 00:02:01,432
長さの積なので、

57
00:02:01,460 --> 00:02:03,822
個々の問題について、

58
00:02:03,822 --> 00:02:05,253
得ている知見に

59
00:02:05,280 --> 00:02:07,481
応じて、

60
00:02:07,490 --> 00:02:09,604
単に間口と奥行を特徴量にするのではなく、

61
00:02:09,620 --> 00:02:11,103
―それは偶然、最初の特徴量だっただけなので―

62
00:02:11,130 --> 00:02:13,489
時には、より良いモデルが実際に得られそうな

63
00:02:13,489 --> 00:02:16,771
新しい特徴量を定義しても良い。

64
00:02:16,790 --> 00:02:18,163
特徴量を選ぶことと

65
00:02:18,163 --> 00:02:19,745
密接に関連して、

66
00:02:19,745 --> 00:02:22,973
多項式回帰というアイデアがある。

67
00:02:23,010 --> 00:02:26,868
こんな住宅の価格のデータセットがあるとしよう。

68
00:02:26,880 --> 00:02:29,646
これにフィットさせたいと思うようなモデルが、いくつか考えられる。

69
00:02:29,660 --> 00:02:32,587
一つは、こんな感じの二次のモデルをフィットさせるということだ。

70
00:02:32,600 --> 00:02:35,598
直線は、あまりこのデータにフィットするようには見えない。

71
00:02:35,598 --> 00:02:36,788
だからこんな感じの

72
00:02:36,788 --> 00:02:38,408
2次のモデルをフィットさせたいと思うかもしれない。

73
00:02:38,420 --> 00:02:40,248
それは、価格が広さの二次関数になっている、

74
00:02:40,248 --> 00:02:42,017
と思っているということだ。

75
00:02:42,020 --> 00:02:43,956
それを使うと、

76
00:02:43,970 --> 00:02:45,018
例えば、データにこのように

77
00:02:45,020 --> 00:02:47,070
フィッティングするだろう。

78
00:02:47,280 --> 00:02:48,560
だがそこで、あなたは2次のモデルは

79
00:02:48,570 --> 00:02:50,013
筋が通らない、と思うかもしれない。

80
00:02:50,013 --> 00:02:52,582
何故なら、二次関数は、最終的には、

81
00:02:52,582 --> 00:02:53,858
下降して戻ってきてしまう。

82
00:02:53,858 --> 00:02:55,591
たぶん、私たちは、家が大きすぎたら

83
00:02:55,600 --> 00:02:58,899
価格は下がるべき、とは思わないだろう。

84
00:02:58,970 --> 00:03:00,649
だから、私たちは

85
00:03:00,650 --> 00:03:02,700
別の多項式のモデルを選ぶかもしれない。

86
00:03:02,700 --> 00:03:04,274
そこで、代わりに三次関数を

87
00:03:04,290 --> 00:03:07,480
選んでも良い。

88
00:03:07,480 --> 00:03:09,225
この場合は、私たちは三次の項を持つことになり、

89
00:03:09,225 --> 00:03:10,764
それでフィッティングすると、

90
00:03:10,800 --> 00:03:12,367
こんな感じのモデルが得られるだろう。

91
00:03:12,390 --> 00:03:13,907
そしてこの緑の線は

92
00:03:13,910 --> 00:03:15,278
先ほどよりもやや良くデータにフィットするだろう。

93
00:03:15,278 --> 00:03:18,052
何故なら最終的に降下して戻ってこないから。

94
00:03:18,052 --> 00:03:21,992
では実際には、どのようにしてモデルにデータをフィッティングするか？

95
00:03:22,020 --> 00:03:23,868
多変量の線形回帰の

96
00:03:23,868 --> 00:03:27,059
機構を用いて、

97
00:03:27,059 --> 00:03:30,692
アルゴリズムを簡単に変更するだけで、それができる。

98
00:03:30,692 --> 00:03:32,632
私たちが知っているフィッティングのやり方の

99
00:03:32,632 --> 00:03:34,217
仮説の形は、

100
00:03:34,217 --> 00:03:35,782
こんな類だ。

101
00:03:35,782 --> 00:03:37,612
hのxは シータ0 足す、

102
00:03:37,612 --> 00:03:41,608
シータ1 x1  足す シータ2 x2 足すシータ3 x3。

103
00:03:41,608 --> 00:03:42,775
そしてもし、私たちが緑の四角で

104
00:03:42,775 --> 00:03:45,220
囲んだような、

105
00:03:45,250 --> 00:03:47,239
三次のモデルにフィッティングしたければ、

106
00:03:47,239 --> 00:03:48,940
こう言い換えれば良い。

107
00:03:48,940 --> 00:03:49,825
住居の価格を予測するには、

108
00:03:49,825 --> 00:03:51,364
シータ0 足す、

109
00:03:51,364 --> 00:03:53,056
シータ1 掛ける 住居の広さ、

110
00:03:53,056 --> 00:03:55,905
足す シータ2 掛ける 住居の広さの二乗。

111
00:03:55,910 --> 00:03:58,974
この項はこちらの項とイコールで、

112
00:03:58,974 --> 00:04:00,885
そしてそこに、

113
00:04:00,890 --> 00:04:02,343
足す シータ3 掛ける 住居の広さの三乗で、

114
00:04:02,350 --> 00:04:05,302
これがこの三番目の項となる。

115
00:04:05,470 --> 00:04:06,967
これら二つの定義を

116
00:04:06,990 --> 00:04:08,668
お互いにマッピングするには、

117
00:04:08,668 --> 00:04:10,339
自然な方法として、

118
00:04:10,339 --> 00:04:12,128
最初の特徴量x1に、

119
00:04:12,150 --> 00:04:13,568
住居の広さを、

120
00:04:13,568 --> 00:04:15,320
入れる。

121
00:04:15,320 --> 00:04:16,721
そして二番目の特徴量x2に

122
00:04:16,721 --> 00:04:17,766
住居の広さの二乗を入れる。

123
00:04:17,766 --> 00:04:20,400
そして三番目の特徴量x3に

124
00:04:20,400 --> 00:04:22,780
住居の広さの三乗を入れる。

125
00:04:22,800 --> 00:04:24,292
そして三つのと特徴量を

126
00:04:24,292 --> 00:04:26,311
このように選んで、

127
00:04:26,311 --> 00:04:27,720
線形回帰を適用することで、

128
00:04:27,720 --> 00:04:30,540
このモデルをフィッティングでき、

129
00:04:30,540 --> 00:04:31,901
結局、データに三次関数のフィッティングが

130
00:04:31,901 --> 00:04:34,374
できる。

131
00:04:34,374 --> 00:04:35,523
もう一つ追加で言っておきたいのは、

132
00:04:35,523 --> 00:04:36,799
もしあなたがこのように

133
00:04:36,800 --> 00:04:38,610
特徴量を選んだら、

134
00:04:38,610 --> 00:04:40,925
その時はフィーチャースケーリングの重要性が

135
00:04:40,925 --> 00:04:43,688
ますます高くなる。

136
00:04:44,130 --> 00:04:45,254
つまり、もし住居の広さが

137
00:04:45,254 --> 00:04:46,794
1から1000までの範囲、

138
00:04:46,800 --> 00:04:47,992
とすると、

139
00:04:47,992 --> 00:04:49,300
つまり 1～1000 平方フィートとすると、

140
00:04:49,310 --> 00:04:50,918
その場合、住居サイズの二乗は、

141
00:04:50,930 --> 00:04:52,175
1 から 100万、つまり

142
00:04:52,175 --> 00:04:54,519
1000 の二乗までの

143
00:04:54,520 --> 00:04:55,953
範囲になる。

144
00:04:55,953 --> 00:04:58,468
そして三番目の特徴量、

145
00:04:58,490 --> 00:05:01,335
xの三乗、、、失礼。

146
00:05:01,360 --> 00:05:03,106
三番目の特徴量xの3は、

147
00:05:03,120 --> 00:05:04,732
これは住宅の広さの三乗だが、

148
00:05:04,732 --> 00:05:05,941
これの範囲は

149
00:05:05,950 --> 00:05:07,478
1から10の9乗までとなってしまう。

150
00:05:07,478 --> 00:05:09,311
だからこれら三つの特徴量は

151
00:05:09,330 --> 00:05:10,955
取る値の範囲が、

152
00:05:10,955 --> 00:05:13,459
非常に異なる。

153
00:05:13,490 --> 00:05:15,105
だからフィーチャースケーリングを適用するのが重要だ。

154
00:05:15,110 --> 00:05:16,509
最急降下法を使う時に

155
00:05:16,509 --> 00:05:18,554
それらがお互いに比較可能な

156
00:05:18,554 --> 00:05:21,139
範囲の値になるように。

157
00:05:21,140 --> 00:05:23,243
最後に、ここに使う特徴量の選択が

158
00:05:23,250 --> 00:05:25,138
どれだけ幅広いものかを示す

159
00:05:25,150 --> 00:05:29,056
最後の例を挙げておこう。

160
00:05:29,090 --> 00:05:30,446
前に、このような二次のモデルは、

161
00:05:30,446 --> 00:05:31,559
あまり良くないかもしれない、と言った。

162
00:05:31,559 --> 00:05:33,122
何故なら、

163
00:05:33,122 --> 00:05:34,408
二次のモデルはデータに良くフィットしているように

164
00:05:34,408 --> 00:05:35,952
思えたとしても、

165
00:05:35,952 --> 00:05:37,514
やがて降下して戻ってきてしまう。

166
00:05:37,514 --> 00:05:39,065
しかし、実際に住居の価格が

167
00:05:39,070 --> 00:05:40,352
下落する、とは予測したくない、

168
00:05:40,352 --> 00:05:43,567
住居の広さが広くなるほどに。

169
00:05:43,567 --> 00:05:45,388
三次のモデルに

170
00:05:45,388 --> 00:05:46,938
進まずに、

171
00:05:46,938 --> 00:05:48,389
他の特徴量の選択など、

172
00:05:48,389 --> 00:05:50,798
様々な選択肢がありうる。

173
00:05:50,800 --> 00:05:52,313
その中で一つだけ

174
00:05:52,313 --> 00:05:53,691
ありそうな選択の例を

175
00:05:53,691 --> 00:05:55,620
挙げておくと、

176
00:05:55,620 --> 00:05:57,263
住居の価格は、

177
00:05:57,263 --> 00:05:58,832
シータ0 足す、

178
00:05:58,850 --> 00:05:59,992
シータ1 掛ける 広さ に、

179
00:05:59,992 --> 00:06:01,264
さらに足す、

180
00:06:01,320 --> 00:06:03,625
シータ2 掛ける 広さのルート。これ。

181
00:06:03,630 --> 00:06:05,364
ルート関数は、

182
00:06:05,364 --> 00:06:08,110
こんな感じの関数だ。

183
00:06:08,110 --> 00:06:09,318
そして何らかのシータ1, シータ2,

184
00:06:09,318 --> 00:06:11,355
シータ3の値があって、

185
00:06:11,355 --> 00:06:14,049
こんなモデルとなる。

186
00:06:14,080 --> 00:06:15,445
そしてそのカーブは

187
00:06:15,445 --> 00:06:16,952
こんな感じに

188
00:06:16,952 --> 00:06:19,500
上がっていくが、

189
00:06:19,520 --> 00:06:21,529
徐々に平坦になっていき、

190
00:06:21,540 --> 00:06:23,877
戻ってくることはない。

191
00:06:24,154 --> 00:06:26,584
だから、洞察を元に――この場合には、

192
00:06:26,584 --> 00:06:27,630
ルートの関数の形と

193
00:06:27,630 --> 00:06:30,952
データの形――

194
00:06:30,990 --> 00:06:32,555
別の特徴量を選ぶと、

195
00:06:32,555 --> 00:06:36,469
より良いモデルが得られることがある。

196
00:06:36,469 --> 00:06:39,026
このビデオでは、多項式回帰について議論した。

197
00:06:39,026 --> 00:06:40,672
それは多項式、例えば二次関数や三次関数などを

198
00:06:40,672 --> 00:06:42,298
データにフィッティングさせる

199
00:06:42,298 --> 00:06:43,868
やり方だ。

200
00:06:43,868 --> 00:06:45,112
そしてそのアイデアを通して、

201
00:06:45,112 --> 00:06:46,640
どの特徴量を使うか、という、

202
00:06:46,640 --> 00:06:47,732
選択肢があることも扱った。

203
00:06:47,748 --> 00:06:48,804
例えば、住居の間口と

204
00:06:48,804 --> 00:06:50,078
奥行を使う代わりに、

205
00:06:50,078 --> 00:06:51,092
それらを掛けあわせて

206
00:06:51,092 --> 00:06:53,133
住居の面積を

207
00:06:53,133 --> 00:06:55,317
得ることもできる。

208
00:06:55,317 --> 00:06:57,551
どうすれば良いか困る、と思うかもしれない。

209
00:06:57,551 --> 00:06:58,895
こんなに多くの、別々の特徴量の選択から

210
00:06:58,896 --> 00:07:03,265
どのように使う特徴量を決めればいいのか。

211
00:07:03,265 --> 00:07:04,594
このクラスの後半では、

212
00:07:04,594 --> 00:07:06,622
どの特徴量を使うのかを、

213
00:07:06,622 --> 00:07:08,083
自動的に選ぶアルゴリズムも議論する。

214
00:07:08,083 --> 00:07:09,466
つまり、アルゴリズムがデータを見て、

215
00:07:09,466 --> 00:07:10,611
自動的に

216
00:07:10,611 --> 00:07:12,040
使うべき特徴量を選んでくれる。

217
00:07:12,040 --> 00:07:13,357
二次関数にフィッティングしたいか、

218
00:07:13,357 --> 00:07:15,528
三次関数か、またはそれ以外か。

219
00:07:15,528 --> 00:07:17,164
だが、それらのアルゴリズムを

220
00:07:17,164 --> 00:07:18,764
得るまでの間は、

221
00:07:18,764 --> 00:07:20,295
当面は、とりあえず

222
00:07:20,295 --> 00:07:21,582
どの特徴量を使うかについては、

223
00:07:21,582 --> 00:07:23,094
選択肢がある、ということを意識しておいて欲しい。

224
00:07:23,094 --> 00:07:25,256
そして異なる特徴量を採用することで、

225
00:07:25,256 --> 00:07:26,888
単に直線をフィットさせるよりも

226
00:07:26,888 --> 00:07:28,156
複雑な関数を

227
00:07:28,156 --> 00:07:30,471
データにフィットさせることができ、

228
00:07:30,471 --> 00:07:32,092
特に、多項式も同様に

229
00:07:32,092 --> 00:07:35,065
組み込むことができ、

230
00:07:35,065 --> 00:07:36,072
適切な知見を

231
00:07:36,072 --> 00:07:37,564
特徴量に織り込むことで、

232
00:07:37,564 --> 00:07:40,020
データに対し、より良いモデルが得られることがある。
