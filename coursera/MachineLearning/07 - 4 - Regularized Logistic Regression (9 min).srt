1
00:00:00,160 --> 00:00:01,480
ロジスティック回帰については

2
00:00:02,110 --> 00:00:04,730
以前2つの種類の最適化アルゴリズムについて議論した。

3
00:00:05,190 --> 00:00:06,190
1つ目は最急降下法を使って

4
00:00:06,560 --> 00:00:09,210
コスト関数 J のシータを最適化する方法。

5
00:00:09,690 --> 00:00:10,770
2つ目として、高度な最適化関数を

6
00:00:11,120 --> 00:00:12,730
使う方法も議論した。

7
00:00:13,520 --> 00:00:14,670
それはあなたが

8
00:00:14,790 --> 00:00:16,300
コスト関数 J のシータの

9
00:00:16,940 --> 00:00:18,160
計算方法と、

10
00:00:18,420 --> 00:00:20,920
偏微分の計算方法を提供する必要があるものだった。

11
00:00:22,450 --> 00:00:23,920
このビデオでは、これらの

12
00:00:24,190 --> 00:00:25,420
どちらのテクニックにも

13
00:00:25,500 --> 00:00:27,570
適用する方法、つまり最急降下法と

14
00:00:27,720 --> 00:00:29,350
より高度な最適化技法の両方で

15
00:00:30,280 --> 00:00:31,770
それらを正規化したロジスティック回帰で

16
00:00:31,950 --> 00:00:33,550
使えるようにする方法をお見せしたい。

17
00:00:35,430 --> 00:00:36,670
さて、そのアイデアはこんなだ。

18
00:00:37,260 --> 00:00:38,770
前に、ロジスティック回帰は

19
00:00:39,190 --> 00:00:40,490
とても高い次数の

20
00:00:40,850 --> 00:00:42,540
このような多項式を含む場合は

21
00:00:42,810 --> 00:00:44,090
オーバーフィットしがちだと

22
00:00:44,290 --> 00:00:45,890
いうことを見てきた。

23
00:00:46,470 --> 00:00:48,250
ここでgは

24
00:00:48,480 --> 00:00:49,970
sigmoid関数だ。

25
00:00:50,030 --> 00:00:51,330
つまり、最終的に

26
00:00:51,530 --> 00:00:53,020
とても、過度に複雑な

27
00:00:53,150 --> 00:00:54,120
まったく直感に反するような

28
00:00:54,360 --> 00:00:55,930
決定境界の仮説になるうなものの場合には、ということ。

29
00:00:56,620 --> 00:00:58,600
つまりはそんな仮説は

30
00:00:58,820 --> 00:00:59,680
このトレーニングセットに対して

31
00:00:59,790 --> 00:01:01,000
そんなに良い仮説とは 思えない。

32
00:01:01,350 --> 00:01:02,990
より一般的に言うと、

33
00:01:03,120 --> 00:01:04,890
特徴量の多いロジスティック回帰の時には、

34
00:01:05,150 --> 00:01:06,630
その特徴量は必ずしも多項式でなくても、

35
00:01:06,670 --> 00:01:07,670
特徴量が多い時には

36
00:01:07,670 --> 00:01:09,720
ロジスティック回帰はオーバーフィットしがちだ。

37
00:01:11,620 --> 00:01:14,010
これが私たちのロジスティック回帰のコスト関数だ。

38
00:01:14,810 --> 00:01:16,210
そしてそれを正規化を用いるように

39
00:01:16,740 --> 00:01:18,820
修正したければ、

40
00:01:18,950 --> 00:01:20,630
そこに以下の項を

41
00:01:20,820 --> 00:01:22,290
追加するだけだ、

42
00:01:22,650 --> 00:01:24,860
+ ラムダ/2m の

43
00:01:25,110 --> 00:01:26,580
和を取ることのj=1から、、、

44
00:01:26,730 --> 00:01:29,670
ここでいつも通り、和は j=1 から取る、

45
00:01:29,800 --> 00:01:31,000
j=0 からではない。

46
00:01:31,550 --> 00:01:33,670
で、シータjの二乗

47
00:01:34,330 --> 00:01:35,470
そしてこれはつまり

48
00:01:35,750 --> 00:01:36,960
シータ1, シータ2, ...とシータnまでが

49
00:01:37,650 --> 00:01:39,140
大きすぎる場合に

50
00:01:39,570 --> 00:01:42,600
ペナルティを課す、という意味だ。

51
00:01:43,610 --> 00:01:44,720
これを行えば、

52
00:01:45,720 --> 00:01:46,750
とても高次の多項式で、

53
00:01:46,750 --> 00:01:48,870
パラメータが多いようなものに

54
00:01:49,250 --> 00:01:51,500
フィッティングしたとしても、

55
00:01:52,210 --> 00:01:53,240
正規化を適用しておけば、

56
00:01:53,910 --> 00:01:55,090
それによってパラメータが小さいままに保たれ、

57
00:01:55,850 --> 00:01:57,580
こんな感じの決定境界を

58
00:01:58,830 --> 00:02:00,040
得られる可能性が高まる。

59
00:02:00,320 --> 00:02:01,460
こちらの方が、陽性と陰性の手本を分離するには

60
00:02:02,500 --> 00:02:03,740
より妥当と言えよう。

61
00:02:05,300 --> 00:02:06,970
つまり、正規化を使えば、

62
00:02:08,140 --> 00:02:09,080
特徴量が多い時でも、

63
00:02:09,220 --> 00:02:11,110
正規化がオーバーフィッティングの問題を

64
00:02:11,620 --> 00:02:13,500
対処してくれる。

65
00:02:14,740 --> 00:02:15,790
実際にはどうやって実装したらいいか？

66
00:02:16,720 --> 00:02:18,280
元々の最急降下法のアルゴリズムでは、

67
00:02:18,710 --> 00:02:20,380
これが私たちの得た更新だった。

68
00:02:20,670 --> 00:02:22,300
私たちは繰り返し、以下の更新を

69
00:02:22,750 --> 00:02:24,610
シータjに対して適用するのだった。

70
00:02:24,740 --> 00:02:26,940
このスライドは前回の線形回帰のスライドと、多くの点で似ている。

71
00:02:27,510 --> 00:02:28,460
とにかく、私がやることは、まずシータ0の更新を

72
00:02:29,210 --> 00:02:31,390
別個に書く。

73
00:02:31,670 --> 00:02:32,930
こうして、最初の行がシータ0の

74
00:02:33,060 --> 00:02:34,110
更新で、

75
00:02:34,230 --> 00:02:35,470
二行目が、

76
00:02:35,590 --> 00:02:36,730
シータ1からシータnまでの

77
00:02:36,880 --> 00:02:38,470
更新となった。

78
00:02:38,900 --> 00:02:40,740
何故ならシータ0は別扱いだから。

79
00:02:41,700 --> 00:02:43,140
そしてこのアルゴリズムを

80
00:02:43,700 --> 00:02:45,370
修正して

81
00:02:46,770 --> 00:02:48,480
正規化したコスト関数を用いるようにするためには、

82
00:02:49,100 --> 00:02:50,510
私が行うべきことは

83
00:02:50,950 --> 00:02:51,810
線形回帰の時と

84
00:02:51,930 --> 00:02:53,700
極めて似ている。

85
00:02:53,870 --> 00:02:55,620
それは、この二番目の更新規則を

86
00:02:55,890 --> 00:02:57,480
以下のように変更するだけだ。

87
00:02:58,510 --> 00:02:59,800
そしてまた、見た目でも

88
00:03:00,380 --> 00:03:02,080
線形回帰の頃にあったものと

89
00:03:02,230 --> 00:03:03,720
同じに見える。

90
00:03:04,580 --> 00:03:05,580
だがもちろん、以前のアルゴリズムと

91
00:03:05,660 --> 00:03:06,590
同じアルゴリズムではない。

92
00:03:06,890 --> 00:03:08,370
何故なら今や、仮説は

93
00:03:08,780 --> 00:03:10,420
これを用いて定義されているから。

94
00:03:10,860 --> 00:03:12,550
だからこれは、正規化された線形回帰と

95
00:03:13,130 --> 00:03:14,390
同じアルゴリズムではない。

96
00:03:14,830 --> 00:03:16,340
何故なら仮説が違うから。

97
00:03:16,940 --> 00:03:18,360
確かに、ここに書きだした更新は

98
00:03:18,630 --> 00:03:20,160
表面上は、以前に得たものと

99
00:03:20,350 --> 00:03:22,130
まったく同じではある。

100
00:03:22,480 --> 00:03:25,310
正規化した線形回帰の時に導出したものと。

101
00:03:26,690 --> 00:03:27,720
そしてもちろん、、、

102
00:03:27,830 --> 00:03:29,360
この議論をまとめると、

103
00:03:29,560 --> 00:03:30,860
この大カッコにくくられた

104
00:03:31,130 --> 00:03:32,330
この項、つまりこのここの項は、

105
00:03:32,670 --> 00:03:35,120
この項は、

106
00:03:35,410 --> 00:03:36,750
もちろん、新しいコスト関数Jのシータの

107
00:03:37,210 --> 00:03:38,590
シータjでの偏微分という

108
00:03:38,660 --> 00:03:41,420
新しい偏微分項だ。

109
00:03:42,300 --> 00:03:43,480
ここで、このJのシータは

110
00:03:43,700 --> 00:03:44,980
前のスライドで定義した、正規化項ありの

111
00:03:45,180 --> 00:03:48,100
コスト関数だ。

112
00:03:49,770 --> 00:03:52,060
以上が正規化した線形回帰だ（訳注：ロジスティック回帰の間違いと思われる、以下同様）

113
00:03:55,200 --> 00:03:56,430
ここからは、正規化した線形回帰を

114
00:03:56,580 --> 00:03:58,290
高度な最適化関数と

115
00:03:58,950 --> 00:04:00,010
どう使っていくかを

116
00:04:00,360 --> 00:04:02,070
議論していこう。

117
00:04:03,180 --> 00:04:05,590
これらの関数を

118
00:04:05,840 --> 00:04:06,800
思い出しておくために触れておくと、

119
00:04:07,080 --> 00:04:08,390
これらの関数のために私たちがすべきことは、

120
00:04:08,450 --> 00:04:09,460
costFuncitonという関数を定義することだった、

121
00:04:09,640 --> 00:04:11,160
それは入力に

122
00:04:11,280 --> 00:04:13,660
パラメータベクトルのシータを受け取り、

123
00:04:13,790 --> 00:04:16,180
ここで今回も、この等式では

124
00:04:16,770 --> 00:04:19,030
0インデックスのベクトルとして書いた。

125
00:04:19,510 --> 00:04:20,690
だからシータ0から

126
00:04:21,180 --> 00:04:22,810
シータnまである。

127
00:04:23,020 --> 00:04:25,920
だがOctaveはベクトルのインデックスを1から始めるから、

128
00:04:26,820 --> 00:04:28,240
シータ0はOctave上では

129
00:04:28,560 --> 00:04:29,990
theta1と書く。

130
00:04:30,120 --> 00:04:31,630
シータ1はOctave上では

131
00:04:31,860 --> 00:04:32,930
theta2と書く。

132
00:04:33,280 --> 00:04:35,070
そんな風にtheta(n+1)まで

133
00:04:35,270 --> 00:04:36,650
降りていく。

134
00:04:36,740 --> 00:04:38,450
そして私たちがやるべきことは、

135
00:04:38,600 --> 00:04:40,240
以下のような関数を提供すること。

136
00:04:41,170 --> 00:04:42,370
costFunctionという関数を

137
00:04:42,780 --> 00:04:44,140
提供することとしよう、

138
00:04:44,360 --> 00:04:46,920
それを以前に見たものに渡す。

139
00:04:47,300 --> 00:04:48,490
fminunc を使って、

140
00:04:49,060 --> 00:04:50,310
それに対し、引数を、

141
00:04:50,540 --> 00:04:52,160
アットマークにコスト関数。

142
00:04:54,530 --> 00:04:55,530
などとする。

143
00:04:55,600 --> 00:04:56,870
fminunc は

144
00:04:57,030 --> 00:04:58,060
fmin の unconstrained（制約なし）

145
00:04:58,280 --> 00:04:59,310
だった。そしてこの

146
00:04:59,650 --> 00:05:01,230
fminuncは、

147
00:05:01,310 --> 00:05:02,300
最小化する対象を受け取って

148
00:05:02,540 --> 00:05:04,340
最小化してくれる。

149
00:05:05,950 --> 00:05:07,050
コスト関数が返すべきものは

150
00:05:07,170 --> 00:05:08,600
主に二つ、

151
00:05:08,700 --> 00:05:10,620
最初はjVal。

152
00:05:11,280 --> 00:05:12,400
そのためには

153
00:05:12,720 --> 00:05:16,950
コスト関数Jのシータを計算するコードを書く。

154
00:05:16,950 --> 00:05:16,950


155
00:05:17,130 --> 00:05:19,030
ここでは正規化したロジスティック回帰を使っているのだから、

156
00:05:19,450 --> 00:05:20,920
当然、コスト関数Jのシータも

157
00:05:20,990 --> 00:05:21,960
前とは変わっている。

158
00:05:22,280 --> 00:05:23,450
具体的には、

159
00:05:24,480 --> 00:05:25,760
今回はコスト関数には、

160
00:05:25,870 --> 00:05:29,580
末尾に追加の正規化項を含む必要がある。

161
00:05:29,850 --> 00:05:30,930
だから、Jのシータを計算する時に

162
00:05:31,030 --> 00:05:33,410
最後に項を追加することを忘れないように。

163
00:05:34,590 --> 00:05:35,520
そしてその次に

164
00:05:36,050 --> 00:05:37,240
このコスト関数が提供しなくてはいけないものは

165
00:05:37,690 --> 00:05:39,010
gradientだ。

166
00:05:39,530 --> 00:05:41,170
gradient1には、

167
00:05:41,400 --> 00:05:42,570
Jのシータの

168
00:05:42,660 --> 00:05:44,080
シータ0による偏微分を

169
00:05:44,240 --> 00:05:45,520
設定する。

170
00:05:45,690 --> 00:05:47,170
gradient2には

171
00:05:47,580 --> 00:05:49,520
これを設定する。などなど。

172
00:05:49,780 --> 00:05:50,900
ここでも、インデックスは1ずれている。

173
00:05:51,220 --> 00:05:52,850
何故ならOctaveは1からのインデックスを

174
00:05:53,110 --> 00:05:54,450
使うから。

175
00:05:55,940 --> 00:05:56,780
そしてこれらの項を見ると、

176
00:05:57,850 --> 00:05:58,680
ここのこの項は

177
00:05:59,410 --> 00:06:00,640
前回のスライドで計算したものと

178
00:06:00,720 --> 00:06:02,840
同じで、これに等しい。

179
00:06:03,030 --> 00:06:03,940
変わっていない。

180
00:06:04,120 --> 00:06:07,250
何故ならシータ0による微分は前と同じだから。

181
00:06:07,650 --> 00:06:09,540
正規化をしなかったバージョンと。

182
00:06:10,960 --> 00:06:13,210
そしてそれ以外の項は、変わる。

183
00:06:13,840 --> 00:06:16,340
具体的には、シータ1に関する微分は、

184
00:06:17,010 --> 00:06:18,830
前回のスライドでやったのと同様で、

185
00:06:19,110 --> 00:06:20,670
イコール、

186
00:06:20,890 --> 00:06:22,560
元の項に、そこからマイナスの

187
00:06:23,450 --> 00:06:24,870
ラムダ/m 掛ける シータ1。

188
00:06:25,310 --> 00:06:27,140
これを渡していることをしっかり確認してほしい。

189
00:06:27,800 --> 00:06:29,370
そしてここにカッコをつけられる、

190
00:06:29,830 --> 00:06:30,980
和を取るのがここまで行ってしまわないように。

191
00:06:31,570 --> 00:06:33,160
同様に、

192
00:06:33,380 --> 00:06:34,800
この他の項も、これも

193
00:06:35,130 --> 00:06:36,180
こんな感じで、この追加の項があり、

194
00:06:37,070 --> 00:06:37,950
これは前のスライドのものと同じだ、

195
00:06:38,030 --> 00:06:39,770
同様で、これは正規化の目的関数の

196
00:06:39,950 --> 00:06:41,450
微分から来ている。

197
00:06:42,230 --> 00:06:43,650
さて、このcostFunctionを

198
00:06:43,820 --> 00:06:45,140
実装して、これを

199
00:06:45,720 --> 00:06:47,370
この fminunc や

200
00:06:48,190 --> 00:06:49,160
それ以外の高度な最適化技法の一つなどに渡せば、

201
00:06:50,050 --> 00:06:51,940
それがこの新しく作った正規化したコスト関数、Jのシータを

202
00:06:52,540 --> 00:06:55,990
最小化してくれることになる。

203
00:06:56,990 --> 00:06:58,220
そして得られるパラメータは、

204
00:06:59,530 --> 00:07:00,740
ロジスティック回帰に正規化を含めたものに

205
00:07:01,450 --> 00:07:02,940
対応している。

206
00:07:04,410 --> 00:07:05,540
さて、ここまでで正規化されたロジスティック回帰を

207
00:07:05,780 --> 00:07:08,210
どう実装するのかを知ったわけだ。

208
00:07:09,780 --> 00:07:10,920
シリコンバレーを回っていると、

209
00:07:11,380 --> 00:07:12,900
私はシリコンバレーに住んでいるのだが、

210
00:07:13,100 --> 00:07:14,900
機械学習のアルゴリズムを用いて

211
00:07:15,420 --> 00:07:16,490
会社に巨万の富をもたらしているエンジニアが

212
00:07:16,610 --> 00:07:18,090
たくさんいる。

213
00:07:19,180 --> 00:07:20,390
そしてここまでで私たちが学んで来た期間は

214
00:07:20,600 --> 00:07:22,860
まだ短いが、

215
00:07:23,620 --> 00:07:25,410
だが線形回帰を理解し、ロジスティック回帰を理解し、

216
00:07:26,510 --> 00:07:28,360
高度な最適化アルゴリズムを理解し、

217
00:07:29,210 --> 00:07:30,710
正規化を理解した今、

218
00:07:30,950 --> 00:07:32,520
率直にいってたぶん、

219
00:07:32,950 --> 00:07:34,270
比較的、かなり多く機械学習について
知っていることになると思う、

220
00:07:35,010 --> 00:07:36,290
ほとんどのエンジニアより、というと言い過ぎだが、

221
00:07:36,750 --> 00:07:38,050
だがたぶん、現時点まででも既に、
機械学習について多くのエンジニアよりも、

222
00:07:38,180 --> 00:07:39,580
もっと多くのことを知っている、

223
00:07:40,240 --> 00:07:42,670
シリコンバレーでとても成功したキャリアを得ている

224
00:07:42,820 --> 00:07:44,760
多くのエンジニアたちと比べて、

225
00:07:45,300 --> 00:07:46,420
会社に巨万の富を生み出しているエンジニアたちや

226
00:07:47,050 --> 00:07:49,950
機械学習のアルゴリズムを使って
プロダクトを作っているエンジニアたちと比べて。

227
00:07:50,370 --> 00:07:51,460
つまり、、、おめでとう！

228
00:07:52,080 --> 00:07:53,120
すでにあなたは、随分遠くまで来たということだ。

229
00:07:53,490 --> 00:07:54,550
そしてあなたは実際に

230
00:07:54,780 --> 00:07:55,990
多くの問題にこれらのことを適用するための

231
00:07:56,310 --> 00:07:58,210
十分な知識を得ているのだ。

232
00:07:59,260 --> 00:08:00,580
だからそれについて、おめでとう！と言いたい。

233
00:08:00,780 --> 00:08:01,880
だがもちろん、まだ教えたいことは

234
00:08:02,350 --> 00:08:03,280
他にもいろいろある。

235
00:08:03,400 --> 00:08:05,180
そこでこれに続く

236
00:08:05,380 --> 00:08:06,540
一連のビデオで

237
00:08:06,560 --> 00:08:07,850
とても強力な非線型の分類器の

238
00:08:08,030 --> 00:08:10,890
クラスについて、議論を開始したい。

239
00:08:11,680 --> 00:08:13,350
線形回帰やロジスティック回帰にも

240
00:08:13,690 --> 00:08:14,940
すでに知っての通り、

241
00:08:15,080 --> 00:08:17,310
多項式の項を入れこむことができるが、

242
00:08:17,460 --> 00:08:18,350
多項式の回帰よりもよりパワフルな

243
00:08:18,510 --> 00:08:21,150
非線型の分類器が存在するということが

244
00:08:21,460 --> 00:08:23,650
既に知られている。

245
00:08:24,640 --> 00:08:25,780
そしてこの後の一連のビデオで

246
00:08:25,810 --> 00:08:28,280
それらについて教えていきたい。

247
00:08:28,510 --> 00:08:29,560
様々な問題に適用できて、

248
00:08:29,760 --> 00:08:30,760
今の手持ちのアルゴリズム以上に

249
00:08:31,380 --> 00:08:32,870
強力なアルゴリズムを得るために。
