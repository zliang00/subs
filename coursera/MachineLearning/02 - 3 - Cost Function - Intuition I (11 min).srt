1
00:00:00,000 --> 00:00:04,100
前のビデオで数学的に目的関数を定義した。

2
00:00:04,100 --> 00:00:08,616
このビデオでは、いくつか具体例を見て、目的関数が何をしていて

3
00:00:08,616 --> 00:00:14,466
なぜそれを使いたいのか、直感的な理解を得て行く。

4
00:00:14,466 --> 00:00:19,396
復習をすると、これが最後に見たものだ。
データに直線を当てはめようとしており、

5
00:00:19,396 --> 00:00:23,958
この数式を仮説として用意し、そのパラメータは theta 0 と theta 1 で

6
00:00:23,958 --> 00:00:28,888
そしてパラメータの選択次第で、異なる直線となることを見た。
ここにデータがあり、h(x) がこうなる。

7
00:00:31,323 --> 00:00:33,758
そして目的関数があり、

8
00:00:33,758 --> 00:00:38,554
そしてそれが最適化の目的だった。

9
00:00:38,554 --> 00:00:43,293
このビデオでは、目的関数 J をより可視化するため、簡素化された仮説関数を使う。

10
00:00:43,293 --> 00:00:48,220
それを右に表示す。これからこの簡素化された仮説を使っていく。

11
00:00:48,220 --> 00:00:53,275
これは、単純に theta 1  x だ。これは見方を変えれば、

12
00:00:53,275 --> 00:00:58,721
パラメータ theta 0 = 0 と設定したものと見なすことができる。
だからパラメータは theta 1 の一つだけとなり、

13
00:00:58,721 --> 00:01:04,372
目的関数は前と似ているが、違いは h(x) 、これが今度は

14
00:01:04,372 --> 00:01:10,309
= theta 1  1 となっている点だ。そしてパラメータは theta 1 一つだけなので、

15
00:01:10,309 --> 00:01:16,246
最適化の目的は J(theta 1) の最小化となる。この意味を図で示すと、

16
00:01:16,246 --> 00:01:21,611
theta 0 = 0 だとすると、それが相当するのは、

17
00:01:21,611 --> 00:01:27,176
仮説関数として原点を通るもの、点 (0, 0) を通るものだけを選ぶことだ。

18
00:01:27,176 --> 00:01:33,415
この簡素化された仮説の定義と目的関数を使って、目的関数の理解を深めよう。

19
00:01:33,415 --> 00:01:40,178
実は、理解したいのは二つの重要な関数だ。

20
00:01:40,178 --> 00:01:46,432
最初に仮説関数、そして二番目に目的関数だ。さて、気づいただろうか。

21
00:01:46,432 --> 00:01:52,068
仮説、つまり h(x) 、
これは theta 1 の値が固定されていれば、これは x に対する関数だ。

22
00:01:52,068 --> 00:01:58,168
だから、仮説は家の広さ x に対する関数だ。

23
00:01:58,168 --> 00:02:03,959
対照的に、目的関数 J は、パラメータ theta 1 に対する関数で、

24
00:02:03,959 --> 00:02:09,993
それは直線の傾きを決定する。これらの関数をプロットして

25
00:02:09,993 --> 00:02:15,481
この両方をもっと理解してみよう。では仮説から始めよう。

26
00:02:15,481 --> 00:02:20,283
左は、例えば訓練セットがこのように3つの点
 (1, 1), (2, 2), (3, 3) から成り立っているとする。

27
00:02:20,283 --> 00:02:25,338
では theta 1 の値を選ぶ。
仮に theta 1 = 1 の場合、そのように theta 1 の値を選ぶと、

28
00:02:25,338 --> 00:02:30,392
すると仮説はこの直線のようになる。

29
00:02:30,392 --> 00:02:35,234
すこし指摘すると、仮説関数をプロットする時には x 軸、

30
00:02:35,234 --> 00:02:40,525
横軸は x とラベルがつけられていて、家の広さを表す。

31
00:02:40,525 --> 00:02:46,551
今は、一時的に theta 1 = 1 とした。

32
00:02:46,551 --> 00:02:52,430
次に行うのは theta 1 = 1 の時に、J(theta 1) がどうなるか見ることだ。

33
00:02:52,430 --> 00:02:58,781
では早速 theta 1 の値を 1 として目的関数を計算しよう。

34
00:02:58,781 --> 00:03:05,761
通常通り、目的関数は以下のように定義される。訓練セットの総計、

35
00:03:05,761 --> 00:03:13,840
通常通りの二乗誤差項。従って、これは、これに等しくなる。

36
00:03:14,740 --> 00:03:25,066
theta 1  x(i) - y(i) そしてこれを簡略化するとこうなる、

37
00:03:25,066 --> 00:03:31,995
0 の二乗足す 0 の二乗足す 0 の二乗、そしてこれはもちろん、0 に等しくなる。

38
00:03:31,995 --> 00:03:39,098
さて、目的関数の内部では、それぞれの項が 0 と等しくなった。
なぜなら、ここにある特定の訓練セット、

39
00:03:39,098 --> 00:03:46,288
3つの訓練サンプルが (1, 1)、(2, 2)、(3, 3) だからだ。

40
00:03:46,288 --> 00:03:54,667
もし theta 1 = 1 なら、h(x(i)) が y(i) に正に等しくなる。

41
00:03:54,667 --> 00:04:04,164
きれい書き直すと、こうだ。だから、h(x) - y のそれぞれの項は 0 となり

42
00:04:04,164 --> 00:04:14,821
それが理由で、J(1) = 0 だと導いた。

43
00:04:14,821 --> 00:04:20,504
さて、これで J(i) が 0 に等しいと判明したので、それをプロットする。

44
00:04:20,504 --> 00:04:26,187
右側で行うのは、目的関数 J のプロットだ。ご覧の通り、目的関数はパラメータ

45
00:04:26,187 --> 00:04:32,017
theta 1 の関数なので、目的関数をプロットする時は、
横軸のラベルは theta 1 になる。

46
00:04:32,017 --> 00:04:38,069
だから、J(1) は 0 なので、それをプロットするとこの x の位置になる。

47
00:04:38,069 --> 00:04:46,464
では、他の例もいくつか見てみよう。

48
00:04:46,464 --> 00:04:52,470
theta 1 はある範囲の別の値を取ることができる。
theta 1 は負の値、0、あるいは正の値を取ることができる。

49
00:04:52,470 --> 00:04:58,876
では、theta 1 = 0.5 の場合はどうだろう。何が起きるだろう。早速プロットしよう。

50
00:04:58,876 --> 00:05:05,442
ここで theta 1 = 0.5 に設定し、すると仮説はこうなる

51
00:05:05,442 --> 00:05:11,688
線の傾きは 0.5 に等しくなる。では J を計算しよう。

52
00:05:11,688 --> 00:05:17,855
J(0.5)。 これは 1/2m、 いつもの目的関数だ。

53
00:05:17,855 --> 00:05:23,769
実は、目的関数は、次の値の二乗の合計となる。

54
00:05:23,769 --> 00:05:29,609
この線の高さ、足す、この線の高さ、足す、この線の高さ。

55
00:05:29,609 --> 00:05:34,783
いいかな。この垂直の距離、

56
00:05:34,783 --> 00:05:42,854
これは y(i) と 予測値 h(x(i)) との差分だ。

57
00:05:42,854 --> 00:05:48,772
いいかな。だから、最初のサンプルは (0.5 - 1) の二乗だ。

58
00:05:49,033 --> 00:05:55,647
なぜなら、仮説の予測は 0.5 だが、一方、実際の値は 1 だったからだ。

59
00:05:55,647 --> 00:06:02,436
二番目のサンプルでは (1-2) の二乗になる。仮説が予測したのは 1 だが、

60
00:06:02,436 --> 00:06:09,663
実際の家の価格は 2 だったからだ。そして最後に、+ (1.5 -3) の二乗。

61
00:06:09,663 --> 00:06:17,263
だから、これは = 1/(2  3) に等しくなりる。

62
00:06:17,263 --> 00:06:24,274
なぜなら m は訓練セットのサイズで、ここには三つの訓練サンプルがあるから。
そしてそれ掛ける

63
00:06:24,274 --> 00:06:33,011
括弧の中を簡略化すると 3.5 だ。だから、3.5/6 となり、それは大体 0.68 だ。

64
00:06:33,011 --> 00:06:41,085
これで分かったのは、J(0.5) が約 0.68 になるということだ。それをプロットしよう。

65
00:06:41,085 --> 00:06:50,308
おっと失礼した。計算ミス。これは実は 0.58 だ。
それをプロットすると、このあたりになる。

66
00:06:50,308 --> 00:07:00,293
いいかな。さらに、もう一つやろう。もし theta 1 = 0 の場合はどうか。

67
00:07:00,293 --> 00:07:08,975
J(0) の値はどうなるか。実は、theta 1 = 0 の場合、

68
00:07:08,975 --> 00:07:16,916
結果的に h(x) は、この横線に等しくなり、このように水平になる。

69
00:07:16,916 --> 00:07:26,882
だから、誤差を計算すると、J(0) = 1/(2m) 掛ける

70
00:07:26,882 --> 00:07:34,659
(1^2 + 2^2 + 3^3)、すなわち

71
00:07:34,659 --> 00:07:41,555
1/6  14、これは大体 2.3 だ。ではこれも早速プロットしてみよう。

72
00:07:41,555 --> 00:07:47,622
これは 2.3 の値になる。そして、もちろんこれを続けて、

73
00:07:47,622 --> 00:07:53,335
他の theta 1 の値で試すこともできる。実は負の値を theta 1 に使うこともできる。

74
00:07:53,335 --> 00:07:59,327
ですから、theta 1 が負なら、h(x) は、

75
00:07:59,327 --> 00:08:05,179
例えば -0.5  x とする。ということは theta 1 = -0.5 なので、それが対応するのは

76
00:08:05,179 --> 00:08:10,188
-0.5 の傾きの仮説になる。そして引き続き、

77
00:08:10,188 --> 00:08:15,694
実際に誤差の計算を続けると、-0.5 の場合

78
00:08:15,694 --> 00:08:21,520
実は大きな誤差が発生する。結果的に、たしか 5.25 ぐらいになる。

79
00:08:21,520 --> 00:08:28,087
このように、異なる値の theta 1 で、こうした値を計算できる。

80
00:08:28,087 --> 00:08:34,413
そしてある範囲の値を計算するとこのような結果になる。

81
00:08:34,413 --> 00:08:40,499
そしてある範囲の値を計算することにより、だんだんとその値を辿っていくことができ、

82
00:08:40,499 --> 00:08:50,999
関数 J(theta) が何をしているかが見えてくる。これが J(theta) だ。

83
00:08:50,999 --> 00:08:57,851
まとめると、theta 1 のそれぞれの値は、異なる仮説に対応する。

84
00:08:57,851 --> 00:09:04,448
左側の異なる直線に当てはまる。そしてtheta 1 のそれぞれの値に対し、

85
00:09:04,448 --> 00:09:11,723
J(theta 1) の異なる値を得ることができる。

86
00:09:11,723 --> 00:09:19,354
例えば、theta 1 = 1 だと、この直線に対応しデータと一致した。

87
00:09:19,354 --> 00:09:27,846
一方、theta 1 = 0.5 では、マゼンタの色で示された点はこの線に対応し、

88
00:09:27,846 --> 00:09:35,340
theta 1 = 0 なら、青色で示す、

89
00:09:35,340 --> 00:09:41,527
それが対応するのは、この横線だ。いいかな。だから、theta 1 のそれぞれの値は

90
00:09:41,527 --> 00:09:48,516
結果的に J(theta) の異なる値に対応するので、
それを使って、右にあるようなプロットを辿ることができる。

91
00:09:48,516 --> 00:09:54,461
さて、思い返してもらいたい、学習アルゴリズムの最適化の目的は

92
00:09:54,461 --> 00:10:01,963
theta 1 の値を選択することだ。
それはJ(theta1) を最小化するものでなければならない。

93
00:10:01,963 --> 00:10:08,076
いいかな。これは線形回帰の目的関数だった。

94
00:10:08,076 --> 00:10:13,710
さて、この曲線を見ると、J(theta 1) を最小化する値は、見ての通り、theta 1 = 1 だ。

95
00:10:13,710 --> 00:10:19,132
そして、見て欲しい、それは正にデータに当てはめることができる最適な直線だ、

96
00:10:19,132 --> 00:10:24,624
theta 1 = 1 と設定することによって。この場合の訓練セットでは

97
00:10:24,624 --> 00:10:30,328
実際に完璧に適合した。そしてこれが、 J(theta 1) を最小化することが

98
00:10:30,328 --> 00:10:36,447
データによく当てはまる直線を見つけることに相当する理由だ。

99
00:10:36,447 --> 00:10:40,884
さて、まとめとして、このビデオでは、プロットをいくつか見た。

100
00:10:40,884 --> 00:10:45,259
目的関数を理解するためだ。そのために、アルゴリズムを簡素化し、

101
00:10:45,259 --> 00:10:50,258
パラメータを一つ theta 1 だけにし、パラメータ theta 0 は 0 に固定した。

102
00:10:50,258 --> 00:10:54,445
次のビデオでは、元々の問題の定式に戻り、

103
00:10:54,445 --> 00:10:59,570
theta 0 および theta 1 の両方を含む可視化を行う。

104
00:10:59,570 --> 00:11:04,757
つまり theta 0 を 0 に固定しない。これにより、元々の線形回帰の定式で、

105
00:11:04,757 --> 00:11:09,257
目的関数 J が何をしているか、さらによい直感的な理解を得られると思う。
