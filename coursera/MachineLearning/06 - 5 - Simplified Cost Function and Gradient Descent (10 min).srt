1
00:00:00,310 --> 00:00:02,286
このビデオでは

2
00:00:02,300 --> 00:00:03,903
ここまでのよりももう少しシンプルな

3
00:00:03,910 --> 00:00:06,513
コスト関数の書き方を探す。

4
00:00:06,520 --> 00:00:08,252
また、ロジスティック回帰の

5
00:00:08,252 --> 00:00:10,779
パラメータを最急降下法を使って

6
00:00:10,779 --> 00:00:13,321
フィッティングする方法を学ぶ。

7
00:00:13,321 --> 00:00:14,210
このビデオが終わる頃には

8
00:00:14,210 --> 00:00:15,589
完全に機能するバージョンの

9
00:00:15,589 --> 00:00:19,201
ロジスティック回帰を実装する方法を知ることになる。

10
00:00:19,201 --> 00:00:24,802
これが私たちのロジスティック回帰のコスト関数だ。

11
00:00:24,802 --> 00:00:27,613
全体のコスト関数は

12
00:00:27,613 --> 00:00:29,477
1/m 掛ける

13
00:00:29,477 --> 00:00:31,003
トレーニングセットすべてで和をとることの

14
00:00:31,003 --> 00:00:32,797
個々の手本における

15
00:00:32,797 --> 00:00:34,580
ラベル yi に対する、

16
00:00:34,580 --> 00:00:36,408
推定のコストだ。

17
00:00:36,408 --> 00:00:39,492
そしてこれは単体の手本に対するコストで、これは前にやった。

18
00:00:39,492 --> 00:00:40,604
そして思い出して欲しいのだが、

19
00:00:40,604 --> 00:00:43,514
分類問題については、

20
00:00:43,514 --> 00:00:45,778
私たちの訓練セットにおいては、

21
00:00:45,778 --> 00:00:47,076
いや、手本全体において

22
00:00:47,076 --> 00:00:48,915
y はいつでも

23
00:00:48,915 --> 00:00:51,056
0 か 1 のどちらかに等しい。

24
00:00:51,056 --> 00:00:51,056
いいかな？

25
00:00:51,056 --> 00:00:52,150
それが y の

26
00:00:52,150 --> 00:00:55,700
数学的な定義だった。

27
00:00:55,720 --> 00:00:57,430
y が 0 か 1 のどちらかなので

28
00:00:57,430 --> 00:00:59,453
このコスト関数を

29
00:00:59,460 --> 00:01:00,735
よりシンプルに

30
00:01:00,760 --> 00:01:03,001
書くことができる。

31
00:01:03,001 --> 00:01:04,980
このコスト関数を

32
00:01:04,980 --> 00:01:06,394
別々の二行の

33
00:01:06,410 --> 00:01:07,966
異なるケース、

34
00:01:07,966 --> 00:01:09,519
y=1 と y=0 のケースに分けて書く代わりに

35
00:01:09,519 --> 00:01:11,123
これら２つの行を

36
00:01:11,130 --> 00:01:12,687
一つの式に

37
00:01:12,687 --> 00:01:16,241
圧縮してみよう。

38
00:01:16,241 --> 00:01:17,743
こちらの方がコスト関数を書き出したり、

39
00:01:17,743 --> 00:01:19,250
最急降下法を導いたりするのに

40
00:01:19,250 --> 00:01:21,493
より便利なことが、後で分かる。

41
00:01:21,493 --> 00:01:24,492
具体的には、コスト関数を以下のように書き下せる。

42
00:01:24,492 --> 00:01:27,304
cost の h(x), y は

43
00:01:27,304 --> 00:01:29,269
これをこのように

44
00:01:29,269 --> 00:01:31,750
-y 掛ける

45
00:01:31,770 --> 00:01:34,201
log h(x)

46
00:01:34,201 --> 00:01:37,730
引くことの

47
00:01:38,060 --> 00:01:41,615
(1-y) 掛ける

48
00:01:41,660 --> 00:01:44,655
log(1-h(x))。

49
00:01:44,670 --> 00:01:45,824
そして

50
00:01:45,824 --> 00:01:48,062
この式が、

51
00:01:48,062 --> 00:01:51,038
この方程式が

52
00:01:51,038 --> 00:01:52,354
ここまで使ってきた

53
00:01:52,354 --> 00:01:54,195
コスト関数と等価な、

54
00:01:54,195 --> 00:01:56,353
よりコンパクトな書き方であることを示す。

55
00:01:56,353 --> 00:02:00,243
どうしてこれがそうなのか、見てみよう。

56
00:02:03,730 --> 00:02:06,190
私たちは２つの場合しかありえないことを知っている。

57
00:02:06,190 --> 00:02:07,210
y は 0 か 1 でなくてはならない。

58
00:02:07,230 --> 00:02:10,857
では y=1 と仮定してみよう。

59
00:02:10,857 --> 00:02:12,480
もし y=1 だと

60
00:02:12,480 --> 00:02:14,822
この等式は

61
00:02:14,822 --> 00:02:17,603
コストが

62
00:02:18,573 --> 00:02:20,172
イコール、

63
00:02:20,172 --> 00:02:23,895
y=1 だと、このこれはイコール 1 で、

64
00:02:23,900 --> 00:02:26,631
そして 1-y はイコール 0 となる。

65
00:02:26,631 --> 00:02:27,852
つまりもし y=1 なら

66
00:02:27,860 --> 00:02:29,348
その時は 1-y は

67
00:02:29,370 --> 00:02:32,336
1-1 になる、つまり 0 だ。

68
00:02:32,336 --> 00:02:34,076
だから２番目の項は

69
00:02:34,076 --> 00:02:36,047
0 を掛けるから、消える。

70
00:02:36,047 --> 00:02:37,380
だから残るのはこの

71
00:02:37,420 --> 00:02:38,631
最初の項だけで、

72
00:02:38,650 --> 00:02:40,654
それは -y 掛ける

73
00:02:40,654 --> 00:02:42,174
log h(x)。

74
00:02:42,174 --> 00:02:43,621
y=1 だからこれは

75
00:02:43,630 --> 00:02:46,313
イコール -log h(x)。

76
00:02:46,320 --> 00:02:48,300
この等式は

77
00:02:48,300 --> 00:02:50,050
ここの上にある

78
00:02:50,060 --> 00:02:53,276
y=1 のものと完全に一致している。

79
00:02:53,276 --> 00:02:55,566
他方のケースの

80
00:02:55,566 --> 00:02:57,275
y=0 の場合。

81
00:02:57,290 --> 00:02:58,718
もしその場合、

82
00:02:58,718 --> 00:03:01,430
このコスト関数の

83
00:03:01,500 --> 00:03:03,584
書き方によると、

84
00:03:03,600 --> 00:03:05,500
y=0 なら

85
00:03:05,500 --> 00:03:08,381
ここの項、これはイコール 0 となる。

86
00:03:08,381 --> 00:03:10,111
一方 1-y は

87
00:03:10,111 --> 00:03:11,270
もし y=0 なら、

88
00:03:11,280 --> 00:03:12,528
これはイコール 1 となる。何故なら 1-y は

89
00:03:12,530 --> 00:03:14,556
1-0 になり、

90
00:03:14,556 --> 00:03:16,650
これはつまり 1 だからだ。

91
00:03:16,650 --> 00:03:18,643
だからこのコスト関数は

92
00:03:18,643 --> 00:03:22,583
単にこの最後の項だけになる。

93
00:03:22,583 --> 00:03:22,583
いいかな？

94
00:03:22,583 --> 00:03:24,724
何故ならここの最初の項は

95
00:03:24,724 --> 00:03:27,493
0 を掛けるので、消える。

96
00:03:27,493 --> 00:03:28,802
だからこの最後の項だけが残り、

97
00:03:28,802 --> 00:03:30,486
それは -log、、、

98
00:03:30,510 --> 00:03:32,566
1-h(x) だ。

99
00:03:32,590 --> 00:03:34,243
そしてここの項が

100
00:03:34,260 --> 00:03:36,013
y=0 の時の以前のものと

101
00:03:36,013 --> 00:03:40,434
まったく同じことを確認できる。

102
00:03:40,450 --> 00:03:42,260
つまり以上で、

103
00:03:42,260 --> 00:03:43,628
このコスト関数の定義は、

104
00:03:43,628 --> 00:03:45,423
これら２つ、

105
00:03:45,423 --> 00:03:47,376
y=0 と y=1 の場合の両方を

106
00:03:47,376 --> 00:03:48,757
コンパクトに

107
00:03:48,757 --> 00:03:50,284
書いているに過ぎない、ということが示された。

108
00:03:50,284 --> 00:03:52,014
それらを一行で、

109
00:03:52,030 --> 00:03:54,580
よりコンパクトに書ける。

110
00:03:54,600 --> 00:03:56,449
ゆえに、私たちは全体のコスト関数を

111
00:03:56,449 --> 00:03:59,598
以下のように書ける。

112
00:03:59,598 --> 00:04:00,628
それはこのように

113
00:04:00,628 --> 00:04:01,746
1/m にこのコスト関数を

114
00:04:01,746 --> 00:04:03,856
和を取って、

115
00:04:03,856 --> 00:04:05,123
そのコスト関数のところに既に見出したものを

116
00:04:05,123 --> 00:04:07,255
代入すると、結局こうなる。

117
00:04:07,255 --> 00:04:09,767
ちなみにマイナスの符号は外に出した。

118
00:04:09,767 --> 00:04:12,214
ところで、何故このコスト関数を選んだのか？

119
00:04:12,230 --> 00:04:16,250
他にも選ぶ方法があるように思えるかもしれないが、

120
00:04:16,250 --> 00:04:17,427
このコースでは

121
00:04:17,430 --> 00:04:19,171
細かく踏み入る時間はないが、

122
00:04:19,171 --> 00:04:21,345
このコスト関数は

123
00:04:21,345 --> 00:04:23,566
統計学の

124
00:04:23,566 --> 00:04:25,416
最尤法推計の原理で

125
00:04:25,440 --> 00:04:26,816
導くことができる。

126
00:04:26,820 --> 00:04:29,754
最尤法は、さまざまなモデルのパラメータ シータを

127
00:04:29,770 --> 00:04:33,014
効率的に探す方法だ。

128
00:04:33,014 --> 00:04:35,843
そしてこれはまた、便利な性質として凸だというのがある。

129
00:04:35,860 --> 00:04:37,666
事実上、これが

130
00:04:37,666 --> 00:04:40,003
みんなが使っている

131
00:04:40,040 --> 00:04:42,736
ロジスティック回帰のコスト関数だ。

132
00:04:42,740 --> 00:04:44,264
もし今私の使った単語が分からない、

133
00:04:44,264 --> 00:04:45,731
最尤法推計の原理というのが

134
00:04:45,731 --> 00:04:47,280
何なのか知らない人も

135
00:04:47,280 --> 00:04:49,706
気にしないでよろしい。

136
00:04:49,706 --> 00:04:51,240
それらは単なる、

137
00:04:51,250 --> 00:04:53,780
このコスト関数にまつわる背景となる

138
00:04:53,790 --> 00:04:55,617
より深い所での理由づけに過ぎなくて

139
00:04:55,630 --> 00:04:58,203
それはこのクラスで扱う範囲を超えているというだけだ。

140
00:04:58,203 --> 00:05:00,683
で、このコスト関数を使って

141
00:05:00,683 --> 00:05:02,601
パラメータをフィッティングするために、

142
00:05:02,601 --> 00:05:04,541
私たちは J のシータを最小化する

143
00:05:04,541 --> 00:05:07,896
パラメータのシータを見つけたい。

144
00:05:07,910 --> 00:05:10,716
つまりこれを最小化しようとすると、

145
00:05:10,716 --> 00:05:15,006
何らかのバラメータのシータの組が得られるわけだ。

146
00:05:15,006 --> 00:05:17,157
最後に、なんらかの特徴量の組 X の

147
00:05:17,157 --> 00:05:18,549
新しいサンプルが与えられると

148
00:05:18,549 --> 00:05:20,164
訓練セットで

149
00:05:20,164 --> 00:05:21,640
フィッティングした

150
00:05:21,640 --> 00:05:23,980
シータを使って、

151
00:05:23,980 --> 00:05:25,793
予測を出力させることができる。

152
00:05:25,800 --> 00:05:27,336
念のためもう一度言うと

153
00:05:27,336 --> 00:05:28,842
仮説の出力をどう解釈すべきかと言えば

154
00:05:28,850 --> 00:05:30,253
y=1 となる

155
00:05:30,253 --> 00:05:33,001
確率、という意味だった。

156
00:05:33,001 --> 00:05:34,656
それはある x と

157
00:05:34,670 --> 00:05:36,900
パラメータのシータの元での確率。

158
00:05:36,900 --> 00:05:38,070
しかし、仮説が単に

159
00:05:38,070 --> 00:05:40,613
y=1 となる確率を

160
00:05:40,613 --> 00:05:43,873
見積もっていると思えば良い。

161
00:05:43,880 --> 00:05:45,579
では残っているタスクは、

162
00:05:45,590 --> 00:05:47,143
J のシータを実際に

163
00:05:47,150 --> 00:05:49,520
どうシータの関数として

164
00:05:49,520 --> 00:05:51,005
最小化するか、

165
00:05:51,010 --> 00:05:52,519
つまりどうパラメータを実際に

166
00:05:52,519 --> 00:05:55,625
訓練セットにフィッティングするかということ。

167
00:05:56,390 --> 00:05:57,819
コスト関数を最小化するのに使う方法として

168
00:05:57,819 --> 00:06:00,599
私たちは最急降下法を使う。

169
00:06:00,600 --> 00:06:02,225
これが私たちのコスト関数。

170
00:06:02,250 --> 00:06:05,307
そしてこれをシータの関数として最小化したい場合は、

171
00:06:05,340 --> 00:06:08,070
これがいつもの最急降下法のテンプレートだ。

172
00:06:08,070 --> 00:06:09,880
こうやって繰り返し

173
00:06:09,880 --> 00:06:12,398
各パラメータを更新していく、

174
00:06:12,398 --> 00:06:14,099
自身 引くことの

175
00:06:14,099 --> 00:06:17,684
学習率アルファ掛ける微分の項、で。

176
00:06:17,684 --> 00:06:19,219
もし解析学を知っているなら

177
00:06:19,219 --> 00:06:20,739
どうぞご自由にこの項を微分して

178
00:06:20,739 --> 00:06:22,788
自分で計算してみてほしい、

179
00:06:22,788 --> 00:06:24,592
私が得たのと同じものに

180
00:06:24,592 --> 00:06:26,664
整理できるはずだ。

181
00:06:26,664 --> 00:06:30,538
だがもしあなたが解析学をあまり分からなくても、気にしないで良い。

182
00:06:30,538 --> 00:06:32,355
もし実際に計算したら

183
00:06:32,370 --> 00:06:34,811
これを得ることになるのだから。

184
00:06:34,811 --> 00:06:37,634
それをここに書く。

185
00:06:37,634 --> 00:06:39,047
i を 1 から m まで

186
00:06:39,047 --> 00:06:41,386
和を取ることの、

187
00:06:41,386 --> 00:06:43,722
要するに、誤差項だ、それに掛けることの

188
00:06:43,722 --> 00:06:46,378
x の i j。

189
00:06:46,390 --> 00:06:48,504
つまりこの偏微分をとって

190
00:06:48,504 --> 00:06:49,716
ここに代入すると

191
00:06:49,716 --> 00:06:51,210
最急降下法のアルゴリズムを

192
00:06:51,230 --> 00:06:55,203
以下のように書き下せる。

193
00:06:55,203 --> 00:06:56,393
要するに、ここまでやったことは

194
00:06:56,393 --> 00:06:57,633
前のスライドから偏微分の項を取ってきて

195
00:06:57,633 --> 00:07:00,163
ここに代入したということ。

196
00:07:00,170 --> 00:07:01,454
だからもし n 個の特徴量があるなら

197
00:07:01,454 --> 00:07:03,856
パラメータもベクトルで

198
00:07:03,856 --> 00:07:06,865
それは

199
00:07:06,865 --> 00:07:08,417
シータ0、シータ1、シータ2、、、と

200
00:07:08,417 --> 00:07:10,031
シータnまであるはず。

201
00:07:10,031 --> 00:07:11,324
そしてこれを使って

202
00:07:11,340 --> 00:07:13,930
シータの値をすべて

203
00:07:13,930 --> 00:07:15,920
同時に更新することになる。

204
00:07:15,950 --> 00:07:17,378
ここでこの更新規則を見て

205
00:07:17,378 --> 00:07:19,498
線形回帰の時に

206
00:07:19,498 --> 00:07:21,175
私たちがやっていた更新規則と

207
00:07:21,180 --> 00:07:23,364
比較すると

208
00:07:23,370 --> 00:07:25,679
この等式は線形回帰の時のものと

209
00:07:25,710 --> 00:07:28,958
完全に一致していると

210
00:07:28,970 --> 00:07:30,529
分かるだろう。

211
00:07:30,550 --> 00:07:31,678
実際以前の

212
00:07:31,678 --> 00:07:33,234
ビデオを見て

213
00:07:33,240 --> 00:07:35,123
更新規則を見ると

214
00:07:35,123 --> 00:07:36,543
線形回帰の最急降下法のルールは

215
00:07:36,550 --> 00:07:38,418
ここの青の四角の中に書いたものと

216
00:07:38,418 --> 00:07:41,268
まったく同じはずだ。

217
00:07:41,268 --> 00:07:43,280
では線形回帰とロジスティック回帰は

218
00:07:43,280 --> 00:07:45,875
同じアルゴリズムなのか？違うのか？

219
00:07:45,900 --> 00:07:47,415
この問いは、ロジスティック回帰で変わった部分を

220
00:07:47,415 --> 00:07:49,468
観察することで解決しよう。

221
00:07:49,500 --> 00:07:51,376
どこが変わったかと言えば

222
00:07:51,380 --> 00:07:54,723
この仮説の定義が変わったのだ。

223
00:07:54,723 --> 00:07:56,788
つまり線形回帰では

224
00:07:56,800 --> 00:07:58,586
h(x) はイコール

225
00:07:58,620 --> 00:08:01,093
シータの転置に x だったが、

226
00:08:01,093 --> 00:08:02,633
今回は h(x) は変わって、

227
00:08:02,633 --> 00:08:04,060
代わりに

228
00:08:04,060 --> 00:08:06,460
(1 プラス e のマイナスシータ転置掛ける x 乗) 分の 1 と

229
00:08:06,460 --> 00:08:07,897
なっている。

230
00:08:07,910 --> 00:08:09,326
つまり更新規則は

231
00:08:09,340 --> 00:08:12,213
見た目上同一に見えるが

232
00:08:12,230 --> 00:08:13,872
仮説の定義が変わっているので

233
00:08:13,872 --> 00:08:15,826
これは実際には線形回帰の

234
00:08:15,826 --> 00:08:19,445
最急降下法とは同じではない。

235
00:08:19,445 --> 00:08:21,063
以前のビデオで、

236
00:08:21,090 --> 00:08:22,889
線形回帰の最急降下法について

237
00:08:22,900 --> 00:08:24,514
話した時は

238
00:08:24,514 --> 00:08:26,128
最急降下法をどうモニターして

239
00:08:26,160 --> 00:08:29,630
正しく収束しているか確認する方法を話した。

240
00:08:29,630 --> 00:08:31,463
普通、私は、ロジスティック回帰でも

241
00:08:31,463 --> 00:08:33,354
最急降下法が正しく収束しているのを確認するために

242
00:08:33,354 --> 00:08:37,193
同じ手法を用いる。

243
00:08:37,220 --> 00:08:38,612
あのテクニックをどうやって

244
00:08:38,612 --> 00:08:40,306
ロジスティック回帰に適用するかは

245
00:08:40,306 --> 00:08:43,984
自分で考えてみて欲しい。

246
00:08:43,984 --> 00:08:46,603
ロジスティック回帰を最急降下法で

247
00:08:46,610 --> 00:08:48,229
実装する場合、

248
00:08:48,229 --> 00:08:50,404
私たちはこれらの異なるパラメータの値、

249
00:08:50,404 --> 00:08:52,093
シータ0からシータnまでの値を持ち、

250
00:08:52,130 --> 00:08:55,816
これらをこの式を使って更新する必要がある。

251
00:08:55,816 --> 00:08:58,770
1つ目の方法としては、for ループ使うというのが考えられる。

252
00:08:58,770 --> 00:09:00,926
つまり for で i イコール 0 から n まで、、、

253
00:09:00,926 --> 00:09:03,658
または i イコール 1 から n+1 まで、でも良いが、

254
00:09:03,658 --> 00:09:07,217
これらのパラメータを順番に更新していく。

255
00:09:07,217 --> 00:09:08,653
でももちろん、for ループを使うよりは

256
00:09:08,653 --> 00:09:10,588
ベクトル化した実装の方が

257
00:09:10,600 --> 00:09:13,163
理想的だ。

258
00:09:13,170 --> 00:09:15,072
するとベクトル化した実装は

259
00:09:15,072 --> 00:09:16,899
これら n+1 のすべての

260
00:09:16,899 --> 00:09:18,310
パラメータを

261
00:09:18,310 --> 00:09:21,110
一回で更新できる。

262
00:09:21,110 --> 00:09:22,233
そしてあなたの理解度を測るために

263
00:09:22,233 --> 00:09:23,675
このアルゴリズムの

264
00:09:23,690 --> 00:09:25,223
ベクトル化したバージョンを

265
00:09:25,223 --> 00:09:27,763
どう実装するか

266
00:09:27,763 --> 00:09:31,020
試してみると良い。

267
00:09:31,030 --> 00:09:32,331
さて、ここまでであなたは

268
00:09:32,350 --> 00:09:35,079
ロジスティック回帰の最急降下法の実装方法を知った。

269
00:09:35,079 --> 00:09:36,706
以前にもう一つ、最後になるアイデアとして

270
00:09:36,706 --> 00:09:40,753
フィーチャースケーリングというものを議論した。

271
00:09:40,753 --> 00:09:42,946
線形回帰において、フィーチャースケーリングが

272
00:09:42,946 --> 00:09:46,502
いかに最急降下法を手早く収束させる助けとなりうるかを見た。

273
00:09:46,502 --> 00:09:48,827
フィーチャースケーリングのアイデアは

274
00:09:48,850 --> 00:09:51,712
ロジスティック回帰の最急降下法でも適用できる。

275
00:09:51,730 --> 00:09:54,874
もしあなたの特徴量の中に、まったく異なるスケールのものがあるなら

276
00:09:54,890 --> 00:09:56,857
フィーチャースケーリングを適用することで

277
00:09:56,857 --> 00:09:58,941
最急降下法を

278
00:09:58,941 --> 00:10:01,550
速く走らせることができる。

279
00:10:01,550 --> 00:10:02,699
以上だ。

280
00:10:02,699 --> 00:10:04,552
あなたはロジスティック回帰を

281
00:10:04,552 --> 00:10:06,549
どう実装するかを知った。

282
00:10:06,549 --> 00:10:08,918
これは非常に強力で

283
00:10:08,918 --> 00:10:10,441
世界中でおそらくもっとも良く使われている

284
00:10:10,441 --> 00:10:11,982
分類アルゴリズムだ。

285
00:10:11,982 --> 00:10:14,130
そしてあなたはどうそれを使うかを知ったのだ。
