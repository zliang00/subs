1
00:00:00,220 --> 00:00:03,688
前回のビデオでは、線形回帰のうち、

2
00:00:03,688 --> 00:00:07,246
複数の特徴量、または変数に対するものを議論した。

3
00:00:07,246 --> 00:00:11,912
このビデオでは、パラメータを仮説にフィットさせることを議論する。

4
00:00:11,912 --> 00:00:15,175
特に、勾配降下法（Gradient Descent）を

5
00:00:15,175 --> 00:00:19,875
複数の特徴の線形回帰に適用する方法を議論する。

6
00:00:19,875 --> 00:00:24,802
私たちの記法を簡単に要約すると、これが複数変数における線形回帰の

7
00:00:24,802 --> 00:00:31,509
私たちの公式な仮説で、そこでは x0=1 という表記を採用する。

8
00:00:31,509 --> 00:00:37,505
このモデルのパラメータは、シータ0からシータnまでだが、

9
00:00:37,505 --> 00:00:42,385
これをn個の別々のパラメータと考える代わりに、

10
00:00:42,385 --> 00:00:51,175
パラメータをシータというn+1次元のベクトルと考える。

11
00:00:51,175 --> 00:00:55,498
つまり、このモデルのパラメータ自身も

12
00:00:55,498 --> 00:00:58,674
ベクトルと考える。

13
00:00:58,674 --> 00:01:03,507
コスト関数は、Jのシータ0からシータnまでで、

14
00:01:03,507 --> 00:01:08,983
普通の誤差項の二乗和で与えられる。だがJをこれらn+1個の数の

15
00:01:08,983 --> 00:01:14,016
関数と考える代わりに、より一般的に、Jを単なるパラメータベクトル、

16
00:01:14,016 --> 00:01:22,275
シータの関数とみなす。つまりここのシータはベクトル。

17
00:01:22,275 --> 00:01:26,897
勾配降下法はこんな感じだ。各シータjを シータj - アルファ x この微分項 で、

18
00:01:26,897 --> 00:01:32,142
繰り返し、何度も更新していく。

19
00:01:32,142 --> 00:01:37,868
そしてこれを、Jのシータと書く。シータjが、シータj マイナスの

20
00:01:37,868 --> 00:01:41,840
学習率アルファ掛ける、コスト関数の微分、、、

21
00:01:41,840 --> 00:01:47,840
正確には、パラメータであるシータ jによる偏微分。

22
00:01:47,840 --> 00:01:51,305
勾配降下法を実装する時に、

23
00:01:51,305 --> 00:01:55,985
特に偏微分の項がどんな感じか、見ていこう。

24
00:01:55,985 --> 00:02:01,383
N=1の特徴量の時に勾配降下法を用いると、こうなる。

25
00:02:01,383 --> 00:02:06,782
パラメータ シータ0とシータ1に、別々の2つの更新規則があった。

26
00:02:06,782 --> 00:02:12,779
これは既に見てきた通りだ。そしてこの項はもちろん、

27
00:02:12,779 --> 00:02:17,672
パラメータ シータ0による、コスト関数の偏微分だ。

28
00:02:17,672 --> 00:02:21,891
同様に、似たような更新規則が、シータ1についてもあった。

29
00:02:21,891 --> 00:02:26,259
少し違うのは、特徴量１つの時は

30
00:02:26,259 --> 00:02:31,992
その特徴量をx(i)と呼べたが、この新しい記法でも、

31
00:02:31,992 --> 00:02:38,462
それを x(i)の1と、一つの特徴量を示すように呼ぶ。

32
00:02:38,462 --> 00:02:41,019
つまりそれは、一つしか特徴量を持たない場合だ。

33
00:02:41,019 --> 00:02:44,496
特徴量が一つより多い場合の、新しいアルゴリズムを見てみよう。

34
00:02:44,496 --> 00:02:47,350
つまり特徴量の数nが、1より大きいこともありうる場合。

35
00:02:47,350 --> 00:02:53,158
勾配降下法の更新規則はこうなる。解析学が分かる人向けに言うと、

36
00:02:53,158 --> 00:02:57,781
コスト関数の定義をとってきて、

37
00:02:57,781 --> 00:03:03,312
そのコスト関数Jを、パラメータ シータiに関して偏微分を取ると、

38
00:03:03,312 --> 00:03:08,119
その偏微分の項は、ここに青い四角で書いた項と

39
00:03:08,119 --> 00:03:10,665
正確に一致する。

40
00:03:10,665 --> 00:03:14,837
これを実装すれば、それは多変量の線形回帰に対する

41
00:03:14,837 --> 00:03:18,962
勾配降下法の、動く実装となる。

42
00:03:18,962 --> 00:03:21,572
このスライドでは、

43
00:03:21,572 --> 00:03:26,882
この新旧のアルゴリズムは同じという、なんとなくの感覚を感じてもらいたい。

44
00:03:26,882 --> 00:03:30,904
言い換えると、両者が何故似たアルゴリズムなのか、
どうしてどちらも勾配降下法アルゴリズムなのかを、感覚的にわかって欲しい。

45
00:03:30,904 --> 00:03:34,363
2つの特徴量、または2つ以上の特徴量がある場合を

46
00:03:34,363 --> 00:03:37,488
考えてみよう。例えばシータ0 シータ1 シータ2の、

47
00:03:37,488 --> 00:03:42,680
3つの更新規則がある。更に別のシータがあっても良い。

48
00:03:42,680 --> 00:03:49,457
ここでシータ0の更新規則を見ると、

49
00:03:49,457 --> 00:03:55,300
これは前にやった、n=1の時の更新と

50
00:03:55,300 --> 00:03:57,350
同じことに気付く。

51
00:03:57,350 --> 00:04:00,203
それらが等しい理由は、もちろん、

52
00:04:00,203 --> 00:04:06,871
私たちの採用した記法の表記では、x(i) 0 は1という表記だから。

53
00:04:06,871 --> 00:04:12,003
従って、マゼンダ色の四角で描いたこれら二項は等価である。

54
00:04:12,003 --> 00:04:16,010
同じように、シータ1の更新規則を見ると、

55
00:04:16,010 --> 00:04:21,540
ここのこの項は、以前のシータ1の時のもの、

56
00:04:21,540 --> 00:04:25,020
つまり以前の方程式または更新規則と等価だ。

57
00:04:25,020 --> 00:04:30,222
もちろん、この新しい記法、x(i)の1を最初の特徴量を表すのに使っていて、

58
00:04:30,222 --> 00:04:37,605
今は一つより多い特徴量を扱っている。

59
00:04:37,605 --> 00:04:43,560
シータ2などにも、似たような更新規則を用いることができる。

60
00:04:43,560 --> 00:04:48,219
このスライドでは多くのことを説明したので、ビデオを一時停止して、

61
00:04:48,219 --> 00:04:52,020
このスライドのすべての数式をゆっくりと見直して、

62
00:04:52,020 --> 00:04:55,446
これらをすべてしっかりと理解していることを確認するよう、強く推奨する。

63
00:04:55,446 --> 00:05:00,440
ここに書かれたアルゴリズムを実装したら、

64
00:05:00,440 --> 00:05:51,300
複数特徴量の線形回帰が実装できる。
